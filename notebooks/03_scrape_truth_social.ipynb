{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import subprocess, sys, importlib, os, re\n",
    "from datetime import datetime\n",
    "import truthbrush as tb\n",
    "import sqlite3\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UNIVERSAL PATCH CELL  (run once, very top of notebook) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import subprocess, sys, importlib, os, types\n",
    "from pathlib import Path\n",
    "\n",
    "# 1ï¸âƒ£  make sure both python-dotenv and curl_cffi exist\n",
    "def ensure(pkg, src=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ModuleNotFoundError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", src or pkg]\n",
    "        )\n",
    "\n",
    "ensure(\"python-dotenv\")\n",
    "ensure(\"curl_cffi\")\n",
    "\n",
    "# 2ï¸âƒ£  reload .env (override=True guarantees fresh values)\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "# 3ï¸âƒ£  import truthbrush and inject curl_cffi so NameError canâ€™t happen\n",
    "import curl_cffi                     # noqa:  F401  (needed for side-effect)\n",
    "import truthbrush.api as tb_api\n",
    "tb_api.curl_cffi = curl_cffi         # hand it to truthbrushâ€™s module scope\n",
    "\n",
    "import truthbrush as tb\n",
    "print(\"âœ” Patch cell finished â€“ environment refreshed, curl_cffi wired\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import truthbrush as tb\n",
    "from datetime import datetime, timezone\n",
    "import random, time\n",
    "\n",
    "# install SINGLE wrapper around _get  (skip if attribute exists)\n",
    "if not hasattr(tb.api.Api, \"_get_base\"):\n",
    "    tb.api.Api._get_base = tb.api.Api._get      # save original once\n",
    "\n",
    "    def _polite_get(self, url, params=None):\n",
    "        resp = self._get_base(url, params)\n",
    "        # header-based sleep\n",
    "        if (self.ratelimit_remaining is not None\n",
    "                and self.ratelimit_remaining <= 10\n",
    "                and self.ratelimit_reset):\n",
    "            wait = max(\n",
    "                0,\n",
    "                (self.ratelimit_reset -\n",
    "                 datetime.utcnow().replace(tzinfo=timezone.utc)).total_seconds()\n",
    "            ) + random.uniform(1, 3)\n",
    "            print(f\"ğŸ“‰ near limit â€“ sleeping {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "        else:\n",
    "            time.sleep(random.uniform(1.5, 3.0))\n",
    "        return resp\n",
    "\n",
    "    tb.api.Api._get = _polite_get\n",
    "    print(\"âœ“ polite-delay wrapper installed\")\n",
    "else:\n",
    "    print(\"âœ“ wrapper already present â€“ no re-patch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"IhoaRRTm-gkV7PyfH70sIofN-pf24NnAIL8fabmc6Sg\"   # put your live token\n",
    "api   = tb.Api(token=TOKEN)\n",
    "api.auth_id = api.auth_id or \"\"\n",
    "print(\"client ready\")\n",
    "\n",
    "print(\"lookup test:\")\n",
    "try:\n",
    "    print(api.lookup(\"realDonaldTrump\")[\"id\"][:8], \"â€¦ lookup OK\")\n",
    "except Exception as e:\n",
    "    print(\"lookup failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ scrape Truth Social & save CSV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, re, csv, pathlib, random, time, json, sys\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------- config ---------------------------------------------------------\n",
    "KEYWORDS = [\n",
    "    \"russia\", \"russian\", \"ukraine\", \"ukrainian\", \"ru-uk\", \"putin\",\n",
    "    \"zelensky\", \"zelenskyy\", \"kremlin\", \"kyiv\", \"crimea\", \"donbas\",\n",
    "    \"mariupol\", \"kherson\", \"luhansk\", \"dnipro\", \"odessa\", \"invasion\", \"war\",\n",
    "]\n",
    "SEED_HANDLES = [\n",
    "    \"realDonaldTrump\", \"TeamTrump\", \"TrumpWarRoom\", \"WhiteHouse\", \"PressSec\"\n",
    "]\n",
    "\n",
    "def keyword_hit(html: str) -> bool:\n",
    "    return any(k in html.lower() for k in KEYWORDS)\n",
    "\n",
    "# ---------- resolve handles ------------------------------------------------\n",
    "def canonical_handle(hint: str) -> str | None:\n",
    "    try:\n",
    "        info = api.lookup(user_handle=hint.lstrip(\"@\"))\n",
    "        return info.get(\"acct\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            page = next(api.search(\"accounts\", hint, limit=1))\n",
    "            return page[\"accounts\"][0][\"acct\"] if page[\"accounts\"] else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "handles = [h for h in (canonical_handle(x) for x in SEED_HANDLES) if h]\n",
    "print(\"Scanning:\", handles)\n",
    "\n",
    "# ---------- scrape with live progress --------------------------------------\n",
    "hits = []\n",
    "try:\n",
    "    for h in handles:\n",
    "        print(f\"\\nâ†³ pulling @{h}\")\n",
    "\n",
    "        # get a generator *but* pull the first post immediately so you know it's alive\n",
    "        gen = api.pull_statuses(username=h, replies=False, verbose=False)\n",
    "        print(\"  â€¦ sending first request\")\n",
    "        try:\n",
    "            first_post = next(gen)\n",
    "            print(\"  âœ“ first post received\")\n",
    "        except StopIteration:\n",
    "            print(\"  âš ï¸ no posts found for this account\")\n",
    "            continue\n",
    "\n",
    "        # prepend that first item back into the stream\n",
    "        def prepend_first(item, iterator):\n",
    "            yield item\n",
    "            yield from iterator\n",
    "        gen = prepend_first(first_post, gen)\n",
    "\n",
    "        pbar = tqdm(gen, unit=\"post\", desc=f\"{h}\", leave=True)\n",
    "        matched = 0\n",
    "        for post in pbar:\n",
    "            if post and post.get(\"content\") and keyword_hit(post[\"content\"]):\n",
    "                matched += 1\n",
    "                hits.append(\n",
    "                    {\n",
    "                        \"created_at\": post[\"created_at\"],\n",
    "                        \"account\"   : h,\n",
    "                        \"id\"        : post[\"id\"],\n",
    "                        \"text\"      : re.sub(r\"<[^>]+>\", \"\", post[\"content\"]).strip(),\n",
    "                    }\n",
    "                )\n",
    "            if matched % 25 == 0:             # update label every 25 matches\n",
    "                pbar.set_description(f\"{h}  hits:{matched}\")\n",
    "        pbar.close()\n",
    "        print(f\"âœ“ @{h}: {matched} matches collected\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nâ¹ Interrupted by user â€“ proceeding with what we have â€¦\")\n",
    "\n",
    "print(f\"\\nâœ“ total matches: {len(hits)}\")\n",
    "\n",
    "# ---------- preview top 5 ---------------------------------------------------\n",
    "for p in hits[:5]:\n",
    "    ts = dt_parse.isoparse(p[\"created_at\"]).strftime(\"%Y-%m-%d %H:%M\")\n",
    "    print(f\"[{ts}] @{p['account']} â†’ {p['text'][:120]}â€¦\")\n",
    "\n",
    "# ---------- save CSV --------------------------------------------------------\n",
    "ROOT = pathlib.Path.cwd()\n",
    "while ROOT.parent != ROOT and not (ROOT / \".git\").exists() and not (ROOT / \".env\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "out_path = ROOT / \"outputs\" / \"trump_ru_uk_truths.csv\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"created_at\", \"account\", \"id\", \"text\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(hits)\n",
    "\n",
    "print(\"ğŸ“„ CSV written â†’\", out_path.relative_to(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ merge & dedup Truth-Social match files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd, hashlib, pathlib, textwrap\n",
    "\n",
    "ROOT = pathlib.Path.cwd()\n",
    "while ROOT.parent != ROOT and not (ROOT/\".git\").exists() and not (ROOT/\".env\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "CSV_FILES = [\n",
    "    ROOT / \"outputs\" / \"trump_ru_uk_truths.csv\",\n",
    "    ROOT / \"outputs\" / \"new_truth_scrape_matches.csv\",\n",
    "]\n",
    "\n",
    "frames = []\n",
    "for p in CSV_FILES:\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p, dtype=str)       # keep ids as str\n",
    "        frames.append(df)\n",
    "        print(f\"âœ“ loaded {p.name:<30} rows={len(df):>5}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  {p} not found\")\n",
    "\n",
    "if not frames:\n",
    "    raise FileNotFoundError(\"No CSVs found â€“ check paths above.\")\n",
    "\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# -------- exact-ID dedup ----------------------------------------\n",
    "before   = len(df_all)\n",
    "df_all   = df_all.drop_duplicates(subset=\"id\")\n",
    "\n",
    "# -------- fallback fuzzy key  -----------------------------------\n",
    "# Sometimes Truthbrush returns slightly different IDs for the same content.\n",
    "# Build a cheap content hash (first 120 chars lower-cased, account-specific).\n",
    "def row_key(r):\n",
    "    snippet = (r[\"text\"][:120] if isinstance(r[\"text\"], str) else \"\").lower()\n",
    "    return hashlib.md5((r[\"account\"] + snippet).encode()).hexdigest()\n",
    "\n",
    "df_all[\"dup_key\"] = df_all.apply(row_key, axis=1)\n",
    "df_all = df_all.drop_duplicates(subset=\"dup_key\").drop(columns=\"dup_key\")\n",
    "\n",
    "after_exact  = before\n",
    "after_final  = len(df_all)\n",
    "\n",
    "print(f\"\"\"\n",
    "rows in concat   : {before}\n",
    "unique post IDs  : {after_exact}\n",
    "unique after key : {after_final}\n",
    "duplicates dropped: {before - after_final}\n",
    "\"\"\".strip())\n",
    "\n",
    "# save\n",
    "out_path = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "df_all.to_csv(out_path, index=False)\n",
    "print(\"ğŸ’¾ merged CSV â†’\", out_path.relative_to(ROOT))\n",
    "\n",
    "# peek a few rows\n",
    "print(\"\\nâ–¶ sample rows\")\n",
    "for _, r in df_all.head(5).iterrows():\n",
    "    print(textwrap.shorten(r['text'].replace(\"\\n\",\" \"), width=100, placeholder=\"â€¦\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€ monthly conflict-keyword Truths (all accounts) â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"created_at\"])\n",
    "\n",
    "if df.empty:\n",
    "    print(\"âš ï¸  DataFrame is empty â€“ check the CSV path or merge step.\")\n",
    "else:\n",
    "    # bucket by month\n",
    "    df[\"month\"] = df[\"created_at\"].dt.to_period(\"M\").astype(str)\n",
    "    counts = df.groupby(\"month\").size()\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Conflict-related Truths per month\")\n",
    "    plt.ylabel(\"Post count\")\n",
    "    plt.xticks(rotation=70, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------- paths\n",
    "vol_csv = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "esc_csv = ROOT / \"outputs\" / \"headline_scores_partial.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------- volume\n",
    "vol = (\n",
    "    pd.read_csv(vol_csv, parse_dates=[\"created_at\"])\n",
    "      .assign(day=lambda d: d[\"created_at\"].dt.date)\n",
    ")\n",
    "\n",
    "vol_daily = (\n",
    "    vol.groupby(\"day\")\n",
    "       .size()\n",
    "       .rename(\"post_count\")\n",
    "       .to_frame()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------- escalation (updated for new CSV format)\n",
    "# Now the CSV has columns: 'date', 'source', 'title', 'score'\n",
    "esc = pd.read_csv(esc_csv, parse_dates=[\"date\"])\n",
    "esc = esc.set_index(\"date\")\n",
    "\n",
    "# If there is already a 'roll7' column, use it; otherwise compute a 7-day rolling average of 'score'\n",
    "if \"roll7\" in esc.columns:\n",
    "    esc_7d = esc[\"roll7\"].rename(\"escalation_7d\")\n",
    "else:\n",
    "    esc_7d = (\n",
    "        esc[\"score\"]\n",
    "            .rolling(7, min_periods=1)\n",
    "            .mean()\n",
    "            .rename(\"escalation_7d\")\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------- merge & plot\n",
    "df = vol_daily.join(esc_7d, how=\"outer\").fillna(0)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(11, 4))\n",
    "ax1.bar(\n",
    "    df.index,\n",
    "    df[\"post_count\"],\n",
    "    width=1,\n",
    "    color=\"#4a90e2\",\n",
    "    alpha=0.55,\n",
    "    label=\"Trump RU/UA daily count\"\n",
    ")\n",
    "ax1.set_ylabel(\"daily post count\", color=\"#4a90e2\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"#4a90e2\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    df.index,\n",
    "    df[\"escalation_7d\"],\n",
    "    color=\"crimson\",\n",
    "    linewidth=2,\n",
    "    label=\"7-day mean escalation\"\n",
    ")\n",
    "ax2.set_ylabel(\"escalation index (0-10)\", color=\"crimson\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"crimson\")\n",
    "\n",
    "plt.title(\"Trump attention to RU/UA vs. conflict escalation, 2022-2025\")\n",
    "\n",
    "# Get handles & labels from each axis separately\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Combine them into single lists\n",
    "all_handles = handles1 + handles2\n",
    "all_labels = labels1 + labels2\n",
    "\n",
    "ax1.legend(all_handles, all_labels, loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Build GOLD-SET CANDIDATE sample for Truth-Social posts         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT      = Path.cwd().resolve().parents[0]\n",
    "ALL_CSV   = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "GOLD_CSV  = ROOT / \"outputs\" / \"truth_gold_candidates.csv\"\n",
    "\n",
    "N_SAMPLES = 500                          # total sample size\n",
    "TIME_BINS = 8                            # split full range into equal bins\n",
    "\n",
    "# â”€â”€ load & sanity check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(ALL_CSV, parse_dates=[\"created_at\"])\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"ğŸ’¥ merged Truth CSV seems empty â€“ aborting\")\n",
    "\n",
    "# â”€â”€ create time-bins (equal-length) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = df.sort_values(\"created_at\")\n",
    "cut_edges = np.linspace(df[\"created_at\"].min().value,\n",
    "                        df[\"created_at\"].max().value,\n",
    "                        TIME_BINS + 1)\n",
    "# assign a bin label 0 â€¦ TIME_BINS-1\n",
    "df[\"time_bin\"] = pd.cut(df[\"created_at\"].view(\"int64\"),\n",
    "                        bins=cut_edges, labels=False, include_lowest=True)\n",
    "\n",
    "# â”€â”€ stratified sample: time_bin Ã— account â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "samples = []\n",
    "per_bin = int(np.ceil(N_SAMPLES / TIME_BINS))\n",
    "\n",
    "for bin_id, grp in df.groupby(\"time_bin\"):\n",
    "    # target size in this bin\n",
    "    n = min(per_bin, len(grp))\n",
    "    # proportional by account count\n",
    "    acc_counts = grp[\"account\"].value_counts(normalize=True)\n",
    "    wants = (acc_counts * n).round().astype(int)\n",
    "\n",
    "    # adjust rounding drift\n",
    "    while wants.sum() < n:\n",
    "        wants.loc[wants.idxmax()] += 1\n",
    "    while wants.sum() > n:\n",
    "        wants.loc[wants.idxmax()] -= 1\n",
    "\n",
    "    # sample inside each account slice\n",
    "    for acc, k in wants.items():\n",
    "        slice_ = grp[grp[\"account\"] == acc]\n",
    "        k = min(k, len(slice_))\n",
    "        samples.append(slice_.sample(k, random_state=42))\n",
    "\n",
    "gold_df = (pd.concat(samples)\n",
    "             .sort_values(\"created_at\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "gold_df.to_csv(GOLD_CSV, index=False)\n",
    "print(f\"ğŸ¯ gold-set candidates written â†’ {GOLD_CSV.relative_to(ROOT)}   \"\n",
    "      f\"(rows={len(gold_df)})\")\n",
    "\n",
    "# quick peek\n",
    "print(\"\\nâ–¶ random preview\")\n",
    "for _, r in gold_df.sample(5, random_state=1).iterrows():\n",
    "    print(f\"[{r.created_at:%Y-%m-%d}] @{r.account}: {r.text[:110]}â€¦\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  FILTER Truth Social posts for Ukraine-Russia war relevance           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "INPUT_CSV = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "OUTPUT_CSV = ROOT / \"outputs\" / \"truth_ukraine_filtered.csv\"\n",
    "MODEL = \"claude-3-5-haiku-20241022\"  # Cheap and fast for filtering\n",
    "\n",
    "# Filtering prompt\n",
    "FILTER_PROMPT = \"\"\"You are filtering social media posts to identify which ones are about the Russia-Ukraine war.\n",
    "\n",
    "A post is RELEVANT (score 1) if it:\n",
    "- Directly mentions the Russia-Ukraine war, conflict, or invasion\n",
    "- Discusses military actions between Russia and Ukraine\n",
    "- Mentions Ukrainian or Russian leaders in context of the war\n",
    "- Discusses weapons, aid, or sanctions related to the conflict\n",
    "- References peace talks, negotiations, or ceasefire between Russia and Ukraine\n",
    "\n",
    "A post is NOT RELEVANT (score 0) if it:\n",
    "- Only mentions Russia or Ukraine in passing without war context\n",
    "- Discusses other conflicts (Israel, Syria, etc.) without Ukraine connection\n",
    "- Is about domestic US politics without Ukraine war connection\n",
    "- Mentions \"invasion\" referring to immigration/borders, not Ukraine\n",
    "- Uses war metaphors for non-Ukraine topics\n",
    "\n",
    "Respond with ONLY a single digit: 1 for relevant, 0 for not relevant.\n",
    "\n",
    "Examples:\n",
    "\"Putin is bombing Ukrainian cities again\" â†’ 1\n",
    "\"Biden's border invasion must stop\" â†’ 0\n",
    "\"We need to send more weapons to Ukraine\" â†’ 1\n",
    "\"Trump will end all wars including Ukraine\" â†’ 1\n",
    "\"China tariffs are destroying our economy\" â†’ 0\"\"\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV, parse_dates=[\"created_at\"])\n",
    "print(f\"ğŸ“Š Processing {len(df)} Truth Social posts for Ukraine relevance\")\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Quiet logging\n",
    "for name in (\"httpx\", \"anthropic\"):\n",
    "    logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "# Add index for tracking\n",
    "df[\"batch_idx\"] = range(len(df))\n",
    "\n",
    "# Prepare batch requests\n",
    "requests_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row.get(\"text\")) or str(row[\"text\"]).strip() == \"\":\n",
    "        continue\n",
    "        \n",
    "    request = {\n",
    "        \"custom_id\": str(row[\"batch_idx\"]),\n",
    "        \"params\": {\n",
    "            \"model\": MODEL,\n",
    "            \"max_tokens\": 5,\n",
    "            \"temperature\": 0,\n",
    "            \"system\": FILTER_PROMPT,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": str(row[\"text\"])[:1000]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    requests_list.append(request)\n",
    "\n",
    "print(f\"ğŸ“ Prepared {len(requests_list)} requests for filtering\")\n",
    "\n",
    "# Create batch\n",
    "batch = client.messages.batches.create(requests=requests_list)\n",
    "print(f\"ğŸš€ Launched batch {batch.id}\")\n",
    "\n",
    "# Monitor progress\n",
    "bar = tqdm.tqdm(total=len(requests_list), desc=\"Filtering\", unit=\"post\")\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    batch_status = client.messages.batches.retrieve(batch.id)\n",
    "    completed = (batch_status.request_counts.succeeded + \n",
    "                batch_status.request_counts.errored + \n",
    "                batch_status.request_counts.canceled + \n",
    "                batch_status.request_counts.expired)\n",
    "    bar.n = completed\n",
    "    bar.refresh()\n",
    "    \n",
    "    if batch_status.processing_status == \"ended\":\n",
    "        bar.close()\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"âœ… Filtering complete in {elapsed_time/60:.1f} minutes\")\n",
    "\n",
    "# Parse results\n",
    "relevance_scores = {}\n",
    "errors = []\n",
    "\n",
    "# Retrieve results\n",
    "batch_final = client.messages.batches.retrieve(batch.id)\n",
    "\n",
    "if batch_final.results_url:\n",
    "    print(f\"ğŸ“¥ Fetching results from batch {batch.id}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(batch_final.results_url, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                result = json.loads(line)\n",
    "                custom_id = result.get(\"custom_id\")\n",
    "                \n",
    "                if custom_id is None:\n",
    "                    continue\n",
    "                \n",
    "                idx = int(custom_id)\n",
    "                \n",
    "                if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                    errors.append(f\"Request {custom_id} failed\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the response\n",
    "                message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "                \n",
    "                # Parse the score (should be just \"0\" or \"1\")\n",
    "                if message_content in [\"0\", \"1\"]:\n",
    "                    relevance_scores[idx] = int(message_content)\n",
    "                else:\n",
    "                    errors.append(f\"Invalid response for {custom_id}: {message_content}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors.append(f\"Error parsing result: {e}\")\n",
    "                continue\n",
    "\n",
    "# Map scores back to dataframe\n",
    "df[\"is_ukraine_relevant\"] = df[\"batch_idx\"].map(relevance_scores)\n",
    "\n",
    "# Filter to only relevant posts\n",
    "df_relevant = df[df[\"is_ukraine_relevant\"] == 1].copy()\n",
    "df_irrelevant = df[df[\"is_ukraine_relevant\"] == 0].copy()\n",
    "\n",
    "# Save filtered results\n",
    "df_relevant.drop(columns=[\"batch_idx\", \"is_ukraine_relevant\"]).to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nğŸ“Š Filtering Results:\")\n",
    "print(f\"   Total posts: {len(df)}\")\n",
    "print(f\"   Ukraine-relevant: {len(df_relevant)} ({len(df_relevant)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Not relevant: {len(df_irrelevant)} ({len(df_irrelevant)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Failed to classify: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\nâš ï¸  Errors encountered: {len(errors)}\")\n",
    "    for error in errors[:5]:\n",
    "        print(f\"   - {error}\")\n",
    "\n",
    "# Show examples of filtered out posts\n",
    "print(\"\\nğŸ” Examples of posts filtered OUT as not Ukraine-related:\")\n",
    "for _, row in df_irrelevant.head(5).iterrows():\n",
    "    print(f\"   - {row['text'][:100]}...\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Filtered data saved to: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## First Pass New Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Comprehensive Truth Social Ukraine/Russia Scraper â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, re, csv, pathlib, random, time, json, sys\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "from collections import deque\n",
    "\n",
    "# Use the already instantiated api object from your previous cell\n",
    "\n",
    "# ---------- Date Range Configuration ---------------------------------------\n",
    "# Russia invaded Ukraine on Feb 24, 2022. Start a few days before\n",
    "START_DATE = datetime(2022, 2, 20, 0, 0, 0, tzinfo=timezone.utc)\n",
    "END_DATE = datetime(2025, 6, 5, 23, 59, 59, tzinfo=timezone.utc)\n",
    "\n",
    "print(f\"Collection period: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"Total days: {(END_DATE - START_DATE).days}\")\n",
    "\n",
    "# ---------- Enhanced Keywords Configuration --------------------------------\n",
    "KEYWORDS = [\n",
    "    # Core terms\n",
    "    \"russia\", \"russian\", \"russians\", \"Ñ€Ğ¾ÑÑĞ¸Ñ\", \"Ñ€ÑƒÑÑĞºĞ¸Ğ¹\",\n",
    "    \"ukraine\", \"ukrainian\", \"ukrainians\", \"ÑƒĞºÑ€Ğ°Ñ—Ğ½Ğ°\", \"ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¸Ğ¹\", \n",
    "    \"putin\", \"Ğ¿ÑƒÑ‚Ğ¸Ğ½\", \"zelensky\", \"zelenskyy\", \"Ğ·ĞµĞ»ĞµĞ½ÑÑŒĞºĞ¸Ğ¹\",\n",
    "    \n",
    "    # Conflict terms\n",
    "    \"invasion\", \"war\", \"conflict\", \"attack\", \"offensive\", \"defensive\",\n",
    "    \"sanctions\", \"embargo\", \"military\", \"troops\", \"soldiers\", \"army\",\n",
    "    \n",
    "    # Geographic locations\n",
    "    \"kremlin\", \"kyiv\", \"kiev\", \"moscow\", \"crimea\", \"Ğ´Ğ¾Ğ½Ğ±Ğ°Ñ\", \"donbas\", \n",
    "    \"donetsk\", \"luhansk\", \"lugansk\", \"mariupol\", \"kherson\", \"Ñ…ĞµÑ€ÑĞ¾Ğ½\",\n",
    "    \"zaporizhzhia\", \"odessa\", \"Ğ¾Ğ´ĞµÑÑĞ°\", \"dnipro\", \"kharkiv\", \"Ñ…Ğ°Ñ€ÑŒĞºĞ¾Ğ²\",\n",
    "    \"bakhmut\", \"severodonetsk\", \"melitopol\", \"berdyansk\",\n",
    "    \n",
    "    # Military/political terms\n",
    "    \"nato\", \"Ğ½Ğ°Ñ‚Ğ¾\", \"wagner\", \"Ğ²Ğ°Ğ³Ğ½ĞµÑ€\", \"azov\", \"Ğ°Ğ·Ğ¾Ğ²\", \"referendum\",\n",
    "    \"annexation\", \"liberation\", \"occupation\", \"peacekeeping\", \"ceasefire\",\n",
    "    \"himars\", \"javelin\", \"patriot\", \"abrams\", \"leopard\", \"f16\",\n",
    "    \n",
    "    # Narrative indicators\n",
    "    \"denazification\", \"demilitarization\", \"special operation\", \n",
    "    \"ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ\", \"smo\", \"proxy war\", \"biolabs\", \"bioweapons\",\n",
    "    \n",
    "    # Support indicators\n",
    "    \"standwithukraine\", \"standwithrussia\", \"slavaukraini\", \"ÑĞ»Ğ°Ğ²Ğ°\",\n",
    "    \"istandwithputin\", \"istandwithzelensky\", \"stopthewar\", \"nowar\",\n",
    "    \n",
    "    # Economic terms\n",
    "    \"gas\", \"oil\", \"grain\", \"ruble\", \"hryvnia\", \"swift\", \"nordstream\",\n",
    "    \"gazprom\", \"rosneft\", \"grain deal\", \"black sea\"\n",
    "]\n",
    "\n",
    "# ---------- VERIFIED Truth Social Accounts ---------------------------------\n",
    "# Based on research, these accounts actually exist and are active\n",
    "VERIFIED_ACCOUNTS = [\n",
    "    # Core political figures (confirmed active)\n",
    "    \"realDonaldTrump\",      # 3.8M followers\n",
    "    \"DevinNunes\",           # CEO of Truth Social\n",
    "    \n",
    "    # Politicians known to be on Truth Social\n",
    "    \"DanScavino\",           # Trump's deputy chief of staff\n",
    "    \"KariLake\",             # AZ gubernatorial candidate\n",
    "    \"JDVance1\",             # Ohio Senator (check exact handle)\n",
    "    \"MarcoPolo\",            # Marco Rubio (verify handle)\n",
    "    \"LeadRight\",            # Confirmed Truth Social account\n",
    "    \n",
    "    # Media personalities (research shows these exist)\n",
    "    \"DonaldJTrumpJr\",       # Confirmed account\n",
    "    \"EricTrump\",            # Confirmed account\n",
    "    \"TuckerCarlson\",        # If he has account\n",
    "    \"DineshDSouza\",         # Conservative commentator\n",
    "    \n",
    "    # News organizations\n",
    "    \"BreitbartNews\",        # Confirmed to have bot account\n",
    "    \"RSBNetwork\",           # Right Side Broadcasting\n",
    "    \"OANN\",                 # One America News (if exists)\n",
    "    \n",
    "    # Other verified accounts that discuss geopolitics\n",
    "    \"truthsupport\",         # Official Truth Social account\n",
    "    \"BidenHQ\",              # Biden campaign trolling account (confirmed)\n",
    "]\n",
    "\n",
    "# Additional accounts to try (may or may not exist)\n",
    "POSSIBLE_ACCOUNTS = [\n",
    "    \"Warroom\", \"WarRoomPandemic\", \"RealGenFlynn\", \"SebGorka\",\n",
    "    \"CharlieKirk\", \"JackPosobiec\", \"CandaceOwens\", \"LauraLoomer\",\n",
    "    \"MattGaetz\", \"MTG\", \"MarjorieTaylorGreene\", \"RandPaul\",\n",
    "    \"TedCruz\", \"JoshHawley\", \"RonDeSantis\", \"TulsiGabbard\"\n",
    "]\n",
    "\n",
    "# ---------- Database Setup with Date Indexing ------------------------------\n",
    "def setup_database():\n",
    "    db_path = pathlib.Path(\"truth_social_ukraine_data.db\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            created_at TEXT,\n",
    "            created_date TEXT,  -- For easier date queries\n",
    "            account TEXT,\n",
    "            account_id TEXT,\n",
    "            text TEXT,\n",
    "            url TEXT,\n",
    "            in_reply_to_id TEXT,\n",
    "            reblog_of_id TEXT,\n",
    "            favourites_count INTEGER,\n",
    "            reblogs_count INTEGER,\n",
    "            replies_count INTEGER,\n",
    "            language TEXT,\n",
    "            visibility TEXT,\n",
    "            collection_method TEXT,\n",
    "            collected_at TEXT,\n",
    "            keywords_matched TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create index for date-based queries\n",
    "    cursor.execute('''\n",
    "        CREATE INDEX IF NOT EXISTS idx_created_date ON posts(created_date)\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE INDEX IF NOT EXISTS idx_account ON posts(account)\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# ---------- Helper Functions -----------------------------------------------\n",
    "def is_within_date_range(post_date_str):\n",
    "    \"\"\"Check if post is within our date range\"\"\"\n",
    "    try:\n",
    "        post_date = dt_parse.isoparse(post_date_str)\n",
    "        if post_date.tzinfo is None:\n",
    "            post_date = post_date.replace(tzinfo=timezone.utc)\n",
    "        return START_DATE <= post_date <= END_DATE\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def keyword_hit(text: str) -> list:\n",
    "    \"\"\"Return list of matched keywords\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    matched = [k for k in KEYWORDS if k.lower() in text_lower]\n",
    "    return matched\n",
    "\n",
    "def clean_html(html: str) -> str:\n",
    "    \"\"\"Remove HTML tags and decode entities\"\"\"\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', html)\n",
    "    text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
    "    text = text.replace('&quot;', '\"').replace('&#39;', \"'\")\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "def process_post(post, collection_method, conn):\n",
    "    \"\"\"Process and store a single post if within date range\"\"\"\n",
    "    if not post or not isinstance(post, dict):\n",
    "        return False\n",
    "    \n",
    "    # Check date range first\n",
    "    created_at = post.get('created_at', '')\n",
    "    if not is_within_date_range(created_at):\n",
    "        return False\n",
    "        \n",
    "    text = clean_html(post.get('content', ''))\n",
    "    matched_keywords = keyword_hit(text)\n",
    "    \n",
    "    if not matched_keywords:\n",
    "        return False\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    account = post.get('account', {})\n",
    "    \n",
    "    # Extract date for easier querying\n",
    "    created_date = created_at[:10] if created_at else None\n",
    "    \n",
    "    data = {\n",
    "        'id': post.get('id'),\n",
    "        'created_at': created_at,\n",
    "        'created_date': created_date,\n",
    "        'account': account.get('acct', 'unknown'),\n",
    "        'account_id': account.get('id', 'unknown'),\n",
    "        'text': text,\n",
    "        'url': post.get('url', ''),\n",
    "        'in_reply_to_id': post.get('in_reply_to_id'),\n",
    "        'reblog_of_id': post.get('reblog', {}).get('id') if post.get('reblog') else None,\n",
    "        'favourites_count': post.get('favourites_count', 0),\n",
    "        'reblogs_count': post.get('reblogs_count', 0),\n",
    "        'replies_count': post.get('replies_count', 0),\n",
    "        'language': post.get('language'),\n",
    "        'visibility': post.get('visibility'),\n",
    "        'collection_method': collection_method,\n",
    "        'collected_at': datetime.now(timezone.utc).isoformat(),\n",
    "        'keywords_matched': ','.join(matched_keywords[:10])  # Limit to first 10\n",
    "    }\n",
    "    \n",
    "    cursor.execute('''\n",
    "        INSERT OR IGNORE INTO posts VALUES (\n",
    "            :id, :created_at, :created_date, :account, :account_id, :text, :url,\n",
    "            :in_reply_to_id, :reblog_of_id, :favourites_count,\n",
    "            :reblogs_count, :replies_count, :language, :visibility,\n",
    "            :collection_method, :collected_at, :keywords_matched\n",
    "        )\n",
    "    ''', data)\n",
    "    \n",
    "    conn.commit()\n",
    "    return cursor.rowcount > 0\n",
    "\n",
    "# ---------- Verify Account Exists ------------------------------------------\n",
    "def verify_account(handle):\n",
    "    \"\"\"Check if account exists on Truth Social\"\"\"\n",
    "    try:\n",
    "        user_info = api.lookup(user_handle=handle.lstrip(\"@\"))\n",
    "        if user_info and user_info.get('acct'):\n",
    "            return user_info.get('acct')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try search as fallback\n",
    "    try:\n",
    "        results = list(api.search(\"accounts\", handle, limit=1))\n",
    "        if results and results[0].get('accounts'):\n",
    "            return results[0]['accounts'][0].get('acct')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# ---------- Collection Functions -------------------------------------------\n",
    "def collect_via_search_deep(query, conn):\n",
    "    \"\"\"Deep search with pagination through entire date range\"\"\"\n",
    "    collected = 0\n",
    "    oldest_date_seen = END_DATE\n",
    "    newest_date_seen = START_DATE\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nğŸ” Deep searching: '{query}'\")\n",
    "        \n",
    "        # Use Truthbrush search with high limit\n",
    "        search_gen = api.search(\"statuses\", query, limit=40)\n",
    "        \n",
    "        with tqdm(desc=f\"Search: {query}\", unit=\"posts\") as pbar:\n",
    "            page_count = 0\n",
    "            consecutive_old_posts = 0\n",
    "            \n",
    "            for page in search_gen:\n",
    "                if not page or 'statuses' not in page:\n",
    "                    break\n",
    "                    \n",
    "                statuses = page.get('statuses', [])\n",
    "                if not statuses:\n",
    "                    break\n",
    "                \n",
    "                page_count += 1\n",
    "                posts_in_range = 0\n",
    "                \n",
    "                for post in statuses:\n",
    "                    post_date_str = post.get('created_at', '')\n",
    "                    \n",
    "                    # Track date range\n",
    "                    try:\n",
    "                        post_date = dt_parse.isoparse(post_date_str)\n",
    "                        if post_date.tzinfo is None:\n",
    "                            post_date = post_date.replace(tzinfo=timezone.utc)\n",
    "                            \n",
    "                        if post_date < oldest_date_seen:\n",
    "                            oldest_date_seen = post_date\n",
    "                        if post_date > newest_date_seen:\n",
    "                            newest_date_seen = post_date\n",
    "                            \n",
    "                        # If post is before our start date, increment counter\n",
    "                        if post_date < START_DATE:\n",
    "                            consecutive_old_posts += 1\n",
    "                        else:\n",
    "                            consecutive_old_posts = 0\n",
    "                            \n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Process post\n",
    "                    if process_post(post, f\"search:{query}\", conn):\n",
    "                        collected += 1\n",
    "                        posts_in_range += 1\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                # Update progress with date info\n",
    "                if page_count % 5 == 0:\n",
    "                    pbar.set_description(\n",
    "                        f\"Search: {query} [{oldest_date_seen.strftime('%Y-%m-%d')} \"\n",
    "                        f\"to {newest_date_seen.strftime('%Y-%m-%d')}]\"\n",
    "                    )\n",
    "                \n",
    "                # Stop if we've gone too far back in time\n",
    "                if consecutive_old_posts > 100:\n",
    "                    print(f\"    Reached posts before {START_DATE.date()}, stopping\")\n",
    "                    break\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(random.uniform(1, 3))\n",
    "                \n",
    "                # Stop after many pages to avoid getting blocked\n",
    "                if page_count >= 50:\n",
    "                    print(f\"    Reached page limit (50)\")\n",
    "                    break\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"  Search error: {str(e)[:100]}...\")\n",
    "        \n",
    "    print(f\"  âœ“ Collected {collected} posts spanning \"\n",
    "          f\"{oldest_date_seen.strftime('%Y-%m-%d')} to \"\n",
    "          f\"{newest_date_seen.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    return collected\n",
    "\n",
    "def collect_from_accounts_comprehensive(account_list, conn):\n",
    "    \"\"\"Collect ALL posts within date range from accounts\"\"\"\n",
    "    total_collected = 0\n",
    "    \n",
    "    # First, verify which accounts actually exist\n",
    "    print(\"\\nğŸ” Verifying accounts...\")\n",
    "    valid_accounts = []\n",
    "    \n",
    "    for handle in account_list:\n",
    "        verified = verify_account(handle)\n",
    "        if verified:\n",
    "            valid_accounts.append(verified)\n",
    "            print(f\"  âœ“ {handle} â†’ @{verified}\")\n",
    "        else:\n",
    "            print(f\"  âœ— {handle} not found\")\n",
    "        time.sleep(random.uniform(0.5, 1))\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_accounts)} valid accounts\")\n",
    "    \n",
    "    # Collect from each valid account\n",
    "    for idx, handle in enumerate(valid_accounts, 1):\n",
    "        try:\n",
    "            print(f\"\\n[{idx}/{len(valid_accounts)}] Collecting from @{handle}\")\n",
    "            \n",
    "            # Pull ALL statuses from account\n",
    "            gen = api.pull_statuses(username=handle, replies=False, verbose=False)\n",
    "            \n",
    "            collected = 0\n",
    "            posts_checked = 0\n",
    "            oldest_date = END_DATE\n",
    "            newest_date = START_DATE\n",
    "            posts_before_range = 0\n",
    "            \n",
    "            with tqdm(desc=f\"@{handle}\", unit=\"posts\") as pbar:\n",
    "                for post in gen:\n",
    "                    posts_checked += 1\n",
    "                    \n",
    "                    # Check if we've gone before our date range\n",
    "                    post_date_str = post.get('created_at', '')\n",
    "                    try:\n",
    "                        post_date = dt_parse.isoparse(post_date_str)\n",
    "                        if post_date.tzinfo is None:\n",
    "                            post_date = post_date.replace(tzinfo=timezone.utc)\n",
    "                            \n",
    "                        if post_date < oldest_date:\n",
    "                            oldest_date = post_date\n",
    "                        if post_date > newest_date:\n",
    "                            newest_date = post_date\n",
    "                            \n",
    "                        if post_date < START_DATE:\n",
    "                            posts_before_range += 1\n",
    "                            # If we've seen 50 posts before our range, stop\n",
    "                            if posts_before_range > 50:\n",
    "                                print(f\"    Reached posts before {START_DATE.date()}\")\n",
    "                                break\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Process post\n",
    "                    if process_post(post, f\"account:{handle}\", conn):\n",
    "                        collected += 1\n",
    "                        total_collected += 1\n",
    "                        pbar.update(1)\n",
    "                    \n",
    "                    # Update progress\n",
    "                    if posts_checked % 100 == 0:\n",
    "                        pbar.set_description(\n",
    "                            f\"@{handle} [checked:{posts_checked} collected:{collected}]\"\n",
    "                        )\n",
    "                    \n",
    "                    # Brief pause every 100 posts\n",
    "                    if posts_checked % 100 == 0:\n",
    "                        time.sleep(random.uniform(1, 2))\n",
    "                    \n",
    "                    # Stop if we've checked too many (to avoid blocks)\n",
    "                    if posts_checked >= 2000:\n",
    "                        print(f\"    Reached post limit (2000)\")\n",
    "                        break\n",
    "            \n",
    "            print(f\"  âœ“ Collected {collected} matching posts from {posts_checked} checked\")\n",
    "            print(f\"    Date range: {oldest_date.strftime('%Y-%m-%d')} to {newest_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Longer pause between accounts\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Error with @{handle}: {str(e)[:100]}...\")\n",
    "            continue\n",
    "            \n",
    "    return total_collected\n",
    "\n",
    "# ---------- Main Collection Function ---------------------------------------\n",
    "def run_comprehensive_collection():\n",
    "    \"\"\"Run comprehensive collection for entire war period\"\"\"\n",
    "    conn = setup_database()\n",
    "    total_collected = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Truth Social Ukraine/Russia Comprehensive Collection\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "    print(f\"Keywords: {len(KEYWORDS)}\")\n",
    "    print(f\"Target accounts: {len(VERIFIED_ACCOUNTS) + len(POSSIBLE_ACCOUNTS)}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Deep search across all keywords\n",
    "        print(\"ğŸ“¡ PHASE 1: Deep historical search\")\n",
    "        print(\"Searching through entire war timeline...\\n\")\n",
    "        \n",
    "        search_queries = [\n",
    "            # Primary searches\n",
    "            \"ukraine\", \"russia\", \"putin\", \"zelensky\", \"war ukraine\",\n",
    "            \"ukraine invasion\", \"special operation\", \"ukraine russia\",\n",
    "            \n",
    "            # Geopolitical searches\n",
    "            \"nato ukraine\", \"ukraine aid\", \"sanctions russia\", \"grain deal\",\n",
    "            \"nordstream\", \"crimea\", \"donbas\", \"mariupol\",\n",
    "            \n",
    "            # Narrative searches\n",
    "            \"denazification\", \"biolabs ukraine\", \"proxy war ukraine\",\n",
    "            \"ukraine corruption\", \"zelensky dictator\", \"putin war\"\n",
    "        ]\n",
    "        \n",
    "        search_collected = 0\n",
    "        for idx, query in enumerate(search_queries, 1):\n",
    "            print(f\"[{idx}/{len(search_queries)}] Query: '{query}'\")\n",
    "            collected = collect_via_search_deep(query, conn)\n",
    "            search_collected += collected\n",
    "            total_collected += collected\n",
    "            \n",
    "            # Show running stats\n",
    "            elapsed = (time.time() - start_time) / 60\n",
    "            rate = total_collected / elapsed if elapsed > 0 else 0\n",
    "            print(f\"  ğŸ“Š Running total: {total_collected} posts \"\n",
    "                  f\"({elapsed:.1f} min, {rate:.1f} posts/min)\\n\")\n",
    "            \n",
    "            # Pause between searches\n",
    "            if idx < len(search_queries):\n",
    "                sleep_time = random.uniform(10, 20)\n",
    "                print(f\"  ğŸ’¤ Pausing {sleep_time:.0f}s...\\n\")\n",
    "                time.sleep(sleep_time)\n",
    "        \n",
    "        print(f\"\\nâœ“ Phase 1 complete: {search_collected} posts from searches\")\n",
    "        \n",
    "        # Phase 2: Comprehensive account collection\n",
    "        print(\"\\nğŸ“¡ PHASE 2: Account-based collection\")\n",
    "        print(\"Collecting from verified accounts...\\n\")\n",
    "        \n",
    "        # Combine and deduplicate accounts\n",
    "        all_accounts = list(set(VERIFIED_ACCOUNTS + POSSIBLE_ACCOUNTS))\n",
    "        \n",
    "        account_collected = collect_from_accounts_comprehensive(all_accounts, conn)\n",
    "        total_collected += account_collected\n",
    "        \n",
    "        print(f\"\\nâœ“ Phase 2 complete: {account_collected} posts from accounts\")\n",
    "        \n",
    "        # Phase 3: Timeline scanning (brief)\n",
    "        print(\"\\nğŸ“¡ PHASE 3: Recent timeline scan\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Scanning recent public posts...\")\n",
    "            timeline_gen = api.trending()  # Get trending posts\n",
    "            timeline_collected = 0\n",
    "            \n",
    "            for post in timeline_gen:\n",
    "                if isinstance(post, dict) and 'content' in post:\n",
    "                    if process_post(post, \"trending\", conn):\n",
    "                        timeline_collected += 1\n",
    "                        total_collected += 1\n",
    "                        \n",
    "                if timeline_collected >= 100:  # Just get recent ones\n",
    "                    break\n",
    "                    \n",
    "            print(f\"âœ“ Phase 3 complete: {timeline_collected} trending posts\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Timeline error: {str(e)[:100]}...\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nâ¹ Collection interrupted - saving data...\")\n",
    "        \n",
    "    finally:\n",
    "        # Final statistics\n",
    "        elapsed_total = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COLLECTION COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total posts collected: {total_collected:,}\")\n",
    "        print(f\"Time taken: {elapsed_total:.1f} minutes ({elapsed_total/60:.1f} hours)\")\n",
    "        print(f\"Average rate: {total_collected/elapsed_total:.1f} posts/minute\")\n",
    "        \n",
    "        # Analyze date coverage\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT \n",
    "                MIN(created_at) as earliest,\n",
    "                MAX(created_at) as latest,\n",
    "                COUNT(DISTINCT created_date) as unique_days,\n",
    "                COUNT(DISTINCT account) as unique_accounts,\n",
    "                COUNT(*) as total_posts\n",
    "            FROM posts\n",
    "        ''')\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[0]:\n",
    "            print(f\"\\nDate coverage:\")\n",
    "            print(f\"  Earliest post: {result[0][:10]}\")\n",
    "            print(f\"  Latest post: {result[1][:10]}\")\n",
    "            print(f\"  Unique days: {result[2]}\")\n",
    "            print(f\"  Unique accounts: {result[3]}\")\n",
    "            \n",
    "            # Posts per month\n",
    "            cursor.execute('''\n",
    "                SELECT \n",
    "                    SUBSTR(created_date, 1, 7) as month,\n",
    "                    COUNT(*) as post_count\n",
    "                FROM posts\n",
    "                GROUP BY SUBSTR(created_date, 1, 7)\n",
    "                ORDER BY month\n",
    "            ''')\n",
    "            \n",
    "            print(f\"\\nPosts by month:\")\n",
    "            for month, count in cursor.fetchall():\n",
    "                print(f\"  {month}: {count:,} posts\")\n",
    "        \n",
    "        # Export to CSV\n",
    "        export_to_csv(conn)\n",
    "        conn.close()\n",
    "\n",
    "def export_to_csv(conn):\n",
    "    \"\"\"Export database to CSV with metadata\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT * FROM posts ORDER BY created_at DESC')\n",
    "    \n",
    "    ROOT = pathlib.Path.cwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = ROOT / \"outputs\" / f\"truth_social_ukraine_comprehensive_{timestamp}.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([desc[0] for desc in cursor.description])\n",
    "        writer.writerows(cursor.fetchall())\n",
    "    \n",
    "    print(f\"\\nğŸ“„ CSV exported to: {out_path}\")\n",
    "\n",
    "# Run the collection\n",
    "if __name__ == \"__main__\":\n",
    "    run_comprehensive_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Second Pass with Account Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Complete Ukraine/Russia Account Discovery & Historical Scraper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, re, csv, pathlib, random, time, json, sys\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "\n",
    "# ========== TUNABLE PARAMETERS FOR SPEED/DEPTH ==========\n",
    "# Adjust these to control speed vs thoroughness:\n",
    "\n",
    "# Search parameters\n",
    "SEARCH_PAGES_PER_QUERY = 30    # Increase to 20-50 for deeper search\n",
    "SEARCH_LIMIT_PER_PAGE = 70      # Max 100 for faster discovery\n",
    "SEARCH_DELAY = 0.5              # Decrease to 0.1 for faster (risky)\n",
    "\n",
    "# Reply collection\n",
    "COLLECT_REPLIES = True          # Set False to skip reply collection (faster)\n",
    "MAX_REPLIES_PER_POST = 500      # Increase for more thorough comment mining\n",
    "REPLY_DELAY = 0.3               # Decrease to 0.1 for faster\n",
    "\n",
    "# Historical scraping\n",
    "MAX_POSTS_PER_ACCOUNT = 5000    # Increase to 10000+ for complete history\n",
    "ACCOUNT_SCRAPE_DELAY = 1        # Decrease to 0.5 for faster\n",
    "BATCH_PROCESS_SIZE = 500        # How often to show progress\n",
    "\n",
    "# Date range\n",
    "START_DATE = \"2022-02-20\"       # Beginning of data collection\n",
    "END_DATE = \"2025-06-05\"         # End of data collection\n",
    "\n",
    "# ========== KEYWORDS ==========\n",
    "KEYWORDS = [\n",
    "    \"ukraine\", \"ukrainian\", \"ÑƒĞºÑ€Ğ°Ñ—Ğ½Ğ°\", \"ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¸Ğ¹\", \"kyiv\", \"kiev\",\n",
    "    \"russia\", \"russian\", \"Ñ€Ğ¾ÑÑĞ¸Ñ\", \"Ñ€ÑƒÑÑĞºĞ¸Ğ¹\", \"moscow\", \"kremlin\",\n",
    "    \"putin\", \"Ğ¿ÑƒÑ‚Ğ¸Ğ½\", \"zelensky\", \"zelenskyy\", \"Ğ·ĞµĞ»ĞµĞ½ÑÑŒĞºĞ¸Ğ¹\",\n",
    "    \"war\", \"invasion\", \"conflict\", \"special operation\", \"smo\",\n",
    "    \"nato\", \"sanctions\", \"donbas\", \"crimea\", \"mariupol\", \"bakhmut\"\n",
    "]\n",
    "\n",
    "SEARCH_QUERIES = [\n",
    "    \"ukraine\", \"russia\", \"putin\", \"zelensky\", \"ukraine war\",\n",
    "    \"special operation\", \"ukraine invasion\", \"nato ukraine\",\n",
    "    \"sanctions russia\", \"ukraine aid\", \"crimea\", \"donbas\",\n",
    "    \"ukraine conflict\", \"russia invasion\", \"ukrainian\"\n",
    "]\n",
    "\n",
    "# ========== DATABASE SETUP ==========\n",
    "def setup_database():\n",
    "    db_path = pathlib.Path(\"ukraine_complete_scrape.db\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Posts table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            created_at TEXT,\n",
    "            account TEXT,\n",
    "            text TEXT,\n",
    "            in_reply_to_id TEXT,\n",
    "            collection_method TEXT,\n",
    "            keywords_matched TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Accounts table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS accounts (\n",
    "            account TEXT PRIMARY KEY,\n",
    "            discovered_from TEXT,\n",
    "            discovery_date TEXT,\n",
    "            is_active_poster INTEGER,\n",
    "            is_commenter INTEGER,\n",
    "            total_posts INTEGER DEFAULT 0\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# ========== HELPER FUNCTIONS ==========\n",
    "def clean_text(html):\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', html)\n",
    "    text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def matches_keywords(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    return [k for k in KEYWORDS if k.lower() in text_lower]\n",
    "\n",
    "def save_account(conn, account_name, source, is_poster=False, is_commenter=False):\n",
    "    \"\"\"Save discovered account to database\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT OR REPLACE INTO accounts (account, discovered_from, discovery_date, is_active_poster, is_commenter)\n",
    "        VALUES (?, ?, ?, \n",
    "                COALESCE((SELECT is_active_poster FROM accounts WHERE account = ?), ?),\n",
    "                COALESCE((SELECT is_commenter FROM accounts WHERE account = ?), ?))\n",
    "    ''', (account_name, source, datetime.now().isoformat(), \n",
    "          account_name, int(is_poster), account_name, int(is_commenter)))\n",
    "    conn.commit()\n",
    "\n",
    "def save_post(conn, post, method):\n",
    "    \"\"\"Save post to database\"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        text = clean_text(post.get('content', ''))\n",
    "        keywords = ','.join(matches_keywords(text)[:5])\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT OR IGNORE INTO posts \n",
    "            (id, created_at, account, text, in_reply_to_id, collection_method, keywords_matched)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (post.get('id'), post.get('created_at'), \n",
    "              post.get('account', {}).get('acct'), text,\n",
    "              post.get('in_reply_to_id'), method, keywords))\n",
    "        conn.commit()\n",
    "        return cursor.rowcount > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# ========== PHASE 1: DISCOVERY ==========\n",
    "def discover_all_accounts(conn):\n",
    "    \"\"\"Discover accounts via search and replies\"\"\"\n",
    "    discovered = set()\n",
    "    posts_with_replies = []\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"PHASE 1: ACCOUNT DISCOVERY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Search discovery\n",
    "    for query_idx, query in enumerate(SEARCH_QUERIES, 1):\n",
    "        print(f\"\\n[{query_idx}/{len(SEARCH_QUERIES)}] Searching: '{query}'\")\n",
    "        posts_found = 0\n",
    "        accounts_found = 0\n",
    "        \n",
    "        try:\n",
    "            # Try to get multiple pages of results\n",
    "            total_pages = 0\n",
    "            max_id = None\n",
    "            \n",
    "            for page_attempt in range(SEARCH_PAGES_PER_QUERY):\n",
    "                try:\n",
    "                    # Create search with pagination support\n",
    "                    search_gen = api.search(\"statuses\", query, limit=SEARCH_LIMIT_PER_PAGE)\n",
    "                    \n",
    "                    # Convert generator to list to see what we get\n",
    "                    results = list(search_gen)\n",
    "                    \n",
    "                    if not results:\n",
    "                        break\n",
    "                    \n",
    "                    # Process each page in results\n",
    "                    for page in results:\n",
    "                        if not page or 'statuses' not in page:\n",
    "                            continue\n",
    "                            \n",
    "                        statuses = page.get('statuses', [])\n",
    "                        if not statuses:\n",
    "                            continue\n",
    "                            \n",
    "                        total_pages += 1\n",
    "                        \n",
    "                        for post in statuses:\n",
    "                            posts_found += 1\n",
    "                            \n",
    "                            # Get poster account\n",
    "                            account = post.get('account', {})\n",
    "                            if account.get('acct'):\n",
    "                                acct = account['acct']\n",
    "                                discovered.add(acct)\n",
    "                                save_account(conn, acct, f\"search:{query}\", is_poster=True)\n",
    "                                accounts_found += 1\n",
    "                            \n",
    "                            # Save post\n",
    "                            save_post(conn, post, f\"search:{query}\")\n",
    "                            \n",
    "                            # Track posts that might have replies\n",
    "                            if post.get('replies_count', 0) > 0:\n",
    "                                posts_with_replies.append((post['id'], acct))\n",
    "                            \n",
    "                            # Get mentioned accounts\n",
    "                            for mention in post.get('mentions', []):\n",
    "                                if mention.get('acct'):\n",
    "                                    discovered.add(mention['acct'])\n",
    "                                    save_account(conn, mention['acct'], f\"mention_in:{query}\")\n",
    "                        \n",
    "                        # Update last ID for potential pagination\n",
    "                        if statuses:\n",
    "                            max_id = statuses[-1].get('id')\n",
    "                    \n",
    "                    # If we only got one page, search might not paginate\n",
    "                    if total_pages <= 1:\n",
    "                        break\n",
    "                        \n",
    "                except StopIteration:\n",
    "                    break\n",
    "                \n",
    "                time.sleep(random.uniform(SEARCH_DELAY * 0.5, SEARCH_DELAY * 1.5))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Search error: {str(e)[:50]}\")            \n",
    "        print(f\"  âœ“ Found {posts_found} posts, {accounts_found} poster accounts\")\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total discovered from search: {len(discovered)} accounts\")\n",
    "    \n",
    "    # Reply discovery\n",
    "        if COLLECT_REPLIES and posts_with_replies:\n",
    "            print(f\"\\nğŸ” Collecting replies from {len(posts_with_replies)} posts...\")\n",
    "            reply_accounts = 0\n",
    "            \n",
    "            with tqdm(total=min(len(posts_with_replies), 100), desc=\"Getting replies\") as pbar:\n",
    "                for idx, (post_id, original_poster) in enumerate(posts_with_replies[:100]):\n",
    "                    try:\n",
    "                        # Use pull_comments to get replies\n",
    "                        comments = list(api.pull_comments(post_id=post_id))\n",
    "                        \n",
    "                        for comment in comments[:MAX_REPLIES_PER_POST]:\n",
    "                            # Each comment is a status/post object\n",
    "                            comment_account = comment.get('account', {})\n",
    "                            if comment_account.get('acct'):\n",
    "                                acct = comment_account['acct']\n",
    "                                if acct not in discovered:\n",
    "                                    discovered.add(acct)\n",
    "                                    save_account(conn, acct, f\"reply_to:{original_poster}\", is_commenter=True)\n",
    "                                    reply_accounts += 1\n",
    "                                \n",
    "                                # Save comment as post\n",
    "                                save_post(conn, comment, f\"reply_to:{post_id}\")\n",
    "                        \n",
    "                        time.sleep(random.uniform(REPLY_DELAY * 0.5, REPLY_DELAY * 1.5))\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        # Print error for first few to debug\n",
    "                        if idx < 3:\n",
    "                            print(f\"\\n  Comment error for {post_id}: {str(e)[:50]}\")\n",
    "                            \n",
    "        print(f\"  âœ“ Found {reply_accounts} additional commenting accounts\")\n",
    "    \n",
    "    print(f\"\\nâœ… TOTAL UNIQUE ACCOUNTS DISCOVERED: {len(discovered)}\")\n",
    "    return list(discovered)\n",
    "\n",
    "# ========== PHASE 2: HISTORICAL SCRAPING ==========\n",
    "def scrape_historical_data(account_list, conn):\n",
    "    \"\"\"Scrape historical posts from all discovered accounts\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2: HISTORICAL DATA COLLECTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Scraping {len(account_list)} accounts back to {START_DATE}\\n\")\n",
    "    \n",
    "    total_posts = 0\n",
    "    accounts_scraped = 0\n",
    "    \n",
    "    for idx, handle in enumerate(account_list, 1):\n",
    "        try:\n",
    "            # Clean the handle - remove @ and any whitespace\n",
    "            handle_clean = handle.strip().lstrip('@')\n",
    "            \n",
    "            # Verify account exists using lookup\n",
    "            try:\n",
    "                user_info = api.lookup(user_handle=handle_clean)\n",
    "                if not user_info:\n",
    "                    print(f\" [NOT FOUND]\")\n",
    "                    continue\n",
    "            except Exception as lookup_error:\n",
    "                print(f\" [LOOKUP ERROR]\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"[{idx}/{len(account_list)}] @{handle_clean}\", end='', flush=True)\n",
    "            \n",
    "            # Pull statuses using clean handle\n",
    "            gen = api.pull_statuses(username=handle_clean, replies=False, verbose=False)\n",
    "            \n",
    "            account_posts = 0\n",
    "            for post in gen:\n",
    "                # Date check\n",
    "                created_at = post.get('created_at', '')\n",
    "                if created_at < START_DATE:\n",
    "                    break\n",
    "                if created_at > END_DATE:\n",
    "                    continue\n",
    "                \n",
    "                # Quick keyword check\n",
    "                text = clean_text(post.get('content', ''))\n",
    "                if matches_keywords(text):\n",
    "                    if save_post(conn, post, f\"historical:{handle}\"):\n",
    "                        account_posts += 1\n",
    "                        total_posts += 1\n",
    "                    \n",
    "                    # Progress indicator\n",
    "                    if account_posts % BATCH_PROCESS_SIZE == 0:\n",
    "                        print(f\" {account_posts}\", end='', flush=True)\n",
    "                \n",
    "                # Stop if hit limit\n",
    "                if account_posts >= MAX_POSTS_PER_ACCOUNT:\n",
    "                    break\n",
    "            \n",
    "            print(f\" â†’ {account_posts} posts\")\n",
    "            accounts_scraped += 1\n",
    "            \n",
    "            # Update account post count\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('UPDATE accounts SET total_posts = ? WHERE account = ?', \n",
    "                         (account_posts, handle))\n",
    "            conn.commit()\n",
    "            \n",
    "            time.sleep(random.uniform(ACCOUNT_SCRAPE_DELAY * 0.5, ACCOUNT_SCRAPE_DELAY * 1.5))\n",
    "\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nâ¹ Interrupted by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR: {str(e)[:30]}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Scraped {accounts_scraped} accounts, collected {total_posts} posts\")\n",
    "    return total_posts\n",
    "\n",
    "# ========== EXPORT FUNCTIONS ==========\n",
    "def export_accounts_csv(conn):\n",
    "    \"\"\"Export all discovered accounts to CSV\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        SELECT account, discovered_from, discovery_date, \n",
    "               is_active_poster, is_commenter, total_posts\n",
    "        FROM accounts\n",
    "        ORDER BY total_posts DESC\n",
    "    ''')\n",
    "    \n",
    "    ROOT = pathlib.Path.cwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = ROOT / \"outputs\" / f\"ukraine_accounts_{timestamp}.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['account', 'discovered_from', 'discovery_date', \n",
    "                        'is_poster', 'is_commenter', 'total_posts'])\n",
    "        writer.writerows(cursor.fetchall())\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Accounts CSV: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "def export_posts_csv(conn):\n",
    "    \"\"\"Export all posts to CSV\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT * FROM posts ORDER BY created_at DESC')\n",
    "    \n",
    "    ROOT = pathlib.Path.cwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = ROOT / \"outputs\" / f\"ukraine_posts_{timestamp}.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'created_at', 'account', 'text', \n",
    "                        'in_reply_to_id', 'collection_method', 'keywords_matched'])\n",
    "        writer.writerows(cursor.fetchall())\n",
    "    \n",
    "    print(f\"ğŸ“„ Posts CSV: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# ========== MAIN EXECUTION ==========\n",
    "def run_complete_scrape():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    start_time = time.time()\n",
    "    conn = setup_database()\n",
    "    \n",
    "    try:\n",
    "        # Phase 1: Discover all accounts\n",
    "        discovered_accounts = discover_all_accounts(conn)\n",
    "        \n",
    "        # Export accounts list\n",
    "        export_accounts_csv(conn)\n",
    "        \n",
    "        # Phase 2: Historical scraping\n",
    "        if discovered_accounts:\n",
    "            scrape_historical_data(discovered_accounts, conn)\n",
    "        \n",
    "        # Final statistics\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('SELECT COUNT(DISTINCT account) FROM accounts')\n",
    "        total_accounts = cursor.fetchone()[0]\n",
    "        \n",
    "        cursor.execute('SELECT COUNT(*) FROM posts')\n",
    "        total_posts = cursor.fetchone()[0]\n",
    "        \n",
    "        elapsed = (time.time() - start_time) / 60\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"COLLECTION COMPLETE\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total accounts found: {total_accounts}\")\n",
    "        print(f\"Total posts collected: {total_posts}\")\n",
    "        print(f\"Time taken: {elapsed:.1f} minutes\")\n",
    "        print(f\"Rate: {total_posts/elapsed:.1f} posts/minute\")\n",
    "        \n",
    "        # Export final data\n",
    "        export_posts_csv(conn)\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Run the scraper\n",
    "if __name__ == \"__main__\":\n",
    "    run_complete_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Historical Scraper with Smart Rate Limiting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import pandas as pd\n",
    "import os, re, csv, pathlib, random, time, json, sys\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "import sqlite3\n",
    "import truthbrush as tb\n",
    "\n",
    "# ========== INITIALIZE API FIRST ==========\n",
    "api = tb.api.Api()\n",
    "print(\"âœ“ API initialized\")\n",
    "\n",
    "# ========== INSTALL SMART RATE LIMITER ==========\n",
    "# This is the key to faster scraping!\n",
    "if not hasattr(tb.api.Api, \"_get_base\"):\n",
    "    tb.api.Api._get_base = tb.api.Api._get\n",
    "\n",
    "    def _polite_get(self, url, params=None):\n",
    "        try:\n",
    "            resp = self._get_base(url, params)\n",
    "            \n",
    "            # Check if we got rate limited by Cloudflare\n",
    "            if resp and hasattr(resp, 'text') and 'Error 1015' in str(resp.text):\n",
    "                print(\"\\nğŸš« Cloudflare rate limit detected! Backing off...\")\n",
    "                time.sleep(60)  # Wait 1 minute\n",
    "                return None\n",
    "                \n",
    "            # Only delay if we're near the API rate limit\n",
    "            if (self.ratelimit_remaining is not None\n",
    "                    and self.ratelimit_remaining <= 10\n",
    "                    and self.ratelimit_reset):\n",
    "                wait = max(\n",
    "                    0,\n",
    "                    (self.ratelimit_reset -\n",
    "                     datetime.utcnow().replace(tzinfo=timezone.utc)).total_seconds()\n",
    "                ) + random.uniform(1, 3)\n",
    "                print(f\"\\nğŸ“‰ Near API rate limit - sleeping {wait:.1f}s\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Add small delay to avoid Cloudflare triggers\n",
    "                time.sleep(random.uniform(0.3, 0.5))  # Slower to avoid Cloudflare\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            if \"1015\" in str(e) or \"rate limit\" in str(e).lower():\n",
    "                print(\"\\nğŸš« Rate limited - waiting 60 seconds...\")\n",
    "                time.sleep(60)\n",
    "            raise\n",
    "\n",
    "    tb.api.Api._get = _polite_get\n",
    "    print(\"âœ“ Smart rate limiter installed\")\n",
    "else:\n",
    "    print(\"âœ“ Rate limiter already installed\")\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "MAX_POSTS_PER_ACCOUNT = 5000    \n",
    "ACCOUNT_SCRAPE_DELAY = 0.1      # Minimal delay between accounts\n",
    "BATCH_SIZE = 100                # Batch database commits\n",
    "\n",
    "# Date range\n",
    "START_DATE = \"2022-02-20\"       \n",
    "END_DATE = \"2025-06-05\"         \n",
    "\n",
    "# Keywords\n",
    "KEYWORDS = [\n",
    "    \"ukraine\", \"ukrainian\", \"ÑƒĞºÑ€Ğ°Ñ—Ğ½Ğ°\", \"ÑƒĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ¸Ğ¹\", \"kyiv\", \"kiev\",\n",
    "    \"russia\", \"russian\", \"Ñ€Ğ¾ÑÑĞ¸Ñ\", \"Ñ€ÑƒÑÑĞºĞ¸Ğ¹\", \"moscow\", \"kremlin\",\n",
    "    \"putin\", \"Ğ¿ÑƒÑ‚Ğ¸Ğ½\", \"zelensky\", \"zelenskyy\", \"Ğ·ĞµĞ»ĞµĞ½ÑÑŒĞºĞ¸Ğ¹\",\n",
    "    \"war\", \"invasion\", \"conflict\", \"special operation\", \"smo\",\n",
    "    \"nato\", \"sanctions\", \"donbas\", \"crimea\", \"mariupol\", \"bakhmut\"\n",
    "]\n",
    "\n",
    "# Path to your existing accounts CSV\n",
    "ACCOUNTS_CSV = \"/Users/willbeeson/Projects/Courses/GOV20/ukraine-final-project/notebooks/outputs/ukraine_accounts_20250605_141035.csv\"\n",
    "\n",
    "# High-value accounts\n",
    "HIGH_VALUE_ACCOUNTS = [\n",
    "    \"realDonaldTrump\", \"DevinNunes\", \"DonaldJTrumpJr\", \"EricTrump\",\n",
    "    \"TuckerCarlson\", \"DineshDSouza\", \"JackPosobiec\", \"WarRoom\",\n",
    "    \"BreitbartNews\", \"OANN\", \"KariLake\", \"MattGaetz\"\n",
    "]\n",
    "\n",
    "# ========== OPTIMIZED HELPER FUNCTIONS ==========\n",
    "def setup_database():\n",
    "    db_path = pathlib.Path(\"ukraine_historical_scrape.db\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL\")  # Faster writes\n",
    "    conn.execute(\"PRAGMA synchronous=NORMAL\")  # Less disk sync\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS posts (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            created_at TEXT,\n",
    "            account TEXT,\n",
    "            text TEXT,\n",
    "            in_reply_to_id TEXT,\n",
    "            collection_method TEXT,\n",
    "            keywords_matched TEXT,\n",
    "            scraped_at TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS scrape_status (\n",
    "            account TEXT PRIMARY KEY,\n",
    "            posts_found INTEGER,\n",
    "            posts_collected INTEGER,\n",
    "            status TEXT,\n",
    "            last_updated TEXT\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def clean_text(html):\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', ' ', html)\n",
    "    text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "# Precompile keyword patterns for faster matching\n",
    "KEYWORD_PATTERNS = [k.lower() for k in KEYWORDS]\n",
    "\n",
    "def matches_keywords(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    text_lower = text.lower()\n",
    "    return [k for k in KEYWORD_PATTERNS if k in text_lower]\n",
    "\n",
    "def save_posts_batch(conn, posts_batch):\n",
    "    \"\"\"Save multiple posts at once - much faster!\"\"\"\n",
    "    if not posts_batch:\n",
    "        return 0\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    data = []\n",
    "    \n",
    "    for post, method in posts_batch:\n",
    "        text = clean_text(post.get('content', ''))\n",
    "        keywords = ','.join(matches_keywords(text)[:5])\n",
    "        \n",
    "        data.append((\n",
    "            post.get('id'), \n",
    "            post.get('created_at'), \n",
    "            post.get('account', {}).get('acct'), \n",
    "            text,\n",
    "            post.get('in_reply_to_id'), \n",
    "            method, \n",
    "            keywords,\n",
    "            datetime.now().isoformat()\n",
    "        ))\n",
    "    \n",
    "    cursor.executemany('''\n",
    "        INSERT OR IGNORE INTO posts \n",
    "        (id, created_at, account, text, in_reply_to_id, collection_method, keywords_matched, scraped_at)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', data)\n",
    "    \n",
    "    conn.commit()\n",
    "    return cursor.rowcount\n",
    "\n",
    "def update_scrape_status(conn, account, posts_found, posts_collected, status):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        INSERT OR REPLACE INTO scrape_status \n",
    "        (account, posts_found, posts_collected, status, last_updated)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (account, posts_found, posts_collected, status, datetime.now().isoformat()))\n",
    "    conn.commit()\n",
    "\n",
    "# ========== MAIN SCRAPING FUNCTION ==========\n",
    "def scrape_accounts_from_csv():\n",
    "    \"\"\"Load accounts from CSV and scrape their historical posts\"\"\"\n",
    "    \n",
    "    # Load existing accounts\n",
    "    print(f\"Loading accounts from: {ACCOUNTS_CSV}\")\n",
    "    accounts_df = pd.read_csv(ACCOUNTS_CSV)\n",
    "    existing_accounts = accounts_df['account'].tolist()\n",
    "    \n",
    "    # Add high-value accounts\n",
    "    all_accounts = list(set(existing_accounts + HIGH_VALUE_ACCOUNTS))\n",
    "    \n",
    "    print(f\"Total accounts to scrape: {len(all_accounts)}\")\n",
    "    print(f\"  - From CSV: {len(existing_accounts)}\")\n",
    "    print(f\"  - High-value additions: {len(set(HIGH_VALUE_ACCOUNTS) - set(existing_accounts))}\")\n",
    "    \n",
    "    # Setup database\n",
    "    conn = setup_database()\n",
    "    print(\"âœ“ Database ready\")\n",
    "    \n",
    "    # Check if we've already scraped some accounts\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT account FROM scrape_status WHERE status = \"completed\"')\n",
    "    already_scraped = {row[0] for row in cursor.fetchall()}\n",
    "    \n",
    "    if already_scraped:\n",
    "        print(f\"\\nResuming: {len(already_scraped)} accounts already scraped\")\n",
    "        accounts_to_scrape = [a for a in all_accounts if a not in already_scraped]\n",
    "    else:\n",
    "        accounts_to_scrape = all_accounts\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HISTORICAL DATA COLLECTION\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accounts to scrape: {len(accounts_to_scrape)}\")\n",
    "    print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Max posts per account: {MAX_POSTS_PER_ACCOUNT}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Global stats\n",
    "    global_start_time = time.time()\n",
    "    total_posts_all_accounts = 0\n",
    "    successful_accounts = 0\n",
    "    \n",
    "    for idx, account in enumerate(accounts_to_scrape, 1):\n",
    "        try:\n",
    "            # Clean handle\n",
    "            handle = account.strip().lstrip('@')\n",
    "            \n",
    "            # Show what we're doing (like original)\n",
    "            print(f\"[{idx}/{len(accounts_to_scrape)}] @{handle}\", end='', flush=True)\n",
    "            \n",
    "            # Verify account\n",
    "            try:\n",
    "                user_info = api.lookup(user_handle=handle)\n",
    "                if not user_info:\n",
    "                    print(\" [NOT FOUND]\")\n",
    "                    update_scrape_status(conn, handle, 0, 0, \"not_found\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(\" [LOOKUP ERROR]\")\n",
    "                update_scrape_status(conn, handle, 0, 0, \"lookup_error\")\n",
    "                continue\n",
    "            \n",
    "            # Space for progress bar\n",
    "            print(\"\")  \n",
    "            \n",
    "            # Pull statuses\n",
    "            print(f\"  â†’ Sending first request...\", end='', flush=True)\n",
    "            gen = api.pull_statuses(username=handle, replies=False, verbose=False)\n",
    "            \n",
    "            # Try to get first post to verify generator works\n",
    "            try:\n",
    "                first_post = next(gen)\n",
    "                print(\" âœ“ Connected\")\n",
    "            except StopIteration:\n",
    "                print(\" âš ï¸ No posts found\")\n",
    "                update_scrape_status(conn, handle, 0, 0, \"no_posts\")\n",
    "                continue\n",
    "            \n",
    "            # Prepend first post back\n",
    "            def prepend_first(item, iterator):\n",
    "                yield item\n",
    "                yield from iterator\n",
    "            gen = prepend_first(first_post, gen)\n",
    "            \n",
    "            # Account-specific counters\n",
    "            posts_found = 0\n",
    "            posts_collected = 0\n",
    "            posts_before_date = 0\n",
    "            posts_batch = []\n",
    "            \n",
    "            # Account rate monitoring\n",
    "            account_start = time.time()\n",
    "            \n",
    "            # Progress bar - simpler format to avoid errors\n",
    "            pbar = tqdm(desc=f\"  @{handle}\", unit=\" posts\", position=0, leave=False)\n",
    "            \n",
    "            for post in gen:\n",
    "                posts_found += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Date check\n",
    "                created_at = post.get('created_at', '')\n",
    "                if created_at < START_DATE:\n",
    "                    posts_before_date += 1\n",
    "                    if posts_before_date > 50:\n",
    "                        pbar.set_description(f\"  @{handle} - Reached old posts\")\n",
    "                        break\n",
    "                    continue\n",
    "                    \n",
    "                if created_at > END_DATE:\n",
    "                    continue\n",
    "                \n",
    "                # Keyword check - optimized\n",
    "                text = clean_text(post.get('content', ''))\n",
    "                if text and matches_keywords(text):\n",
    "                    posts_batch.append((post, f\"historical:{handle}\"))\n",
    "                    posts_collected += 1\n",
    "                    total_posts_all_accounts += 1\n",
    "                    \n",
    "                    # Batch save\n",
    "                    if len(posts_batch) >= BATCH_SIZE:\n",
    "                        save_posts_batch(conn, posts_batch)\n",
    "                        posts_batch = []\n",
    "                        pbar.set_description(f\"  @{handle} - {posts_collected} collected\")\n",
    "                \n",
    "                # Update progress with detailed info\n",
    "                if posts_found % 50 == 0:  # Update more frequently\n",
    "                    elapsed = time.time() - account_start\n",
    "                    rate = posts_found / elapsed if elapsed > 0 else 0\n",
    "                    \n",
    "                    global_elapsed = time.time() - global_start_time\n",
    "                    global_rate = total_posts_all_accounts / global_elapsed if global_elapsed > 0 else 0\n",
    "                    \n",
    "                    # Update postfix with current stats\n",
    "                    pbar.set_postfix_str(f\"collected: {posts_collected} | {rate:.1f} posts/s | global: {global_rate:.1f} p/s\")\n",
    "                    \n",
    "                    # Also show date range being processed\n",
    "                    pbar.set_description(f\"  @{handle} [{created_at[:10] if created_at else 'N/A'}]\")\n",
    "                \n",
    "                # Limit check\n",
    "                if posts_collected >= MAX_POSTS_PER_ACCOUNT:\n",
    "                    pbar.set_description(f\"  @{handle} - Hit collection limit\")\n",
    "                    break\n",
    "                    \n",
    "                if posts_found >= 10000:\n",
    "                    pbar.set_description(f\"  @{handle} - Hit scan limit\")\n",
    "                    break\n",
    "            \n",
    "            # Save remaining batch\n",
    "            if posts_batch:\n",
    "                save_posts_batch(conn, posts_batch)\n",
    "            \n",
    "            pbar.close()\n",
    "            \n",
    "            # Account summary (like original format)\n",
    "            account_time = time.time() - account_start\n",
    "            account_rate = posts_found / account_time if account_time > 0 else 0\n",
    "            \n",
    "            print(f\"  âœ“ Collected: {posts_collected}/{posts_found} posts [{account_rate:.1f} posts/s]\")\n",
    "            \n",
    "            # Global progress update\n",
    "            global_elapsed = time.time() - global_start_time\n",
    "            global_rate = total_posts_all_accounts / global_elapsed if global_elapsed > 0 else 0\n",
    "            print(f\"  ğŸ“Š Total progress: {total_posts_all_accounts} posts from {successful_accounts + 1} accounts [{global_rate:.1f} posts/s overall]\\n\")\n",
    "            \n",
    "            successful_accounts += 1\n",
    "            update_scrape_status(conn, handle, posts_found, posts_collected, \"completed\")\n",
    "            \n",
    "            # NO delay between accounts - go straight to next one\n",
    "            # time.sleep(random.uniform(ACCOUNT_SCRAPE_DELAY * 0.5, ACCOUNT_SCRAPE_DELAY * 1.5))\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nâ¹ Interrupted - progress saved\")\n",
    "            # Save any remaining batch\n",
    "            if 'posts_batch' in locals() and posts_batch:\n",
    "                save_posts_batch(conn, posts_batch)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR: {str(e)[:100]}\")\n",
    "            update_scrape_status(conn, handle, 0, 0, f\"error: {str(e)[:50]}\")\n",
    "    \n",
    "    # Final stats\n",
    "    total_time = time.time() - global_start_time\n",
    "    overall_rate = total_posts_all_accounts / total_time if total_time > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COLLECTION COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT COUNT(*) FROM posts')\n",
    "    total_in_db = cursor.fetchone()[0]\n",
    "    \n",
    "    cursor.execute('SELECT COUNT(DISTINCT account) FROM posts')\n",
    "    unique_accounts = cursor.fetchone()[0]\n",
    "    \n",
    "    print(f\"Total posts collected: {total_in_db}\")\n",
    "    print(f\"Unique accounts with posts: {unique_accounts}\")\n",
    "    print(f\"Accounts successfully scraped: {successful_accounts}\")\n",
    "    print(f\"Overall rate: {overall_rate:.1f} posts/second\")\n",
    "    print(f\"Total time: {total_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Export\n",
    "    export_posts_csv(conn)\n",
    "    export_status_csv(conn)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "def export_posts_csv(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT * FROM posts ORDER BY created_at DESC')\n",
    "    \n",
    "    ROOT = pathlib.Path.cwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = ROOT / \"outputs\" / f\"ukraine_historical_posts_{timestamp}.csv\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['id', 'created_at', 'account', 'text', \n",
    "                        'in_reply_to_id', 'collection_method', 'keywords_matched', 'scraped_at'])\n",
    "        writer.writerows(cursor.fetchall())\n",
    "    \n",
    "    print(f\"\\nğŸ“„ Posts CSV: {out_path}\")\n",
    "\n",
    "def export_status_csv(conn):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT * FROM scrape_status ORDER BY posts_collected DESC')\n",
    "    \n",
    "    ROOT = pathlib.Path.cwd()\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_path = ROOT / \"outputs\" / f\"scrape_status_{timestamp}.csv\"\n",
    "    \n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['account', 'posts_found', 'posts_collected', 'status', 'last_updated'])\n",
    "        writer.writerows(cursor.fetchall())\n",
    "    \n",
    "    print(f\"ğŸ“„ Status CSV: {out_path}\")\n",
    "\n",
    "# Run it\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_accounts_from_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this debug function to check available methods:\n",
    "def check_api_methods():\n",
    "    print(\"\\nAvailable API methods:\")\n",
    "    for method in dir(api):\n",
    "        if not method.startswith('_') and callable(getattr(api, method)):\n",
    "            print(f\"  - {method}\")\n",
    "\n",
    "# Call it before Phase 1:\n",
    "check_api_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple Truth Social Ukraine Scraper\n",
    "Just run this single file to collect Ukraine-related posts fast!\n",
    "\"\"\"\n",
    "\n",
    "import truthbrush as tb\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "\n",
    "print(\"ğŸš€ Truth Social Ukraine Speed Scraper\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize API\n",
    "api = tb.api.Api()\n",
    "print(\"âœ“ API connected\")\n",
    "\n",
    "# Keywords to search\n",
    "KEYWORDS = [\"ukraine\", \"russia\", \"putin\", \"zelensky\", \"war\", \"nato\", \"kyiv\", \"invasion\", \"sanctions\", \"missile\"]\n",
    "\n",
    "# Storage\n",
    "all_posts = []\n",
    "seen_ids = set()\n",
    "\n",
    "def clean_text(html):\n",
    "    \"\"\"Remove HTML tags\"\"\"\n",
    "    import re\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    return re.sub(r'<[^>]+>', ' ', html).strip()\n",
    "\n",
    "def save_posts(posts, filename):\n",
    "    \"\"\"Save posts to CSV\"\"\"\n",
    "    if not posts:\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(posts)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"ğŸ’¾ Saved {len(posts)} posts to {filename}\")\n",
    "\n",
    "# MAIN COLLECTION LOOP\n",
    "print(f\"\\nğŸ“¡ Starting collection at {datetime.now()}\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # 1. Search for Ukraine posts\n",
    "    print(\"\\nğŸ” Phase 1: Searching for Ukraine content...\")\n",
    "    \n",
    "    for i, keyword in enumerate(KEYWORDS):\n",
    "        print(f\"  Searching: {keyword} ({i+1}/{len(KEYWORDS)})\")\n",
    "        \n",
    "        try:\n",
    "            # Use the search API\n",
    "            results = api.search(searchtype=\"statuses\", query=keyword, limit=40)\n",
    "            \n",
    "            # Handle the results\n",
    "            if hasattr(results, '__iter__'):\n",
    "                for page in results:\n",
    "                    if isinstance(page, dict) and 'statuses' in page:\n",
    "                        posts = page['statuses']\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    for post in posts:\n",
    "                        if post.get('id') not in seen_ids:\n",
    "                            seen_ids.add(post.get('id'))\n",
    "                            \n",
    "                            # Extract data\n",
    "                            post_data = {\n",
    "                                'id': post.get('id'),\n",
    "                                'created_at': post.get('created_at'),\n",
    "                                'account': post.get('account', {}).get('acct', ''),\n",
    "                                'text': clean_text(post.get('content', '')),\n",
    "                                'url': post.get('url', ''),\n",
    "                                'reblogs_count': post.get('reblogs_count', 0),\n",
    "                                'favourites_count': post.get('favourites_count', 0),\n",
    "                                'keyword': keyword\n",
    "                            }\n",
    "                            all_posts.append(post_data)\n",
    "                    \n",
    "                    # Progress\n",
    "                    if len(all_posts) % 100 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = len(all_posts) / elapsed\n",
    "                        print(f\"    ğŸ“Š {len(all_posts)} posts | {rate:.1f} posts/sec\")\n",
    "                    \n",
    "                    # Stop if we have enough from this keyword\n",
    "                    if len(all_posts) > 500 * (i + 1):\n",
    "                        break\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"    âš ï¸  Error with {keyword}: {str(e)[:50]}\")\n",
    "            continue\n",
    "        \n",
    "        # Small delay between keywords\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # 2. Get trending posts\n",
    "    print(\"\\nğŸ“ˆ Phase 2: Checking trending posts...\")\n",
    "    try:\n",
    "        trends = api.trending(limit=20)\n",
    "        ukraine_trends = 0\n",
    "        \n",
    "        for post in trends:\n",
    "            text = clean_text(post.get('content', ''))\n",
    "            if any(kw in text.lower() for kw in KEYWORDS):\n",
    "                if post.get('id') not in seen_ids:\n",
    "                    seen_ids.add(post.get('id'))\n",
    "                    post_data = {\n",
    "                        'id': post.get('id'),\n",
    "                        'created_at': post.get('created_at'),\n",
    "                        'account': post.get('account', {}).get('acct', ''),\n",
    "                        'text': text,\n",
    "                        'url': post.get('url', ''),\n",
    "                        'reblogs_count': post.get('reblogs_count', 0),\n",
    "                        'favourites_count': post.get('favourites_count', 0),\n",
    "                        'keyword': 'trending'\n",
    "                    }\n",
    "                    all_posts.append(post_data)\n",
    "                    ukraine_trends += 1\n",
    "        \n",
    "        print(f\"  âœ“ Found {ukraine_trends} trending Ukraine posts\")\n",
    "    except:\n",
    "        print(\"  âš ï¸  Could not get trending posts\")\n",
    "    \n",
    "    # 3. Quick user scraping for top accounts\n",
    "    print(\"\\nğŸ‘¥ Phase 3: Checking top Ukraine accounts...\")\n",
    "    \n",
    "    # Find most active accounts from our posts\n",
    "    account_counts = {}\n",
    "    for post in all_posts:\n",
    "        acc = post.get('account', '')\n",
    "        if acc:\n",
    "            account_counts[acc] = account_counts.get(acc, 0) + 1\n",
    "    \n",
    "    # Get top 10 most active accounts\n",
    "    top_accounts = sorted(account_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    for account, count in top_accounts:\n",
    "        print(f\"  Checking @{account} ({count} posts found)...\")\n",
    "        \n",
    "        try:\n",
    "            posts_checked = 0\n",
    "            for post in api.pull_statuses(account, replies=False, verbose=False):\n",
    "                posts_checked += 1\n",
    "                \n",
    "                # Quick Ukraine check\n",
    "                text = clean_text(post.get('content', ''))\n",
    "                if any(kw in text.lower() for kw in KEYWORDS):\n",
    "                    if post.get('id') not in seen_ids:\n",
    "                        seen_ids.add(post.get('id'))\n",
    "                        post_data = {\n",
    "                            'id': post.get('id'),\n",
    "                            'created_at': post.get('created_at'),\n",
    "                            'account': account,\n",
    "                            'text': text,\n",
    "                            'url': post.get('url', ''),\n",
    "                            'reblogs_count': post.get('reblogs_count', 0),\n",
    "                            'favourites_count': post.get('favourites_count', 0),\n",
    "                            'keyword': f'user:{account}'\n",
    "                        }\n",
    "                        all_posts.append(post_data)\n",
    "                \n",
    "                # Don't check too many per user\n",
    "                if posts_checked > 100:\n",
    "                    break\n",
    "                    \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        time.sleep(0.5)  # Be nice to the API\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nâ¹ï¸  Stopped by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Error: {e}\")\n",
    "\n",
    "# Final stats and save\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"ğŸ“Š COLLECTION COMPLETE\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total posts: {len(all_posts)}\")\n",
    "print(f\"Unique posts: {len(seen_ids)}\")\n",
    "print(f\"Time: {elapsed/60:.1f} minutes\")\n",
    "print(f\"Rate: {len(all_posts)/elapsed:.1f} posts/second\")\n",
    "\n",
    "# Save to CSV\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f\"ukraine_truth_social_{timestamp}.csv\"\n",
    "save_posts(all_posts, filename)\n",
    "\n",
    "print(f\"\\nâœ… Done! Your data is in: {filename}\")\n",
    "\n",
    "# Quick stats\n",
    "if all_posts:\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    print(f\"\\nğŸ“ˆ Quick Stats:\")\n",
    "    print(f\"  - Date range: {df['created_at'].min()} to {df['created_at'].max()}\")\n",
    "    print(f\"  - Top accounts: {df['account'].value_counts().head(5).to_dict()}\")\n",
    "    print(f\"  - Most reblogged: {df.nlargest(3, 'reblogs_count')[['text', 'reblogs_count']].values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
