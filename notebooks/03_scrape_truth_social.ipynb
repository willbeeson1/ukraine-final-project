{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import subprocess, sys, importlib, os, re\n",
    "from datetime import datetime\n",
    "import truthbrush as tb\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ UNIVERSAL PATCH CELL  (run once, very top of notebook) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import subprocess, sys, importlib, os, types\n",
    "from pathlib import Path\n",
    "\n",
    "# 1Ô∏è‚É£  make sure both python-dotenv and curl_cffi exist\n",
    "def ensure(pkg, src=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ModuleNotFoundError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", src or pkg]\n",
    "        )\n",
    "\n",
    "ensure(\"python-dotenv\")\n",
    "ensure(\"curl_cffi\")\n",
    "\n",
    "# 2Ô∏è‚É£  reload .env (override=True guarantees fresh values)\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "# 3Ô∏è‚É£  import truthbrush and inject curl_cffi so NameError can‚Äôt happen\n",
    "import curl_cffi                     # noqa:  F401  (needed for side-effect)\n",
    "import truthbrush.api as tb_api\n",
    "tb_api.curl_cffi = curl_cffi         # hand it to truthbrush‚Äôs module scope\n",
    "\n",
    "import truthbrush as tb\n",
    "print(\"‚úî Patch cell finished ‚Äì environment refreshed, curl_cffi wired\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import truthbrush as tb\n",
    "from datetime import datetime, timezone\n",
    "import random, time\n",
    "\n",
    "# install SINGLE wrapper around _get  (skip if attribute exists)\n",
    "if not hasattr(tb.api.Api, \"_get_base\"):\n",
    "    tb.api.Api._get_base = tb.api.Api._get      # save original once\n",
    "\n",
    "    def _polite_get(self, url, params=None):\n",
    "        resp = self._get_base(url, params)\n",
    "        # header-based sleep\n",
    "        if (self.ratelimit_remaining is not None\n",
    "                and self.ratelimit_remaining <= 10\n",
    "                and self.ratelimit_reset):\n",
    "            wait = max(\n",
    "                0,\n",
    "                (self.ratelimit_reset -\n",
    "                 datetime.utcnow().replace(tzinfo=timezone.utc)).total_seconds()\n",
    "            ) + random.uniform(1, 3)\n",
    "            print(f\"üìâ near limit ‚Äì sleeping {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "        else:\n",
    "            time.sleep(random.uniform(1.5, 3.0))\n",
    "        return resp\n",
    "\n",
    "    tb.api.Api._get = _polite_get\n",
    "    print(\"‚úì polite-delay wrapper installed\")\n",
    "else:\n",
    "    print(\"‚úì wrapper already present ‚Äì no re-patch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"29Sw-s-Bj5TITP-j7D2zYHuwpDe-trJn6AY5uZW0yD4\"   # put your live token\n",
    "api   = tb.Api(token=TOKEN)\n",
    "api.auth_id = api.auth_id or \"\"\n",
    "print(\"client ready\")\n",
    "\n",
    "print(\"lookup test:\")\n",
    "try:\n",
    "    print(api.lookup(\"realDonaldTrump\")[\"id\"][:8], \"‚Ä¶ lookup OK\")\n",
    "except Exception as e:\n",
    "    print(\"lookup failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ scrape Truth Social & save CSV ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import os, re, csv, pathlib, random, time, json, sys\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------- config ---------------------------------------------------------\n",
    "KEYWORDS = [\n",
    "    \"russia\", \"russian\", \"ukraine\", \"ukrainian\", \"ru-uk\", \"putin\",\n",
    "    \"zelensky\", \"zelenskyy\", \"kremlin\", \"kyiv\", \"crimea\", \"donbas\",\n",
    "    \"mariupol\", \"kherson\", \"luhansk\", \"dnipro\", \"odessa\", \"invasion\", \"war\",\n",
    "]\n",
    "SEED_HANDLES = [\n",
    "    \"realDonaldTrump\", \"TeamTrump\", \"TrumpWarRoom\", \"WhiteHouse\", \"PressSec\"\n",
    "]\n",
    "\n",
    "def keyword_hit(html: str) -> bool:\n",
    "    return any(k in html.lower() for k in KEYWORDS)\n",
    "\n",
    "# ---------- resolve handles ------------------------------------------------\n",
    "def canonical_handle(hint: str) -> str | None:\n",
    "    try:\n",
    "        info = api.lookup(user_handle=hint.lstrip(\"@\"))\n",
    "        return info.get(\"acct\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            page = next(api.search(\"accounts\", hint, limit=1))\n",
    "            return page[\"accounts\"][0][\"acct\"] if page[\"accounts\"] else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "handles = [h for h in (canonical_handle(x) for x in SEED_HANDLES) if h]\n",
    "print(\"Scanning:\", handles)\n",
    "\n",
    "# ---------- scrape with live progress --------------------------------------\n",
    "hits = []\n",
    "try:\n",
    "    for h in handles:\n",
    "        print(f\"\\n‚Ü≥ pulling @{h}\")\n",
    "\n",
    "        # get a generator *but* pull the first post immediately so you know it's alive\n",
    "        gen = api.pull_statuses(username=h, replies=False, verbose=False)\n",
    "        print(\"  ‚Ä¶ sending first request\")\n",
    "        try:\n",
    "            first_post = next(gen)\n",
    "            print(\"  ‚úì first post received\")\n",
    "        except StopIteration:\n",
    "            print(\"  ‚ö†Ô∏è no posts found for this account\")\n",
    "            continue\n",
    "\n",
    "        # prepend that first item back into the stream\n",
    "        def prepend_first(item, iterator):\n",
    "            yield item\n",
    "            yield from iterator\n",
    "        gen = prepend_first(first_post, gen)\n",
    "\n",
    "        pbar = tqdm(gen, unit=\"post\", desc=f\"{h}\", leave=True)\n",
    "        matched = 0\n",
    "        for post in pbar:\n",
    "            if post and post.get(\"content\") and keyword_hit(post[\"content\"]):\n",
    "                matched += 1\n",
    "                hits.append(\n",
    "                    {\n",
    "                        \"created_at\": post[\"created_at\"],\n",
    "                        \"account\"   : h,\n",
    "                        \"id\"        : post[\"id\"],\n",
    "                        \"text\"      : re.sub(r\"<[^>]+>\", \"\", post[\"content\"]).strip(),\n",
    "                    }\n",
    "                )\n",
    "            if matched % 25 == 0:             # update label every 25 matches\n",
    "                pbar.set_description(f\"{h}  hits:{matched}\")\n",
    "        pbar.close()\n",
    "        print(f\"‚úì @{h}: {matched} matches collected\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπ Interrupted by user ‚Äì proceeding with what we have ‚Ä¶\")\n",
    "\n",
    "print(f\"\\n‚úì total matches: {len(hits)}\")\n",
    "\n",
    "# ---------- preview top 5 ---------------------------------------------------\n",
    "for p in hits[:5]:\n",
    "    ts = dt_parse.isoparse(p[\"created_at\"]).strftime(\"%Y-%m-%d %H:%M\")\n",
    "    print(f\"[{ts}] @{p['account']} ‚Üí {p['text'][:120]}‚Ä¶\")\n",
    "\n",
    "# ---------- save CSV --------------------------------------------------------\n",
    "ROOT = pathlib.Path.cwd()\n",
    "while ROOT.parent != ROOT and not (ROOT / \".git\").exists() and not (ROOT / \".env\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "out_path = ROOT / \"outputs\" / \"trump_ru_uk_truths.csv\"\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"created_at\", \"account\", \"id\", \"text\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(hits)\n",
    "\n",
    "print(\"üìÑ CSV written ‚Üí\", out_path.relative_to(ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ merge & dedup Truth-Social match files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import pandas as pd, hashlib, pathlib, textwrap\n",
    "\n",
    "ROOT = pathlib.Path.cwd()\n",
    "while ROOT.parent != ROOT and not (ROOT/\".git\").exists() and not (ROOT/\".env\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "CSV_FILES = [\n",
    "    ROOT / \"outputs\" / \"trump_ru_uk_truths.csv\",\n",
    "    ROOT / \"outputs\" / \"new_truth_scrape_matches.csv\",\n",
    "]\n",
    "\n",
    "frames = []\n",
    "for p in CSV_FILES:\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p, dtype=str)       # keep ids as str\n",
    "        frames.append(df)\n",
    "        print(f\"‚úì loaded {p.name:<30} rows={len(df):>5}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {p} not found\")\n",
    "\n",
    "if not frames:\n",
    "    raise FileNotFoundError(\"No CSVs found ‚Äì check paths above.\")\n",
    "\n",
    "df_all = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# -------- exact-ID dedup ----------------------------------------\n",
    "before   = len(df_all)\n",
    "df_all   = df_all.drop_duplicates(subset=\"id\")\n",
    "\n",
    "# -------- fallback fuzzy key  -----------------------------------\n",
    "# Sometimes Truthbrush returns slightly different IDs for the same content.\n",
    "# Build a cheap content hash (first 120 chars lower-cased, account-specific).\n",
    "def row_key(r):\n",
    "    snippet = (r[\"text\"][:120] if isinstance(r[\"text\"], str) else \"\").lower()\n",
    "    return hashlib.md5((r[\"account\"] + snippet).encode()).hexdigest()\n",
    "\n",
    "df_all[\"dup_key\"] = df_all.apply(row_key, axis=1)\n",
    "df_all = df_all.drop_duplicates(subset=\"dup_key\").drop(columns=\"dup_key\")\n",
    "\n",
    "after_exact  = before\n",
    "after_final  = len(df_all)\n",
    "\n",
    "print(f\"\"\"\n",
    "rows in concat   : {before}\n",
    "unique post IDs  : {after_exact}\n",
    "unique after key : {after_final}\n",
    "duplicates dropped: {before - after_final}\n",
    "\"\"\".strip())\n",
    "\n",
    "# save\n",
    "out_path = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "df_all.to_csv(out_path, index=False)\n",
    "print(\"üíæ merged CSV ‚Üí\", out_path.relative_to(ROOT))\n",
    "\n",
    "# peek a few rows\n",
    "print(\"\\n‚ñ∂ sample rows\")\n",
    "for _, r in df_all.head(5).iterrows():\n",
    "    print(textwrap.shorten(r['text'].replace(\"\\n\",\" \"), width=100, placeholder=\"‚Ä¶\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ monthly conflict-keyword Truths (all accounts) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "csv_path = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"created_at\"])\n",
    "\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è  DataFrame is empty ‚Äì check the CSV path or merge step.\")\n",
    "else:\n",
    "    # bucket by month\n",
    "    df[\"month\"] = df[\"created_at\"].dt.to_period(\"M\").astype(str)\n",
    "    counts = df.groupby(\"month\").size()\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    counts.plot(kind=\"bar\")\n",
    "    plt.title(\"Conflict-related Truths per month\")\n",
    "    plt.ylabel(\"Post count\")\n",
    "    plt.xticks(rotation=70, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------- paths\n",
    "vol_csv = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "esc_csv = ROOT / \"outputs\" / \"headline_scores_partial.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------- volume\n",
    "vol = (\n",
    "    pd.read_csv(vol_csv, parse_dates=[\"created_at\"])\n",
    "      .assign(day=lambda d: d[\"created_at\"].dt.date)\n",
    ")\n",
    "\n",
    "vol_daily = (\n",
    "    vol.groupby(\"day\")\n",
    "       .size()\n",
    "       .rename(\"post_count\")\n",
    "       .to_frame()\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------- escalation (updated for new CSV format)\n",
    "# Now the CSV has columns: 'date', 'source', 'title', 'score'\n",
    "esc = pd.read_csv(esc_csv, parse_dates=[\"date\"])\n",
    "esc = esc.set_index(\"date\")\n",
    "\n",
    "# If there is already a 'roll7' column, use it; otherwise compute a 7-day rolling average of 'score'\n",
    "if \"roll7\" in esc.columns:\n",
    "    esc_7d = esc[\"roll7\"].rename(\"escalation_7d\")\n",
    "else:\n",
    "    esc_7d = (\n",
    "        esc[\"score\"]\n",
    "            .rolling(7, min_periods=1)\n",
    "            .mean()\n",
    "            .rename(\"escalation_7d\")\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------- merge & plot\n",
    "df = vol_daily.join(esc_7d, how=\"outer\").fillna(0)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(11, 4))\n",
    "ax1.bar(\n",
    "    df.index,\n",
    "    df[\"post_count\"],\n",
    "    width=1,\n",
    "    color=\"#4a90e2\",\n",
    "    alpha=0.55,\n",
    "    label=\"Trump RU/UA daily count\"\n",
    ")\n",
    "ax1.set_ylabel(\"daily post count\", color=\"#4a90e2\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"#4a90e2\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(\n",
    "    df.index,\n",
    "    df[\"escalation_7d\"],\n",
    "    color=\"crimson\",\n",
    "    linewidth=2,\n",
    "    label=\"7-day mean escalation\"\n",
    ")\n",
    "ax2.set_ylabel(\"escalation index (0-10)\", color=\"crimson\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"crimson\")\n",
    "\n",
    "plt.title(\"Trump attention to RU/UA vs. conflict escalation, 2022-2025\")\n",
    "\n",
    "# Get handles & labels from each axis separately\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Combine them into single lists\n",
    "all_handles = handles1 + handles2\n",
    "all_labels = labels1 + labels2\n",
    "\n",
    "ax1.legend(all_handles, all_labels, loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë  Build GOLD-SET CANDIDATE sample for Truth-Social posts         ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT      = Path.cwd().resolve().parents[0]\n",
    "ALL_CSV   = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "GOLD_CSV  = ROOT / \"outputs\" / \"truth_gold_candidates.csv\"\n",
    "\n",
    "N_SAMPLES = 500                          # total sample size\n",
    "TIME_BINS = 8                            # split full range into equal bins\n",
    "\n",
    "# ‚îÄ‚îÄ load & sanity check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = pd.read_csv(ALL_CSV, parse_dates=[\"created_at\"])\n",
    "if df.empty:\n",
    "    raise RuntimeError(\"üí• merged Truth CSV seems empty ‚Äì aborting\")\n",
    "\n",
    "# ‚îÄ‚îÄ create time-bins (equal-length) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "df = df.sort_values(\"created_at\")\n",
    "cut_edges = np.linspace(df[\"created_at\"].min().value,\n",
    "                        df[\"created_at\"].max().value,\n",
    "                        TIME_BINS + 1)\n",
    "# assign a bin label 0 ‚Ä¶ TIME_BINS-1\n",
    "df[\"time_bin\"] = pd.cut(df[\"created_at\"].view(\"int64\"),\n",
    "                        bins=cut_edges, labels=False, include_lowest=True)\n",
    "\n",
    "# ‚îÄ‚îÄ stratified sample: time_bin √ó account ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "samples = []\n",
    "per_bin = int(np.ceil(N_SAMPLES / TIME_BINS))\n",
    "\n",
    "for bin_id, grp in df.groupby(\"time_bin\"):\n",
    "    # target size in this bin\n",
    "    n = min(per_bin, len(grp))\n",
    "    # proportional by account count\n",
    "    acc_counts = grp[\"account\"].value_counts(normalize=True)\n",
    "    wants = (acc_counts * n).round().astype(int)\n",
    "\n",
    "    # adjust rounding drift\n",
    "    while wants.sum() < n:\n",
    "        wants.loc[wants.idxmax()] += 1\n",
    "    while wants.sum() > n:\n",
    "        wants.loc[wants.idxmax()] -= 1\n",
    "\n",
    "    # sample inside each account slice\n",
    "    for acc, k in wants.items():\n",
    "        slice_ = grp[grp[\"account\"] == acc]\n",
    "        k = min(k, len(slice_))\n",
    "        samples.append(slice_.sample(k, random_state=42))\n",
    "\n",
    "gold_df = (pd.concat(samples)\n",
    "             .sort_values(\"created_at\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "gold_df.to_csv(GOLD_CSV, index=False)\n",
    "print(f\"üéØ gold-set candidates written ‚Üí {GOLD_CSV.relative_to(ROOT)}   \"\n",
    "      f\"(rows={len(gold_df)})\")\n",
    "\n",
    "# quick peek\n",
    "print(\"\\n‚ñ∂ random preview\")\n",
    "for _, r in gold_df.sample(5, random_state=1).iterrows():\n",
    "    print(f\"[{r.created_at:%Y-%m-%d}] @{r.account}: {r.text[:110]}‚Ä¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë  FILTER Truth Social posts for Ukraine-Russia war relevance           ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "INPUT_CSV = ROOT / \"outputs\" / \"truth_matches_merged.csv\"\n",
    "OUTPUT_CSV = ROOT / \"outputs\" / \"truth_ukraine_filtered.csv\"\n",
    "MODEL = \"claude-3-5-haiku-20241022\"  # Cheap and fast for filtering\n",
    "\n",
    "# Filtering prompt\n",
    "FILTER_PROMPT = \"\"\"You are filtering social media posts to identify which ones are about the Russia-Ukraine war.\n",
    "\n",
    "A post is RELEVANT (score 1) if it:\n",
    "- Directly mentions the Russia-Ukraine war, conflict, or invasion\n",
    "- Discusses military actions between Russia and Ukraine\n",
    "- Mentions Ukrainian or Russian leaders in context of the war\n",
    "- Discusses weapons, aid, or sanctions related to the conflict\n",
    "- References peace talks, negotiations, or ceasefire between Russia and Ukraine\n",
    "\n",
    "A post is NOT RELEVANT (score 0) if it:\n",
    "- Only mentions Russia or Ukraine in passing without war context\n",
    "- Discusses other conflicts (Israel, Syria, etc.) without Ukraine connection\n",
    "- Is about domestic US politics without Ukraine war connection\n",
    "- Mentions \"invasion\" referring to immigration/borders, not Ukraine\n",
    "- Uses war metaphors for non-Ukraine topics\n",
    "\n",
    "Respond with ONLY a single digit: 1 for relevant, 0 for not relevant.\n",
    "\n",
    "Examples:\n",
    "\"Putin is bombing Ukrainian cities again\" ‚Üí 1\n",
    "\"Biden's border invasion must stop\" ‚Üí 0\n",
    "\"We need to send more weapons to Ukraine\" ‚Üí 1\n",
    "\"Trump will end all wars including Ukraine\" ‚Üí 1\n",
    "\"China tariffs are destroying our economy\" ‚Üí 0\"\"\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(INPUT_CSV, parse_dates=[\"created_at\"])\n",
    "print(f\"üìä Processing {len(df)} Truth Social posts for Ukraine relevance\")\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Quiet logging\n",
    "for name in (\"httpx\", \"anthropic\"):\n",
    "    logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "# Add index for tracking\n",
    "df[\"batch_idx\"] = range(len(df))\n",
    "\n",
    "# Prepare batch requests\n",
    "requests_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row.get(\"text\")) or str(row[\"text\"]).strip() == \"\":\n",
    "        continue\n",
    "        \n",
    "    request = {\n",
    "        \"custom_id\": str(row[\"batch_idx\"]),\n",
    "        \"params\": {\n",
    "            \"model\": MODEL,\n",
    "            \"max_tokens\": 5,\n",
    "            \"temperature\": 0,\n",
    "            \"system\": FILTER_PROMPT,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": str(row[\"text\"])[:1000]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    requests_list.append(request)\n",
    "\n",
    "print(f\"üìù Prepared {len(requests_list)} requests for filtering\")\n",
    "\n",
    "# Create batch\n",
    "batch = client.messages.batches.create(requests=requests_list)\n",
    "print(f\"üöÄ Launched batch {batch.id}\")\n",
    "\n",
    "# Monitor progress\n",
    "bar = tqdm.tqdm(total=len(requests_list), desc=\"Filtering\", unit=\"post\")\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    batch_status = client.messages.batches.retrieve(batch.id)\n",
    "    completed = (batch_status.request_counts.succeeded + \n",
    "                batch_status.request_counts.errored + \n",
    "                batch_status.request_counts.canceled + \n",
    "                batch_status.request_counts.expired)\n",
    "    bar.n = completed\n",
    "    bar.refresh()\n",
    "    \n",
    "    if batch_status.processing_status == \"ended\":\n",
    "        bar.close()\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"‚úÖ Filtering complete in {elapsed_time/60:.1f} minutes\")\n",
    "\n",
    "# Parse results\n",
    "relevance_scores = {}\n",
    "errors = []\n",
    "\n",
    "# Retrieve results\n",
    "batch_final = client.messages.batches.retrieve(batch.id)\n",
    "\n",
    "if batch_final.results_url:\n",
    "    print(f\"üì• Fetching results from batch {batch.id}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(batch_final.results_url, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                result = json.loads(line)\n",
    "                custom_id = result.get(\"custom_id\")\n",
    "                \n",
    "                if custom_id is None:\n",
    "                    continue\n",
    "                \n",
    "                idx = int(custom_id)\n",
    "                \n",
    "                if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                    errors.append(f\"Request {custom_id} failed\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the response\n",
    "                message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "                \n",
    "                # Parse the score (should be just \"0\" or \"1\")\n",
    "                if message_content in [\"0\", \"1\"]:\n",
    "                    relevance_scores[idx] = int(message_content)\n",
    "                else:\n",
    "                    errors.append(f\"Invalid response for {custom_id}: {message_content}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors.append(f\"Error parsing result: {e}\")\n",
    "                continue\n",
    "\n",
    "# Map scores back to dataframe\n",
    "df[\"is_ukraine_relevant\"] = df[\"batch_idx\"].map(relevance_scores)\n",
    "\n",
    "# Filter to only relevant posts\n",
    "df_relevant = df[df[\"is_ukraine_relevant\"] == 1].copy()\n",
    "df_irrelevant = df[df[\"is_ukraine_relevant\"] == 0].copy()\n",
    "\n",
    "# Save filtered results\n",
    "df_relevant.drop(columns=[\"batch_idx\", \"is_ukraine_relevant\"]).to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìä Filtering Results:\")\n",
    "print(f\"   Total posts: {len(df)}\")\n",
    "print(f\"   Ukraine-relevant: {len(df_relevant)} ({len(df_relevant)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Not relevant: {len(df_irrelevant)} ({len(df_irrelevant)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Failed to classify: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    print(f\"\\n‚ö†Ô∏è  Errors encountered: {len(errors)}\")\n",
    "    for error in errors[:5]:\n",
    "        print(f\"   - {error}\")\n",
    "\n",
    "# Show examples of filtered out posts\n",
    "print(\"\\nüîç Examples of posts filtered OUT as not Ukraine-related:\")\n",
    "for _, row in df_irrelevant.head(5).iterrows():\n",
    "    print(f\"   - {row['text'][:100]}...\")\n",
    "\n",
    "print(f\"\\nüíæ Filtered data saved to: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
