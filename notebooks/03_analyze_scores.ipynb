{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Claude Haiku 3.5 Headline Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ANALYZE & VISUALIZE Ukraine War Escalation Scores Over Time          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# â”€â”€ Load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SCORES_CSV = ROOT / \"outputs\" / \"headline_scores_partial.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"analysis_plots\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Loading scored headlines...\")\n",
    "df = pd.read_csv(SCORES_CSV, parse_dates=['date'])\n",
    "df = df[df['score'].notna()]  # Remove any NaN scores\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} scored headlines\")\n",
    "print(f\"ğŸ“… Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "\n",
    "# â”€â”€ Basic statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“ˆ Basic Statistics:\")\n",
    "print(f\"   Mean escalation score: {df['score'].mean():.2f}\")\n",
    "print(f\"   Median score: {df['score'].median():.0f}\")\n",
    "print(f\"   Std deviation: {df['score'].std():.2f}\")\n",
    "\n",
    "# â”€â”€ Calculate daily averages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "daily_avg = df.groupby(df['date'].dt.date).agg({\n",
    "    'score': ['mean', 'median', 'std', 'count']\n",
    "}).round(2)\n",
    "daily_avg.columns = ['mean_score', 'median_score', 'std_dev', 'count']\n",
    "daily_avg = daily_avg.reset_index()\n",
    "daily_avg['date'] = pd.to_datetime(daily_avg['date'])\n",
    "\n",
    "# Save daily averages\n",
    "daily_avg_file = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"\n",
    "daily_avg.to_csv(daily_avg_file, index=False)\n",
    "print(f\"\\nğŸ’¾ Saved daily averages to: {daily_avg_file.name}\")\n",
    "\n",
    "# â”€â”€ Create visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. Score Distribution Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['score'], bins=11, range=(-0.5, 10.5), edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Escalation Score')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.title('Distribution of Escalation Scores (0-10)')\n",
    "plt.xticks(range(11))\n",
    "for i in range(11):\n",
    "    count = (df['score'] == i).sum()\n",
    "    plt.text(i, count + 200, str(count), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Daily Average Escalation Score Over Time\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(daily_avg['date'], daily_avg['mean_score'], linewidth=1.5, alpha=0.8)\n",
    "ax.fill_between(daily_avg['date'], \n",
    "                daily_avg['mean_score'] - daily_avg['std_dev'],\n",
    "                daily_avg['mean_score'] + daily_avg['std_dev'],\n",
    "                alpha=0.2, label='Â±1 std dev')\n",
    "\n",
    "# Add 30-day rolling average\n",
    "rolling_30 = daily_avg.set_index('date')['mean_score'].rolling('30D').mean()\n",
    "ax.plot(rolling_30.index, rolling_30.values, 'r-', linewidth=2, label='30-day average')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Escalation Score')\n",
    "ax.set_title('Daily Average Escalation Score Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'daily_escalation_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Weekly aggregated view\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "weekly_stats = df.groupby('week').agg({\n",
    "    'score': ['mean', 'median', 'count']\n",
    "}).round(2)\n",
    "weekly_stats.columns = ['mean_score', 'median_score', 'count']\n",
    "weekly_stats = weekly_stats.reset_index()\n",
    "weekly_stats['week_start'] = weekly_stats['week'].apply(lambda x: x.start_time)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(weekly_stats['week_start'], weekly_stats['mean_score'], 'o-', label='Mean', markersize=4)\n",
    "plt.plot(weekly_stats['week_start'], weekly_stats['median_score'], 's-', label='Median', markersize=4)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Escalation Score')\n",
    "plt.title('Weekly Average Escalation Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'weekly_escalation_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Heatmap of scores by day of week and hour\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "pivot_heatmap = df.pivot_table(values='score', index='hour', columns='day_of_week', aggfunc='mean')\n",
    "pivot_heatmap = pivot_heatmap[day_order]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_heatmap, cmap='YlOrRd', annot=True, fmt='.2f', cbar_kws={'label': 'Average Score'})\n",
    "plt.title('Average Escalation Score by Hour and Day of Week')\n",
    "plt.ylabel('Hour of Day')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'hourly_weekly_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Monthly trends with box plots\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "monthly_data = []\n",
    "for month in df['month'].unique():\n",
    "    month_scores = df[df['month'] == month]['score']\n",
    "    monthly_data.append(month_scores)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "box_positions = range(len(df['month'].unique()))\n",
    "plt.boxplot(monthly_data, positions=box_positions, widths=0.6)\n",
    "plt.xticks(box_positions, [str(m) for m in df['month'].unique()], rotation=45, ha='right')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Escalation Score')\n",
    "plt.title('Distribution of Escalation Scores by Month')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'monthly_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Proportion of high-escalation headlines over time\n",
    "df['high_escalation'] = df['score'] >= 7\n",
    "daily_high = df.groupby(df['date'].dt.date).agg({\n",
    "    'high_escalation': ['sum', 'mean']\n",
    "})\n",
    "daily_high.columns = ['count', 'proportion']\n",
    "daily_high = daily_high.reset_index()\n",
    "daily_high['date'] = pd.to_datetime(daily_high['date'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Top plot: Count of high-escalation headlines\n",
    "ax1.plot(daily_high['date'], daily_high['count'], 'r-', alpha=0.7)\n",
    "ax1.fill_between(daily_high['date'], 0, daily_high['count'], alpha=0.3, color='red')\n",
    "ax1.set_ylabel('Count of High-Escalation Headlines (â‰¥7)')\n",
    "ax1.set_title('High-Escalation Headlines Over Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom plot: Proportion\n",
    "ax2.plot(daily_high['date'], daily_high['proportion'] * 100, 'b-', alpha=0.7)\n",
    "ax2.fill_between(daily_high['date'], 0, daily_high['proportion'] * 100, alpha=0.3, color='blue')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Percentage of High-Escalation Headlines')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'high_escalation_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7. Key events analysis (identify days with highest average scores)\n",
    "top_days = daily_avg.nlargest(10, 'mean_score')[['date', 'mean_score', 'count']]\n",
    "print(\"\\nğŸ”¥ Top 10 Days with Highest Average Escalation:\")\n",
    "for _, row in top_days.iterrows():\n",
    "    print(f\"   {row['date'].date()}: {row['mean_score']:.2f} (n={row['count']})\")\n",
    "\n",
    "# Save this analysis\n",
    "top_days.to_csv(ROOT / \"outputs\" / \"top_escalation_days.csv\", index=False)\n",
    "\n",
    "# 8. Source analysis\n",
    "source_stats = df.groupby('source').agg({\n",
    "    'score': ['mean', 'count', 'std']\n",
    "}).round(2)\n",
    "source_stats.columns = ['mean_score', 'count', 'std_dev']\n",
    "source_stats = source_stats.sort_values('mean_score', ascending=False)\n",
    "source_stats.to_csv(ROOT / \"outputs\" / \"source_escalation_scores.csv\")\n",
    "\n",
    "print(\"\\nğŸ“° Top 5 Sources by Average Escalation Score:\")\n",
    "for source, row in source_stats.head().iterrows():\n",
    "    print(f\"   {source}: {row['mean_score']:.2f} (n={row['count']})\")\n",
    "\n",
    "# Final summary plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Subplot 1: Score distribution\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(df['score'], bins=11, range=(-0.5, 10.5), edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Score Distribution')\n",
    "\n",
    "# Subplot 2: Daily trend\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(daily_avg['date'], daily_avg['mean_score'], linewidth=1)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Avg Score')\n",
    "ax2.set_title('Daily Average Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Subplot 3: Monthly boxplot\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "monthly_labels = [str(m)[-7:] for m in df['month'].unique()][-12:]  # Last 12 months\n",
    "monthly_data_recent = monthly_data[-12:]\n",
    "ax3.boxplot(monthly_data_recent, labels=monthly_labels)\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Monthly Score Distribution (Last 12 Months)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Subplot 4: Source comparison (top 10)\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "top_sources = source_stats.head(10)\n",
    "ax4.barh(range(len(top_sources)), top_sources['mean_score'])\n",
    "ax4.set_yticks(range(len(top_sources)))\n",
    "ax4.set_yticklabels(top_sources.index)\n",
    "ax4.set_xlabel('Average Escalation Score')\n",
    "ax4.set_title('Top 10 Sources by Average Escalation Score')\n",
    "\n",
    "plt.suptitle('Ukraine War Headlines Escalation Analysis Summary', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'analysis_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Analysis complete! All plots saved to: {OUTPUT_DIR.name}/\")\n",
    "print(\"\\nğŸ“Š Generated visualizations:\")\n",
    "print(\"   1. score_distribution.png - Histogram of all scores\")\n",
    "print(\"   2. daily_escalation_trend.png - Daily averages with 30-day rolling mean\")\n",
    "print(\"   3. weekly_escalation_trend.png - Weekly aggregated view\")\n",
    "print(\"   4. hourly_weekly_heatmap.png - Patterns by hour and day of week\")\n",
    "print(\"   5. monthly_boxplots.png - Monthly distribution boxplots\")\n",
    "print(\"   6. high_escalation_trends.png - Tracking headlines with scores â‰¥7\")\n",
    "print(\"   7. analysis_summary.png - Combined summary dashboard\")\n",
    "print(\"\\nğŸ“„ Generated data files:\")\n",
    "print(\"   - daily_escalation_scores.csv\")\n",
    "print(\"   - top_escalation_days.csv\")\n",
    "print(\"   - source_escalation_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 7-Day Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7-day rolling mean of *headline* escalation --------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "DAILY_CSV = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"  # produced earlier\n",
    "\n",
    "daily = pd.read_csv(DAILY_CSV, parse_dates=[\"date\"])\n",
    "daily = daily.set_index(\"date\").sort_index()\n",
    "\n",
    "# 7-day centred rolling mean\n",
    "daily[\"roll7\"] = daily[\"mean_score\"].rolling(window=7, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(daily.index, daily[\"roll7\"], lw=2, color=\"crimson\", label=\"7-day mean\")\n",
    "plt.scatter(daily.index, daily[\"mean_score\"], s=8, alpha=0.3, label=\"daily mean\")\n",
    "plt.title(\"Headline escalation index â€“ 7-day rolling mean\")\n",
    "plt.ylabel(\"escalation score (0-10)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Claude Haiku 3.5 Truth Social Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ANALYZE & VISUALIZE Truth Social Escalation Scores Over Time         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "TRUTH_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_3-5-haiku-20241022.csv\"\n",
    "HEADLINE_DAILY_CSV = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"truth_analysis_plots\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€ Load Truth Social data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“Š Loading scored Truth Social posts...\")\n",
    "df = pd.read_csv(TRUTH_CSV)\n",
    "\n",
    "# Convert created_at to datetime with flexible format handling\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format='mixed', utc=True)\n",
    "\n",
    "# Remove any rows with NaN scores\n",
    "df = df[df['escalation_score'].notna()]  \n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} scored posts\")\n",
    "print(f\"ğŸ“… Date range: {df['created_at'].min().date()} to {df['created_at'].max().date()}\")\n",
    "\n",
    "# â”€â”€ Basic statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“ˆ Basic Statistics:\")\n",
    "print(f\"   Mean escalation score: {df['escalation_score'].mean():.2f}\")\n",
    "print(f\"   Median score: {df['escalation_score'].median():.0f}\")\n",
    "print(f\"   Std deviation: {df['escalation_score'].std():.2f}\")\n",
    "\n",
    "# Blame direction stats\n",
    "blame_counts = df['blame_direction'].value_counts().sort_index()\n",
    "print(\"\\nğŸ¯ Blame Direction Distribution:\")\n",
    "for direction, count in blame_counts.items():\n",
    "    label = {-1: \"No clear blame\", 0: \"Ukraine/NATO/West\", 1: \"Russia/Putin\"}[direction]\n",
    "    print(f\"   {label}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Call-to-action stats\n",
    "cta_pct = df['has_cta'].mean() * 100\n",
    "print(f\"\\nğŸ“¢ Call-to-Action: {df['has_cta'].sum():,} posts ({cta_pct:.1f}%) have CTAs\")\n",
    "\n",
    "# â”€â”€ Calculate daily averages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "daily_avg = df.groupby(df['created_at'].dt.date).agg({\n",
    "    'escalation_score': ['mean', 'median', 'std', 'count'],\n",
    "    'blame_direction': lambda x: (x == 0).mean(),  # % blaming Ukraine/West\n",
    "    'has_cta': 'mean'  # % with CTA\n",
    "}).round(2)\n",
    "daily_avg.columns = ['mean_score', 'median_score', 'std_dev', 'count', 'pct_blame_west', 'pct_cta']\n",
    "daily_avg = daily_avg.reset_index()\n",
    "daily_avg['created_at'] = pd.to_datetime(daily_avg['created_at'])\n",
    "\n",
    "# Save daily averages\n",
    "daily_avg_file = ROOT / \"outputs\" / \"truth_daily_escalation_scores.csv\"\n",
    "daily_avg.to_csv(daily_avg_file, index=False)\n",
    "print(f\"\\nğŸ’¾ Saved daily averages to: {daily_avg_file.name}\")\n",
    "\n",
    "# â”€â”€ Create visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. Score Distribution Histogram with comparison to headlines\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['escalation_score'], bins=11, range=(-0.5, 10.5), \n",
    "         edgecolor='black', alpha=0.7, label='Truth Social')\n",
    "plt.xlabel('Escalation Score')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.title('Distribution of Truth Social Escalation Scores (0-10)')\n",
    "plt.xticks(range(11))\n",
    "for i in range(11):\n",
    "    count = (df['escalation_score'] == i).sum()\n",
    "    plt.text(i, count + 50, str(count), ha='center', va='bottom', fontsize=9)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Daily Average with 7-day and 30-day rolling averages\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(daily_avg['created_at'], daily_avg['mean_score'], \n",
    "           alpha=0.3, s=20, color='gray', label='Daily mean')\n",
    "\n",
    "# Calculate rolling averages\n",
    "daily_avg_indexed = daily_avg.set_index('created_at').sort_index()\n",
    "rolling_7 = daily_avg_indexed['mean_score'].rolling('7D', center=True).mean()\n",
    "rolling_30 = daily_avg_indexed['mean_score'].rolling('30D').mean()\n",
    "\n",
    "ax.plot(rolling_7.index, rolling_7.values, 'b-', linewidth=2, label='7-day rolling mean')\n",
    "ax.plot(rolling_30.index, rolling_30.values, 'r-', linewidth=2, label='30-day rolling mean')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Escalation Score')\n",
    "ax.set_title('Truth Social: Daily Average Escalation Score Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_daily_escalation_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. OVERLAY PLOT: Truth Social vs Headlines (7-day rolling)\n",
    "print(\"\\nğŸ“Š Creating overlay comparison plot...\")\n",
    "\n",
    "# Load headline data\n",
    "headline_daily = pd.read_csv(HEADLINE_DAILY_CSV, parse_dates=['date'])\n",
    "headline_daily = headline_daily.set_index('date').sort_index()\n",
    "headline_roll7 = headline_daily['mean_score'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Prepare Truth Social 7-day rolling\n",
    "truth_roll7 = rolling_7\n",
    "\n",
    "# Create overlay plot\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot both series\n",
    "ax.plot(headline_roll7.index, headline_roll7.values, \n",
    "        'crimson', linewidth=2.5, label='Headlines (News Media)', alpha=0.8)\n",
    "ax.plot(truth_roll7.index, truth_roll7.values, \n",
    "        'navy', linewidth=2.5, label='Truth Social Posts', alpha=0.8)\n",
    "\n",
    "# Add scatter points for daily values\n",
    "ax.scatter(headline_daily.index, headline_daily['mean_score'], \n",
    "           alpha=0.15, s=10, color='crimson')\n",
    "ax.scatter(daily_avg['created_at'], daily_avg['mean_score'], \n",
    "           alpha=0.15, s=10, color='navy')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Escalation Score (0-10)', fontsize=12)\n",
    "ax.set_title('Ukraine War Escalation: Headlines vs Truth Social (7-day rolling mean)', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key differences\n",
    "ax.annotate('Truth Social\\nlower baseline', \n",
    "            xy=(pd.Timestamp('2023-06-01'), 2.2), \n",
    "            xytext=(pd.Timestamp('2023-06-01'), 1.0),\n",
    "            arrowprops=dict(arrowstyle='->', color='navy', alpha=0.5),\n",
    "            fontsize=10, ha='center', color='navy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'overlay_headlines_vs_truth.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  # Display in notebook\n",
    "plt.close()\n",
    "\n",
    "# 4. Blame direction over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Top: Percentage blaming West/Ukraine\n",
    "rolling_blame = daily_avg_indexed['pct_blame_west'].rolling('7D', center=True).mean() * 100\n",
    "ax1.plot(rolling_blame.index, rolling_blame.values, 'orange', linewidth=2)\n",
    "ax1.fill_between(rolling_blame.index, 0, rolling_blame.values, alpha=0.3, color='orange')\n",
    "ax1.set_ylabel('% Posts Blaming Ukraine/West')\n",
    "ax1.set_title('Truth Social: Blame Attribution Over Time (7-day rolling)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom: Percentage with Call-to-Action\n",
    "rolling_cta = daily_avg_indexed['pct_cta'].rolling('7D', center=True).mean() * 100\n",
    "ax2.plot(rolling_cta.index, rolling_cta.values, 'green', linewidth=2)\n",
    "ax2.fill_between(rolling_cta.index, 0, rolling_cta.values, alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('% Posts with Call-to-Action')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_blame_cta_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Correlation analysis between dimensions\n",
    "corr_matrix = df[['escalation_score', 'blame_direction', 'has_cta']].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Between Truth Social Scoring Dimensions')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Monthly comparison\n",
    "df['month'] = df['created_at'].dt.to_period('M')\n",
    "monthly_stats = df.groupby('month').agg({\n",
    "    'escalation_score': ['mean', 'std', 'count'],\n",
    "    'has_cta': 'mean'\n",
    "}).round(2)\n",
    "monthly_stats.columns = ['mean_score', 'std_score', 'count', 'pct_cta']\n",
    "monthly_stats = monthly_stats.reset_index()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Top: Monthly mean scores with error bars\n",
    "months = [str(m) for m in monthly_stats['month']]\n",
    "x_pos = range(len(months))\n",
    "ax1.errorbar(x_pos, monthly_stats['mean_score'], yerr=monthly_stats['std_score'], \n",
    "             marker='o', capsize=5, capthick=2, linewidth=2)\n",
    "ax1.set_ylabel('Mean Escalation Score')\n",
    "ax1.set_title('Truth Social: Monthly Average Escalation Scores')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bottom: Monthly CTA percentage\n",
    "ax2.bar(x_pos, monthly_stats['pct_cta'] * 100, alpha=0.7, color='green')\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('% Posts with CTA')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(months, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_monthly_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7. High-escalation analysis (scores >= 7)\n",
    "df['high_escalation'] = df['escalation_score'] >= 7\n",
    "daily_high = df.groupby(df['created_at'].dt.date).agg({\n",
    "    'high_escalation': ['sum', 'mean']\n",
    "})\n",
    "daily_high.columns = ['count', 'proportion']\n",
    "daily_high = daily_high.reset_index()\n",
    "daily_high['created_at'] = pd.to_datetime(daily_high['created_at'])\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "daily_high_indexed = daily_high.set_index('created_at')\n",
    "rolling_high = daily_high_indexed['proportion'].rolling('7D', center=True).mean() * 100\n",
    "\n",
    "plt.plot(rolling_high.index, rolling_high.values, 'red', linewidth=2)\n",
    "plt.fill_between(rolling_high.index, 0, rolling_high.values, alpha=0.3, color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('% High-Escalation Posts (â‰¥7)')\n",
    "plt.title('Truth Social: Proportion of High-Escalation Posts (7-day rolling)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_high_escalation_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 8. Top escalation days\n",
    "top_days = daily_avg.nlargest(10, 'mean_score')[['created_at', 'mean_score', 'count']]\n",
    "print(\"\\nğŸ”¥ Top 10 Days with Highest Average Escalation (Truth Social):\")\n",
    "for _, row in top_days.iterrows():\n",
    "    print(f\"   {row['created_at'].date()}: {row['mean_score']:.2f} (n={row['count']})\")\n",
    "\n",
    "# Save top days\n",
    "top_days.to_csv(ROOT / \"outputs\" / \"truth_top_escalation_days.csv\", index=False)\n",
    "\n",
    "# 9. Summary statistics comparison\n",
    "print(\"\\nğŸ“Š COMPARATIVE SUMMARY: Truth Social vs Headlines\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate headline stats for comparison\n",
    "if HEADLINE_DAILY_CSV.exists():\n",
    "    headline_scores = pd.read_csv(ROOT / \"outputs\" / \"headline_scores_partial.csv\")\n",
    "    \n",
    "    print(f\"\\nMean Escalation Score:\")\n",
    "    print(f\"   Headlines: {headline_scores['score'].mean():.2f}\")\n",
    "    print(f\"   Truth Social: {df['escalation_score'].mean():.2f}\")\n",
    "    print(f\"   Difference: {df['escalation_score'].mean() - headline_scores['score'].mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nStandard Deviation:\")\n",
    "    print(f\"   Headlines: {headline_scores['score'].std():.2f}\")\n",
    "    print(f\"   Truth Social: {df['escalation_score'].std():.2f}\")\n",
    "\n",
    "# Final summary dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Subplot 1: Score distribution\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(df['escalation_score'], bins=11, range=(-0.5, 10.5), \n",
    "         edgecolor='black', alpha=0.7, color='navy')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Escalation Score Distribution')\n",
    "\n",
    "# Subplot 2: Blame direction pie chart\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "blame_counts = df['blame_direction'].value_counts()\n",
    "labels = ['No blame', 'Ukraine/West', 'Russia']\n",
    "colors = ['gray', 'orange', 'red']\n",
    "wedges, texts, autotexts = ax2.pie(blame_counts.values, labels=labels, colors=colors, \n",
    "                                    autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Blame Attribution')\n",
    "\n",
    "# Subplot 3: CTA distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "cta_counts = df['has_cta'].value_counts()\n",
    "ax3.bar(['No CTA', 'Has CTA'], cta_counts.values, color=['lightgray', 'green'])\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Call-to-Action Distribution')\n",
    "\n",
    "# Subplot 4: Daily trend (full width)\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "ax4.plot(rolling_7.index, rolling_7.values, 'navy', linewidth=2, label='Truth Social')\n",
    "if 'headline_roll7' in locals():\n",
    "    ax4.plot(headline_roll7.index, headline_roll7.values, 'crimson', \n",
    "             linewidth=2, label='Headlines', alpha=0.7)\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('7-day Rolling Mean')\n",
    "ax4.set_title('Escalation Score Trends Comparison')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Monthly averages\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "ax5.plot(range(len(monthly_stats)), monthly_stats['mean_score'], 'o-', linewidth=2)\n",
    "ax5.set_xticks(range(0, len(monthly_stats), 3))\n",
    "ax5.set_xticklabels([str(m) for m in monthly_stats['month']][::3], rotation=45)\n",
    "ax5.set_xlabel('Month')\n",
    "ax5.set_ylabel('Mean Score')\n",
    "ax5.set_title('Monthly Average Escalation')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Key metrics\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "ax6.axis('off')\n",
    "metrics_text = f\"\"\"Key Metrics:\n",
    "\n",
    "Total Posts: {len(df):,}\n",
    "Date Range: {df['created_at'].min().date()} to {df['created_at'].max().date()}\n",
    "\n",
    "Mean Score: {df['escalation_score'].mean():.2f}\n",
    "Median: {df['escalation_score'].median():.0f}\n",
    "Std Dev: {df['escalation_score'].std():.2f}\n",
    "\n",
    "High Escalation (â‰¥7): {(df['escalation_score'] >= 7).sum():,} ({(df['escalation_score'] >= 7).mean()*100:.1f}%)\n",
    "Has CTA: {df['has_cta'].sum():,} ({df['has_cta'].mean()*100:.1f}%)\n",
    "Blames West: {(df['blame_direction'] == 0).sum():,} ({(df['blame_direction'] == 0).mean()*100:.1f}%)\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes, fontsize=10, \n",
    "         verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.suptitle('Truth Social Ukraine War Posts: Comprehensive Analysis', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_analysis_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Analysis complete! All plots saved to: {OUTPUT_DIR.name}/\")\n",
    "print(\"\\nğŸ“Š Generated visualizations:\")\n",
    "print(\"   1. truth_score_distribution.png - Histogram of escalation scores\")\n",
    "print(\"   2. truth_daily_escalation_trend.png - Daily averages with rolling means\")\n",
    "print(\"   3. overlay_headlines_vs_truth.png - COMPARISON WITH HEADLINES\")\n",
    "print(\"   4. truth_blame_cta_trends.png - Blame and CTA patterns over time\")\n",
    "print(\"   5. truth_correlation_matrix.png - Correlation between dimensions\")\n",
    "print(\"   6. truth_monthly_trends.png - Monthly aggregated view\")\n",
    "print(\"   7. truth_high_escalation_trend.png - High escalation posts tracking\")\n",
    "print(\"   8. truth_analysis_summary.png - Comprehensive dashboard\")\n",
    "print(\"\\nğŸ“„ Generated data files:\")\n",
    "print(\"   - truth_daily_escalation_scores.csv\")\n",
    "print(\"   - truth_top_escalation_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Anthropic Model Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  THREE-MODEL COMPARISON: Haiku 3.5 vs Sonnet 4 vs Opus 4             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HAIKU_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_3-5-haiku-20241022.csv\"\n",
    "SONNET_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "OPUS_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-opus-4-20250514.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"three_model_comparison\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Loading model outputs...\")\n",
    "# Load all three datasets\n",
    "haiku_df = pd.read_csv(HAIKU_CSV)\n",
    "sonnet_df = pd.read_csv(SONNET_CSV)\n",
    "opus_df = pd.read_csv(OPUS_CSV)\n",
    "\n",
    "# Merge on common identifier columns\n",
    "merge_cols = ['created_at', 'account', 'id', 'text']\n",
    "\n",
    "# First merge Haiku and Sonnet\n",
    "comparison_df = pd.merge(\n",
    "    haiku_df[merge_cols + ['escalation_score', 'blame_direction', 'has_cta']],\n",
    "    sonnet_df[merge_cols + ['escalation_score', 'blame_direction', 'has_cta']],\n",
    "    on=merge_cols,\n",
    "    suffixes=('_haiku', '_sonnet'),\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Then merge with Opus\n",
    "comparison_df = pd.merge(\n",
    "    comparison_df,\n",
    "    opus_df[merge_cols + ['escalation_score', 'blame_direction', 'has_cta']],\n",
    "    on=merge_cols,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Rename Opus columns for consistency\n",
    "comparison_df.rename(columns={\n",
    "    'escalation_score': 'escalation_score_opus',\n",
    "    'blame_direction': 'blame_direction_opus',\n",
    "    'has_cta': 'has_cta_opus'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"âœ… Matched {len(comparison_df)} posts scored by all three models\")\n",
    "\n",
    "# Calculate pairwise differences\n",
    "comparison_df['diff_haiku_sonnet'] = comparison_df['escalation_score_haiku'] - comparison_df['escalation_score_sonnet']\n",
    "comparison_df['diff_haiku_opus'] = comparison_df['escalation_score_haiku'] - comparison_df['escalation_score_opus']\n",
    "comparison_df['diff_sonnet_opus'] = comparison_df['escalation_score_sonnet'] - comparison_df['escalation_score_opus']\n",
    "\n",
    "# Agreement metrics\n",
    "print(\"\\nğŸ“ˆ Pairwise Agreement Statistics:\")\n",
    "model_pairs = [('haiku', 'sonnet'), ('haiku', 'opus'), ('sonnet', 'opus')]\n",
    "for m1, m2 in model_pairs:\n",
    "    esc_corr = comparison_df[f'escalation_score_{m1}'].corr(comparison_df[f'escalation_score_{m2}'])\n",
    "    blame_agree = (comparison_df[f'blame_direction_{m1}'] == comparison_df[f'blame_direction_{m2}']).mean()\n",
    "    cta_agree = (comparison_df[f'has_cta_{m1}'] == comparison_df[f'has_cta_{m2}']).mean()\n",
    "    \n",
    "    print(f\"\\n{m1.capitalize()} vs {m2.capitalize()}:\")\n",
    "    print(f\"   Escalation correlation: {esc_corr:.3f}\")\n",
    "    print(f\"   Blame agreement: {blame_agree*100:.1f}%\")\n",
    "    print(f\"   CTA agreement: {cta_agree*100:.1f}%\")\n",
    "\n",
    "# Overall statistics by model\n",
    "print(\"\\nğŸ“Š Model Statistics:\")\n",
    "for model in ['haiku', 'sonnet', 'opus']:\n",
    "    esc_mean = comparison_df[f'escalation_score_{model}'].mean()\n",
    "    esc_std = comparison_df[f'escalation_score_{model}'].std()\n",
    "    blame_west = (comparison_df[f'blame_direction_{model}'] == 0).mean() * 100\n",
    "    blame_russia = (comparison_df[f'blame_direction_{model}'] == 1).mean() * 100\n",
    "    has_cta = comparison_df[f'has_cta_{model}'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{model.capitalize()}:\")\n",
    "    print(f\"   Escalation: mean={esc_mean:.2f}, std={esc_std:.2f}\")\n",
    "    print(f\"   Blames West: {blame_west:.1f}%\")\n",
    "    print(f\"   Blames Russia: {blame_russia:.1f}%\")\n",
    "    print(f\"   Has CTA: {has_cta:.1f}%\")\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "fig.suptitle('Three-Model Comparison: Haiku 3.5 vs Sonnet 4 vs Opus 4', fontsize=18)\n",
    "\n",
    "# Row 1: Pairwise escalation scatter plots\n",
    "for i, (m1, m2) in enumerate(model_pairs):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.scatter(comparison_df[f'escalation_score_{m1}'], \n",
    "               comparison_df[f'escalation_score_{m2}'],\n",
    "               alpha=0.3, s=10)\n",
    "    ax.plot([0, 10], [0, 10], 'r--', alpha=0.5)\n",
    "    ax.set_xlabel(f'{m1.capitalize()} Score')\n",
    "    ax.set_ylabel(f'{m2.capitalize()} Score')\n",
    "    ax.set_title(f'{m1.capitalize()} vs {m2.capitalize()}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation text\n",
    "    corr = comparison_df[f'escalation_score_{m1}'].corr(comparison_df[f'escalation_score_{m2}'])\n",
    "    ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Row 2: Escalation score distributions\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "models = ['haiku', 'sonnet', 'opus']\n",
    "positions = np.arange(11)\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    counts = comparison_df[f'escalation_score_{model}'].value_counts().sort_index()\n",
    "    counts = counts.reindex(range(11), fill_value=0)\n",
    "    ax.bar(positions + i*width, counts.values, width, label=model.capitalize(), alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Escalation Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Escalation Score Distributions by Model')\n",
    "ax.set_xticks(positions + width)\n",
    "ax.set_xticklabels(positions)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Blame direction comparison\n",
    "for i, model in enumerate(models):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    blame_counts = comparison_df[f'blame_direction_{model}'].value_counts()\n",
    "    labels = ['No blame', 'West/NATO', 'Russia']\n",
    "    label_map = {-1: 'No blame', 0: 'West/NATO', 1: 'Russia'}\n",
    "    sizes = [blame_counts.get(j, 0) for j in [-1, 0, 1]]\n",
    "    colors = ['gray', 'orange', 'red']\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, \n",
    "                                       autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title(f'{model.capitalize()} - Blame Attribution')\n",
    "\n",
    "# Row 4: Three-way agreement analysis\n",
    "ax1 = fig.add_subplot(gs[3, 0])\n",
    "# Calculate where all three models agree within 1 point\n",
    "all_agree_esc = ((abs(comparison_df['diff_haiku_sonnet']) <= 1) & \n",
    "                 (abs(comparison_df['diff_haiku_opus']) <= 1) & \n",
    "                 (abs(comparison_df['diff_sonnet_opus']) <= 1)).mean() * 100\n",
    "\n",
    "all_agree_blame = ((comparison_df['blame_direction_haiku'] == comparison_df['blame_direction_sonnet']) & \n",
    "                   (comparison_df['blame_direction_haiku'] == comparison_df['blame_direction_opus'])).mean() * 100\n",
    "\n",
    "all_agree_cta = ((comparison_df['has_cta_haiku'] == comparison_df['has_cta_sonnet']) & \n",
    "                 (comparison_df['has_cta_haiku'] == comparison_df['has_cta_opus'])).mean() * 100\n",
    "\n",
    "agreement_data = [all_agree_esc, all_agree_blame, all_agree_cta]\n",
    "agreement_labels = ['Escalation\\n(within Â±1)', 'Blame\\nDirection', 'Call to\\nAction']\n",
    "\n",
    "bars = ax1.bar(agreement_labels, agreement_data, color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "ax1.set_ylabel('Agreement Rate (%)')\n",
    "ax1.set_title('Three-Way Agreement Rates')\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, agreement_data):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{value:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Systematic bias analysis\n",
    "ax2 = fig.add_subplot(gs[3, 1])\n",
    "mean_scores = [comparison_df[f'escalation_score_{m}'].mean() for m in models]\n",
    "ax2.bar(models, mean_scores, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
    "ax2.set_ylabel('Mean Escalation Score')\n",
    "ax2.set_title('Average Escalation by Model')\n",
    "ax2.set_ylim(0, 3)\n",
    "\n",
    "for i, (model, score) in enumerate(zip(models, mean_scores)):\n",
    "    ax2.text(i, score + 0.05, f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Variance in scoring\n",
    "ax3 = fig.add_subplot(gs[3, 2])\n",
    "variance_data = comparison_df[['escalation_score_haiku', 'escalation_score_sonnet', 'escalation_score_opus']].var(axis=1)\n",
    "ax3.hist(variance_data, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Variance in Scores')\n",
    "ax3.set_ylabel('Number of Posts')\n",
    "ax3.set_title('Distribution of Score Variance Across Models')\n",
    "ax3.axvline(variance_data.mean(), color='red', linestyle='--', label=f'Mean: {variance_data.mean():.2f}')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'three_model_comparison_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify posts with high disagreement\n",
    "high_variance = comparison_df[variance_data > variance_data.quantile(0.95)].copy()\n",
    "high_variance['score_variance'] = variance_data[variance_data > variance_data.quantile(0.95)]\n",
    "print(f\"\\nâš ï¸  Found {len(high_variance)} posts with high variance (top 5%)\")\n",
    "\n",
    "# Save comparison data\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'three_model_comparison_full.csv', index=False)\n",
    "high_variance[['text', 'escalation_score_haiku', 'escalation_score_sonnet', \n",
    "               'escalation_score_opus', 'score_variance']].to_csv(\n",
    "    OUTPUT_DIR / 'high_variance_posts.csv', index=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Results saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Recommended model selection\n",
    "print(\"\\nğŸ¯ Model Selection Guidance:\")\n",
    "print(\"\\nBased on the analysis:\")\n",
    "print(\"- Haiku 3.5: Highest escalation scores, strongest West-blame attribution\")\n",
    "print(\"- Sonnet 4: Lowest blame attribution, very low escalation\")\n",
    "print(\"- Opus 4: Middle ground on blame, lowest escalation scores\")\n",
    "print(\"\\nRecommendation: Validate a sample from high-variance posts to determine\")\n",
    "print(\"which model best aligns with human judgment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### High Variance Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  EXTRACT FOCUSED VALIDATION SAMPLE FROM HIGH-VARIANCE POSTS           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HIGH_VAR_CSV = ROOT / \"outputs\" / \"three_model_comparison\" / \"high_variance_posts.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"focused_validation\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load high variance posts\n",
    "print(\"ğŸ“Š Loading high-variance posts...\")\n",
    "df = pd.read_csv(HIGH_VAR_CSV)\n",
    "print(f\"âœ… Loaded {len(df)} high-variance posts\")\n",
    "\n",
    "# Add analysis columns\n",
    "df['max_score'] = df[['escalation_score_haiku', 'escalation_score_sonnet', 'escalation_score_opus']].max(axis=1)\n",
    "df['min_score'] = df[['escalation_score_haiku', 'escalation_score_sonnet', 'escalation_score_opus']].min(axis=1)\n",
    "df['score_range'] = df['max_score'] - df['min_score']\n",
    "\n",
    "# Categorize disagreement patterns\n",
    "df['haiku_outlier'] = (\n",
    "    (df['escalation_score_haiku'] > df['escalation_score_sonnet'] + 2) & \n",
    "    (df['escalation_score_haiku'] > df['escalation_score_opus'] + 2)\n",
    ")\n",
    "\n",
    "df['sonnet_opus_agree'] = abs(df['escalation_score_sonnet'] - df['escalation_score_opus']) <= 1\n",
    "\n",
    "# Define validation categories\n",
    "validation_samples = {}\n",
    "\n",
    "# Category 1: Haiku scores high (4+) while both others score low (0-1)\n",
    "cat1 = df[(df['escalation_score_haiku'] >= 4) & \n",
    "          (df['escalation_score_sonnet'] <= 1) & \n",
    "          (df['escalation_score_opus'] <= 1)]\n",
    "validation_samples['haiku_high_others_low'] = cat1.head(10)\n",
    "\n",
    "# Category 2: All three disagree significantly\n",
    "cat2 = df[(df['score_range'] >= 4) & (~df['sonnet_opus_agree'])]\n",
    "validation_samples['all_disagree'] = cat2.head(10)\n",
    "\n",
    "# Category 3: Sonnet and Opus agree but Haiku differs by 3+\n",
    "cat3 = df[df['sonnet_opus_agree'] & (abs(df['escalation_score_haiku'] - df['escalation_score_opus']) >= 3)]\n",
    "validation_samples['haiku_outlier_sonnet_opus_agree'] = cat3.head(10)\n",
    "\n",
    "# Category 4: Posts with \"Biden\" or \"Trump\" to check political vs military scoring\n",
    "political_keywords = df[df['text'].str.contains('Biden|Trump|Democrat|Republican|MAGA', case=False, na=False)]\n",
    "cat4 = political_keywords[political_keywords['score_variance'] > 3]\n",
    "validation_samples['political_content'] = cat4.head(10)\n",
    "\n",
    "# Category 5: Posts with explicit war/military language\n",
    "military_keywords = df[df['text'].str.contains('nuclear|missile|weapon|bomb|attack|strike', case=False, na=False)]\n",
    "cat5 = military_keywords[military_keywords['score_variance'] > 3]\n",
    "validation_samples['military_content'] = cat5.head(5)\n",
    "\n",
    "# Combine all samples\n",
    "all_validation = []\n",
    "for category, sample_df in validation_samples.items():\n",
    "    sample_copy = sample_df.copy()\n",
    "    sample_copy['validation_category'] = category\n",
    "    all_validation.append(sample_copy)\n",
    "\n",
    "validation_df = pd.concat(all_validation, ignore_index=True)\n",
    "\n",
    "# Remove duplicates if any post appears in multiple categories\n",
    "validation_df = validation_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Create human-readable output\n",
    "output_df = validation_df[[\n",
    "    'validation_category',\n",
    "    'text',\n",
    "    'escalation_score_haiku',\n",
    "    'escalation_score_sonnet', \n",
    "    'escalation_score_opus',\n",
    "    'score_variance'\n",
    "]].copy()\n",
    "\n",
    "# Add blank columns for human scoring\n",
    "output_df['human_escalation'] = ''\n",
    "output_df['human_blame'] = ''\n",
    "output_df['human_cta'] = ''\n",
    "output_df['human_notes'] = ''\n",
    "\n",
    "# Save full validation set\n",
    "output_df.to_csv(OUTPUT_DIR / 'focused_validation_sample.csv', index=False)\n",
    "\n",
    "# Create a simplified scoring sheet\n",
    "print(\"\\nğŸ“ Creating simplified scoring sheets...\")\n",
    "\n",
    "# Split into manageable chunks (10 posts per sheet)\n",
    "chunk_size = 10\n",
    "for i, chunk_start in enumerate(range(0, len(output_df), chunk_size)):\n",
    "    chunk = output_df.iloc[chunk_start:chunk_start + chunk_size]\n",
    "    \n",
    "    # Create a text file for easier reading\n",
    "    with open(OUTPUT_DIR / f'validation_batch_{i+1}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"VALIDATION BATCH {i+1}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for idx, row in chunk.iterrows():\n",
    "            f.write(f\"POST #{idx + 1}\\n\")\n",
    "            f.write(f\"Category: {row['validation_category']}\\n\")\n",
    "            f.write(f\"Text: {row['text'][:500]}{'...' if len(row['text']) > 500 else ''}\\n\")\n",
    "            f.write(f\"\\nModel Scores:\\n\")\n",
    "            f.write(f\"  Haiku:  {row['escalation_score_haiku']}\\n\")\n",
    "            f.write(f\"  Sonnet: {row['escalation_score_sonnet']}\\n\")\n",
    "            f.write(f\"  Opus:   {row['escalation_score_opus']}\\n\")\n",
    "            f.write(f\"\\nYour Scores:\\n\")\n",
    "            f.write(f\"  Escalation (0-10): _____\\n\")\n",
    "            f.write(f\"  Blame (-1/0/1): _____\\n\")\n",
    "            f.write(f\"  CTA (0/1): _____\\n\")\n",
    "            f.write(f\"  Notes: _______________________________________________\\n\")\n",
    "            f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nâœ… Extracted {len(validation_df)} posts for focused validation\")\n",
    "print(\"\\nğŸ“Š Sample distribution:\")\n",
    "for category, sample_df in validation_samples.items():\n",
    "    print(f\"   {category}: {len(sample_df)} posts\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Files saved to: {OUTPUT_DIR}\")\n",
    "print(\"   - focused_validation_sample.csv (full data)\")\n",
    "print(f\"   - validation_batch_1.txt through validation_batch_{(len(output_df)-1)//chunk_size + 1}.txt (readable format)\")\n",
    "\n",
    "# Show examples of key disagreement patterns\n",
    "print(\"\\nğŸ” Example disagreement patterns:\")\n",
    "\n",
    "print(\"\\n1. Haiku sees escalation, others don't:\")\n",
    "example1 = cat1.iloc[0] if len(cat1) > 0 else None\n",
    "if example1 is not None:\n",
    "    print(f\"   Text: {example1['text'][:150]}...\")\n",
    "    print(f\"   Scores - Haiku: {example1['escalation_score_haiku']}, Sonnet: {example1['escalation_score_sonnet']}, Opus: {example1['escalation_score_opus']}\")\n",
    "\n",
    "print(\"\\n2. Political content scoring:\")\n",
    "example2 = cat4.iloc[0] if len(cat4) > 0 else None\n",
    "if example2 is not None:\n",
    "    print(f\"   Text: {example2['text'][:150]}...\")\n",
    "    print(f\"   Scores - Haiku: {example2['escalation_score_haiku']}, Sonnet: {example2['escalation_score_sonnet']}, Opus: {example2['escalation_score_opus']}\")\n",
    "\n",
    "# Analysis of score patterns\n",
    "print(\"\\nğŸ“ˆ Score Pattern Analysis:\")\n",
    "print(f\"   Posts where Haiku > both others by 3+: {sum(df['haiku_outlier'])}\")\n",
    "print(f\"   Posts where Sonnet & Opus agree (Â±1): {sum(df['sonnet_opus_agree'])}\")\n",
    "print(f\"   Average Haiku score in high-variance set: {df['escalation_score_haiku'].mean():.2f}\")\n",
    "print(f\"   Average Sonnet score in high-variance set: {df['escalation_score_sonnet'].mean():.2f}\")\n",
    "print(f\"   Average Opus score in high-variance set: {df['escalation_score_opus'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PLOT Escalation Scores with Major Events Timeline Overlay            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HEADLINE_DAILY_CSV = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"\n",
    "TRUTH_DAILY_CSV = ROOT / \"outputs\" / \"truth_daily_escalation_scores.csv\"\n",
    "TIMELINE_JSON = ROOT / \"src\" / \"ukraine-war-timeline.json\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"timeline_analysis\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load headline data\n",
    "print(\"ğŸ“Š Loading escalation scores...\")\n",
    "headline_daily = pd.read_csv(HEADLINE_DAILY_CSV, parse_dates=['date'])\n",
    "headline_daily = headline_daily.set_index('date').sort_index()\n",
    "headline_roll7 = headline_daily['mean_score'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Load Truth Social data (best model - likely Sonnet or Opus based on analysis)\n",
    "truth_daily = pd.read_csv(TRUTH_DAILY_CSV, parse_dates=['created_at'])\n",
    "truth_daily.rename(columns={'created_at': 'date'}, inplace=True)\n",
    "truth_daily = truth_daily.set_index('date').sort_index()\n",
    "truth_roll7 = truth_daily['mean_score'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Load timeline events\n",
    "print(\"ğŸ“… Loading timeline events...\")\n",
    "events = []\n",
    "with open(TIMELINE_JSON, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            events.append(json.loads(line))\n",
    "\n",
    "# Convert events to DataFrame\n",
    "events_df = pd.DataFrame(events)\n",
    "events_df['date'] = pd.to_datetime(events_df['date'])\n",
    "\n",
    "# Filter only major events\n",
    "major_events = events_df[events_df['major'] == True].copy()\n",
    "\n",
    "# Create the main plot\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Plot rolling averages\n",
    "ax.plot(headline_roll7.index, headline_roll7.values, \n",
    "        'crimson', linewidth=2.5, label='News Headlines', alpha=0.9)\n",
    "ax.plot(truth_roll7.index, truth_roll7.values, \n",
    "        'navy', linewidth=2.5, label='Truth Social', alpha=0.9)\n",
    "\n",
    "# Add daily scatter points with lower opacity\n",
    "ax.scatter(headline_daily.index, headline_daily['mean_score'], \n",
    "           alpha=0.15, s=15, color='crimson')\n",
    "ax.scatter(truth_daily.index, truth_daily['mean_score'], \n",
    "           alpha=0.15, s=15, color='navy')\n",
    "\n",
    "# Add major event vertical lines and labels\n",
    "for _, event in major_events.iterrows():\n",
    "    event_date = event['date']\n",
    "    \n",
    "    # Only plot if within data range\n",
    "    if (event_date >= min(headline_roll7.index.min(), truth_roll7.index.min()) and \n",
    "        event_date <= max(headline_roll7.index.max(), truth_roll7.index.max())):\n",
    "        \n",
    "        # Add vertical line\n",
    "        ax.axvline(x=event_date, color='red', alpha=0.3, linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Add event label\n",
    "        # Alternate label positions to avoid overlap\n",
    "        y_position = ax.get_ylim()[1] * 0.95 if major_events.index.get_loc(event.name) % 2 == 0 else ax.get_ylim()[1] * 0.85\n",
    "        \n",
    "        ax.text(event_date, y_position, event['label'], \n",
    "                rotation=45, fontsize=8, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Escalation Score (0-10)', fontsize=12)\n",
    "ax.set_title('Ukraine War Escalation: News Headlines vs Truth Social with Major Events\\n(7-day rolling mean)', \n",
    "             fontsize=14, pad=20)\n",
    "ax.legend(fontsize=12, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, max(headline_roll7.max(), truth_roll7.max()) * 1.1)\n",
    "\n",
    "# Add annotations for key patterns\n",
    "ax.annotate('Truth Social\\nconsistently lower', \n",
    "            xy=(pd.Timestamp('2023-06-01'), 2.2), \n",
    "            xytext=(pd.Timestamp('2023-08-01'), 1.0),\n",
    "            arrowprops=dict(arrowstyle='->', color='navy', alpha=0.5),\n",
    "            fontsize=10, ha='center', color='navy')\n",
    "\n",
    "ax.annotate('Headlines spike\\nwith major events', \n",
    "            xy=(pd.Timestamp('2023-06-04'), headline_roll7.loc['2023-06-04']), \n",
    "            xytext=(pd.Timestamp('2023-04-01'), 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='crimson', alpha=0.5),\n",
    "            fontsize=10, ha='center', color='crimson')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'escalation_timeline_overlay.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create a focused plot for specific periods of interest\n",
    "print(\"\\nğŸ“Š Creating focused analysis plots...\")\n",
    "\n",
    "# Function to create period-specific plots\n",
    "def plot_period(start_date, end_date, title_suffix):\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Filter data for period\n",
    "    headline_period = headline_roll7[start_date:end_date]\n",
    "    truth_period = truth_roll7[start_date:end_date]\n",
    "    events_period = major_events[(major_events['date'] >= start_date) & \n",
    "                                 (major_events['date'] <= end_date)]\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(headline_period.index, headline_period.values, \n",
    "            'crimson', linewidth=2.5, label='News Headlines')\n",
    "    ax.plot(truth_period.index, truth_period.values, \n",
    "            'navy', linewidth=2.5, label='Truth Social')\n",
    "    \n",
    "    # Add events\n",
    "    for _, event in events_period.iterrows():\n",
    "        ax.axvline(x=event['date'], color='red', alpha=0.4, linestyle='--')\n",
    "        ax.text(event['date'], ax.get_ylim()[1] * 0.9, event['label'],\n",
    "                rotation=45, fontsize=9, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.6))\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Escalation Score (0-10)')\n",
    "    ax.set_title(f'Escalation Patterns: {title_suffix}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f\"escalation_period_{start_date.strftime('%Y%m')}_{end_date.strftime('%Y%m')}.png\"\n",
    "    plt.savefig(OUTPUT_DIR / filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create period-specific plots\n",
    "plot_period(pd.Timestamp('2023-05-01'), pd.Timestamp('2023-07-31'), \n",
    "            'Ukrainian Counteroffensive Period')\n",
    "plot_period(pd.Timestamp('2024-01-01'), pd.Timestamp('2024-03-31'), \n",
    "            'Russian Winter Offensive 2024')\n",
    "plot_period(pd.Timestamp('2024-11-01'), pd.Timestamp('2025-02-28'), \n",
    "            'Trump Election and Policy Shift')\n",
    "\n",
    "# Generate event impact analysis\n",
    "print(\"\\nğŸ“ˆ Analyzing event impacts...\")\n",
    "\n",
    "# Calculate average scores before/after major events\n",
    "event_impacts = []\n",
    "for _, event in major_events.iterrows():\n",
    "    event_date = event['date']\n",
    "    \n",
    "    # 7 days before and after\n",
    "    before_start = event_date - pd.Timedelta(days=14)\n",
    "    before_end = event_date - pd.Timedelta(days=1)\n",
    "    after_start = event_date + pd.Timedelta(days=1)\n",
    "    after_end = event_date + pd.Timedelta(days=14)\n",
    "    \n",
    "    # Calculate means if data exists\n",
    "    try:\n",
    "        headline_before = headline_daily.loc[before_start:before_end]['mean_score'].mean()\n",
    "        headline_after = headline_daily.loc[after_start:after_end]['mean_score'].mean()\n",
    "        truth_before = truth_daily.loc[before_start:before_end]['mean_score'].mean()\n",
    "        truth_after = truth_daily.loc[after_start:after_end]['mean_score'].mean()\n",
    "        \n",
    "        event_impacts.append({\n",
    "            'event': event['label'],\n",
    "            'date': event_date,\n",
    "            'headline_change': headline_after - headline_before,\n",
    "            'truth_change': truth_after - truth_before,\n",
    "            'headline_before': headline_before,\n",
    "            'headline_after': headline_after,\n",
    "            'truth_before': truth_before,\n",
    "            'truth_after': truth_after\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Save event impact analysis\n",
    "impact_df = pd.DataFrame(event_impacts)\n",
    "impact_df.to_csv(OUTPUT_DIR / 'event_impact_analysis.csv', index=False)\n",
    "\n",
    "print(\"\\nğŸ“Š Event Impact Summary:\")\n",
    "print(impact_df[['event', 'headline_change', 'truth_change']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ… All visualizations saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nğŸ“ Generated files:\")\n",
    "print(\"   - escalation_timeline_overlay.png (main comparison with all events)\")\n",
    "print(\"   - Period-specific analysis plots\")\n",
    "print(\"   - event_impact_analysis.csv (quantitative impact measures)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
