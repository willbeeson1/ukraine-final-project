{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Claude Haiku 3.5 Headline Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ANALYZE & VISUALIZE Ukraine War Escalation Scores Over Time          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# SET VARS\n",
    "MODEL_DATE = \"from_sonnet-4_ran_6-5-1-10\"\n",
    "\n",
    "# â”€â”€ Load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SCORES_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\" # changes source of individual days\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / f\"headline_analysis_plots_{MODEL_DATE}\" \n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Loading scored headlines...\")\n",
    "df = pd.read_csv(SCORES_CSV, parse_dates=['date'])\n",
    "df = df[df['score'].notna()]  # Remove any NaN scores\n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} scored headlines\")\n",
    "print(f\"ğŸ“… Date range: {df['date'].min().date()} to {df['date'].max().date()}\")\n",
    "\n",
    "# â”€â”€ Basic statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“ˆ Basic Statistics:\")\n",
    "print(f\"   Mean escalation score: {df['score'].mean():.2f}\")\n",
    "print(f\"   Median score: {df['score'].median():.0f}\")\n",
    "print(f\"   Std deviation: {df['score'].std():.2f}\")\n",
    "\n",
    "# â”€â”€ Calculate daily averages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "daily_avg = df.groupby(df['date'].dt.date).agg({\n",
    "    'score': ['mean', 'median', 'std', 'count']\n",
    "}).round(2)\n",
    "daily_avg.columns = ['mean_score', 'median_score', 'std_dev', 'count']\n",
    "daily_avg = daily_avg.reset_index()\n",
    "daily_avg['date'] = pd.to_datetime(daily_avg['date'])\n",
    "\n",
    "# Save daily averages\n",
    "daily_avg_file = ROOT / \"outputs\" / f\"daily_escalation_scores_{MODEL_DATE}.csv\"  # UPDATES TO INCLUDE MODEL\n",
    "daily_avg.to_csv(daily_avg_file, index=False)\n",
    "print(f\"\\nğŸ’¾ Saved daily averages to: {daily_avg_file.name}\")\n",
    "\n",
    "# â”€â”€ Create visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. Score Distribution Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['score'], bins=11, range=(-0.5, 10.5), edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Escalation Score')\n",
    "plt.ylabel('Number of Headlines')\n",
    "plt.title('Distribution of Escalation Scores (0-10)')\n",
    "plt.xticks(range(11))\n",
    "for i in range(11):\n",
    "    count = (df['score'] == i).sum()\n",
    "    plt.text(i, count + 200, str(count), ha='center', va='bottom')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'score_distribution_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Daily Average Escalation Score Over Time\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.plot(daily_avg['date'], daily_avg['mean_score'], linewidth=1.5, alpha=0.8)\n",
    "ax.fill_between(daily_avg['date'], \n",
    "                daily_avg['mean_score'] - daily_avg['std_dev'],\n",
    "                daily_avg['mean_score'] + daily_avg['std_dev'],\n",
    "                alpha=0.2, label='Â±1 std dev')\n",
    "\n",
    "# Add 30-day rolling average\n",
    "rolling_30 = daily_avg.set_index('date')['mean_score'].rolling('30D').mean()\n",
    "ax.plot(rolling_30.index, rolling_30.values, 'r-', linewidth=2, label='30-day average')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Escalation Score')\n",
    "ax.set_title('Daily Average Escalation Score Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'daily_escalation_trend_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Weekly aggregated view\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "weekly_stats = df.groupby('week').agg({\n",
    "    'score': ['mean', 'median', 'count']\n",
    "}).round(2)\n",
    "weekly_stats.columns = ['mean_score', 'median_score', 'count']\n",
    "weekly_stats = weekly_stats.reset_index()\n",
    "weekly_stats['week_start'] = weekly_stats['week'].apply(lambda x: x.start_time)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(weekly_stats['week_start'], weekly_stats['mean_score'], 'o-', label='Mean', markersize=4)\n",
    "plt.plot(weekly_stats['week_start'], weekly_stats['median_score'], 's-', label='Median', markersize=4)\n",
    "plt.xlabel('Week')\n",
    "plt.ylabel('Escalation Score')\n",
    "plt.title('Weekly Average Escalation Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'weekly_escalation_trend_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Heatmap of scores by day of week and hour\n",
    "df['hour'] = df['date'].dt.hour\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "pivot_heatmap = df.pivot_table(values='score', index='hour', columns='day_of_week', aggfunc='mean')\n",
    "pivot_heatmap = pivot_heatmap[day_order]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pivot_heatmap, cmap='YlOrRd', annot=True, fmt='.2f', cbar_kws={'label': 'Average Score'})\n",
    "plt.title('Average Escalation Score by Hour and Day of Week')\n",
    "plt.ylabel('Hour of Day')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'hourly_weekly_heatmap_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Monthly trends with box plots\n",
    "df['month'] = df['date'].dt.to_period('M')\n",
    "monthly_data = []\n",
    "for month in df['month'].unique():\n",
    "    month_scores = df[df['month'] == month]['score']\n",
    "    monthly_data.append(month_scores)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "box_positions = range(len(df['month'].unique()))\n",
    "plt.boxplot(monthly_data, positions=box_positions, widths=0.6)\n",
    "plt.xticks(box_positions, [str(m) for m in df['month'].unique()], rotation=45, ha='right')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Escalation Score')\n",
    "plt.title('Distribution of Escalation Scores by Month')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'monthly_boxplots_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Proportion of high-escalation headlines over time\n",
    "df['high_escalation'] = df['score'] >= 7\n",
    "daily_high = df.groupby(df['date'].dt.date).agg({\n",
    "    'high_escalation': ['sum', 'mean']\n",
    "})\n",
    "daily_high.columns = ['count', 'proportion']\n",
    "daily_high = daily_high.reset_index()\n",
    "daily_high['date'] = pd.to_datetime(daily_high['date'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Top plot: Count of high-escalation headlines\n",
    "ax1.plot(daily_high['date'], daily_high['count'], 'r-', alpha=0.7)\n",
    "ax1.fill_between(daily_high['date'], 0, daily_high['count'], alpha=0.3, color='red')\n",
    "ax1.set_ylabel('Count of High-Escalation Headlines (â‰¥7)')\n",
    "ax1.set_title('High-Escalation Headlines Over Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom plot: Proportion\n",
    "ax2.plot(daily_high['date'], daily_high['proportion'] * 100, 'b-', alpha=0.7)\n",
    "ax2.fill_between(daily_high['date'], 0, daily_high['proportion'] * 100, alpha=0.3, color='blue')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Percentage of High-Escalation Headlines')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'high_escalation_trends_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7. Key events analysis (identify days with highest average scores)\n",
    "top_days = daily_avg.nlargest(10, 'mean_score')[['date', 'mean_score', 'count']]\n",
    "print(\"\\nğŸ”¥ Top 10 Days with Highest Average Escalation:\")\n",
    "for _, row in top_days.iterrows():\n",
    "    print(f\"   {row['date'].date()}: {row['mean_score']:.2f} (n={row['count']})\")\n",
    "\n",
    "# Save this analysis\n",
    "top_days.to_csv(ROOT / \"outputs\" / f\"top_escalation_days_{MODEL_DATE}.csv\", index=False)\n",
    "\n",
    "# 8. Source analysis\n",
    "source_stats = df.groupby('source').agg({\n",
    "    'score': ['mean', 'count', 'std']\n",
    "}).round(2)\n",
    "source_stats.columns = ['mean_score', 'count', 'std_dev']\n",
    "source_stats = source_stats.sort_values('mean_score', ascending=False)\n",
    "source_stats.to_csv(ROOT / \"outputs\" / f\"source_escalation_scores_{MODEL_DATE}.csv\")\n",
    "\n",
    "print(\"\\nğŸ“° Top 5 Sources by Average Escalation Score:\")\n",
    "for source, row in source_stats.head().iterrows():\n",
    "    print(f\"   {source}: {row['mean_score']:.2f} (n={row['count']})\")\n",
    "\n",
    "# Final summary plot\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Subplot 1: Score distribution\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(df['score'], bins=11, range=(-0.5, 10.5), edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Score Distribution')\n",
    "\n",
    "# Subplot 2: Daily trend\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(daily_avg['date'], daily_avg['mean_score'], linewidth=1)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Avg Score')\n",
    "ax2.set_title('Daily Average Score')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Subplot 3: Monthly boxplot\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "monthly_labels = [str(m)[-7:] for m in df['month'].unique()][-12:]  # Last 12 months\n",
    "monthly_data_recent = monthly_data[-12:]\n",
    "ax3.boxplot(monthly_data_recent, labels=monthly_labels)\n",
    "ax3.set_xlabel('Month')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Monthly Score Distribution (Last 12 Months)')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Subplot 4: Source comparison (top 10)\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "top_sources = source_stats.head(10)\n",
    "ax4.barh(range(len(top_sources)), top_sources['mean_score'])\n",
    "ax4.set_yticks(range(len(top_sources)))\n",
    "ax4.set_yticklabels(top_sources.index)\n",
    "ax4.set_xlabel('Average Escalation Score')\n",
    "ax4.set_title('Top 10 Sources by Average Escalation Score')\n",
    "\n",
    "plt.suptitle('Ukraine War Headlines Escalation Analysis Summary', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / f'analysis_summary_{MODEL_DATE}.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Analysis complete! All plots saved to: {OUTPUT_DIR.name}/\")\n",
    "print(\"\\nğŸ“Š Generated visualizations:\")\n",
    "print(\"   1. score_distribution.png - Histogram of all scores\")\n",
    "print(\"   2. daily_escalation_trend.png - Daily averages with 30-day rolling mean\")\n",
    "print(\"   3. weekly_escalation_trend.png - Weekly aggregated view\")\n",
    "print(\"   4. hourly_weekly_heatmap.png - Patterns by hour and day of week\")\n",
    "print(\"   5. monthly_boxplots.png - Monthly distribution boxplots\")\n",
    "print(\"   6. high_escalation_trends.png - Tracking headlines with scores â‰¥7\")\n",
    "print(\"   7. analysis_summary.png - Combined summary dashboard\")\n",
    "print(\"\\nğŸ“„ Generated data files:\")\n",
    "print(\"   - daily_escalation_scores.csv\")\n",
    "print(\"   - top_escalation_days.csv\")\n",
    "print(\"   - source_escalation_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 7-Day Rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7-day rolling mean of *headline* escalation --------------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "DAILY_CSV_HAIKU = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"  # produced earlier\n",
    "DAILY_CSV_SONNET = ROOT / \"outputs\" / f\"daily_escalation_scores_{MODEL_DATE}.csv\"\n",
    "\n",
    "dailyH = pd.read_csv(DAILY_CSV_HAIKU, parse_dates=[\"date\"])\n",
    "dailyH = dailyH.set_index(\"date\").sort_index()\n",
    "\n",
    "dailyS = pd.read_csv(DAILY_CSV_SONNET, parse_dates=[\"date\"])\n",
    "dailyS = dailyS.set_index(\"date\").sort_index()\n",
    "\n",
    "# 7-day centred rolling mean\n",
    "dailyH[\"roll7\"] = dailyH[\"mean_score\"].rolling(window=7, center=True).mean()\n",
    "dailyS[\"roll7\"] = dailyS[\"mean_score\"].rolling(window=7, center=True).mean()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(dailyH.index, dailyH[\"roll7\"], lw=2, color=\"crimson\", label=\"Haiku 7-day mean\")\n",
    "plt.plot(dailyS.index, dailyS[\"roll7\"], lw=2, color=\"blue\", label=\"Sonnet 7-day mean\")\n",
    "\n",
    "plt.scatter(dailyH.index, dailyH[\"mean_score\"], s=8, alpha=0.3, label=\"Haiku daily mean\")\n",
    "plt.scatter(dailyS.index, dailyS[\"mean_score\"], s=8, alpha=0.3, label=\"Sonnet daily mean\")\n",
    "\n",
    "plt.title(\"Headline escalation index â€“ 7-day rolling mean\")\n",
    "plt.ylabel(\"escalation score (0-10)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Claude Haiku 3.5 Truth Social Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ANALYZE & VISUALIZE Truth Social Escalation Scores Over Time         â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "TRUTH_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_3-5-haiku-20241022.csv\"\n",
    "HEADLINE_DAILY_CSV = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"truth_analysis_plots\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# â”€â”€ Load Truth Social data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"ğŸ“Š Loading scored Truth Social posts...\")\n",
    "df = pd.read_csv(TRUTH_CSV)\n",
    "\n",
    "# Convert created_at to datetime with flexible format handling\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], format='mixed', utc=True)\n",
    "\n",
    "# Remove any rows with NaN scores\n",
    "df = df[df['escalation_score'].notna()]  \n",
    "\n",
    "print(f\"âœ… Loaded {len(df):,} scored posts\")\n",
    "print(f\"ğŸ“… Date range: {df['created_at'].min().date()} to {df['created_at'].max().date()}\")\n",
    "\n",
    "# â”€â”€ Basic statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“ˆ Basic Statistics:\")\n",
    "print(f\"   Mean escalation score: {df['escalation_score'].mean():.2f}\")\n",
    "print(f\"   Median score: {df['escalation_score'].median():.0f}\")\n",
    "print(f\"   Std deviation: {df['escalation_score'].std():.2f}\")\n",
    "\n",
    "# Blame direction stats\n",
    "blame_counts = df['blame_direction'].value_counts().sort_index()\n",
    "print(\"\\nğŸ¯ Blame Direction Distribution:\")\n",
    "for direction, count in blame_counts.items():\n",
    "    label = {-1: \"No clear blame\", 0: \"Ukraine/NATO/West\", 1: \"Russia/Putin\"}[direction]\n",
    "    print(f\"   {label}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Call-to-action stats\n",
    "cta_pct = df['has_cta'].mean() * 100\n",
    "print(f\"\\nğŸ“¢ Call-to-Action: {df['has_cta'].sum():,} posts ({cta_pct:.1f}%) have CTAs\")\n",
    "\n",
    "# â”€â”€ Calculate daily averages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "daily_avg = df.groupby(df['created_at'].dt.date).agg({\n",
    "    'escalation_score': ['mean', 'median', 'std', 'count'],\n",
    "    'blame_direction': lambda x: (x == 0).mean(),  # % blaming Ukraine/West\n",
    "    'has_cta': 'mean'  # % with CTA\n",
    "}).round(2)\n",
    "daily_avg.columns = ['mean_score', 'median_score', 'std_dev', 'count', 'pct_blame_west', 'pct_cta']\n",
    "daily_avg = daily_avg.reset_index()\n",
    "daily_avg['created_at'] = pd.to_datetime(daily_avg['created_at'])\n",
    "\n",
    "# Save daily averages\n",
    "daily_avg_file = ROOT / \"outputs\" / \"truth_daily_escalation_scores.csv\"\n",
    "daily_avg.to_csv(daily_avg_file, index=False)\n",
    "print(f\"\\nğŸ’¾ Saved daily averages to: {daily_avg_file.name}\")\n",
    "\n",
    "# â”€â”€ Create visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. Score Distribution Histogram with comparison to headlines\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['escalation_score'], bins=11, range=(-0.5, 10.5), \n",
    "         edgecolor='black', alpha=0.7, label='Truth Social')\n",
    "plt.xlabel('Escalation Score')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.title('Distribution of Truth Social Escalation Scores (0-10)')\n",
    "plt.xticks(range(11))\n",
    "for i in range(11):\n",
    "    count = (df['escalation_score'] == i).sum()\n",
    "    plt.text(i, count + 50, str(count), ha='center', va='bottom', fontsize=9)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_score_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 2. Daily Average with 7-day and 30-day rolling averages\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "ax.scatter(daily_avg['created_at'], daily_avg['mean_score'], \n",
    "           alpha=0.3, s=20, color='gray', label='Daily mean')\n",
    "\n",
    "# Calculate rolling averages\n",
    "daily_avg_indexed = daily_avg.set_index('created_at').sort_index()\n",
    "rolling_7 = daily_avg_indexed['mean_score'].rolling('7D', center=True).mean()\n",
    "rolling_30 = daily_avg_indexed['mean_score'].rolling('30D').mean()\n",
    "\n",
    "ax.plot(rolling_7.index, rolling_7.values, 'b-', linewidth=2, label='7-day rolling mean')\n",
    "ax.plot(rolling_30.index, rolling_30.values, 'r-', linewidth=2, label='30-day rolling mean')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Average Escalation Score')\n",
    "ax.set_title('Truth Social: Daily Average Escalation Score Over Time')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_daily_escalation_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. OVERLAY PLOT: Truth Social vs Headlines (7-day rolling)\n",
    "print(\"\\nğŸ“Š Creating overlay comparison plot...\")\n",
    "\n",
    "# Load headline data\n",
    "headline_daily = pd.read_csv(HEADLINE_DAILY_CSV, parse_dates=['date'])\n",
    "headline_daily = headline_daily.set_index('date').sort_index()\n",
    "headline_roll7 = headline_daily['mean_score'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Prepare Truth Social 7-day rolling\n",
    "truth_roll7 = rolling_7\n",
    "\n",
    "# Create overlay plot\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Plot both series\n",
    "ax.plot(headline_roll7.index, headline_roll7.values, \n",
    "        'crimson', linewidth=2.5, label='Headlines (News Media)', alpha=0.8)\n",
    "ax.plot(truth_roll7.index, truth_roll7.values, \n",
    "        'navy', linewidth=2.5, label='Truth Social Posts', alpha=0.8)\n",
    "\n",
    "# Add scatter points for daily values\n",
    "ax.scatter(headline_daily.index, headline_daily['mean_score'], \n",
    "           alpha=0.15, s=10, color='crimson')\n",
    "ax.scatter(daily_avg['created_at'], daily_avg['mean_score'], \n",
    "           alpha=0.15, s=10, color='navy')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Escalation Score (0-10)', fontsize=12)\n",
    "ax.set_title('Ukraine War Escalation: Headlines vs Truth Social (7-day rolling mean)', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations for key differences\n",
    "ax.annotate('Truth Social\\nlower baseline', \n",
    "            xy=(pd.Timestamp('2023-06-01'), 2.2), \n",
    "            xytext=(pd.Timestamp('2023-06-01'), 1.0),\n",
    "            arrowprops=dict(arrowstyle='->', color='navy', alpha=0.5),\n",
    "            fontsize=10, ha='center', color='navy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'overlay_headlines_vs_truth.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()  # Display in notebook\n",
    "plt.close()\n",
    "\n",
    "# 4. Blame direction over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Top: Percentage blaming West/Ukraine\n",
    "rolling_blame = daily_avg_indexed['pct_blame_west'].rolling('7D', center=True).mean() * 100\n",
    "ax1.plot(rolling_blame.index, rolling_blame.values, 'orange', linewidth=2)\n",
    "ax1.fill_between(rolling_blame.index, 0, rolling_blame.values, alpha=0.3, color='orange')\n",
    "ax1.set_ylabel('% Posts Blaming Ukraine/West')\n",
    "ax1.set_title('Truth Social: Blame Attribution Over Time (7-day rolling)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom: Percentage with Call-to-Action\n",
    "rolling_cta = daily_avg_indexed['pct_cta'].rolling('7D', center=True).mean() * 100\n",
    "ax2.plot(rolling_cta.index, rolling_cta.values, 'green', linewidth=2)\n",
    "ax2.fill_between(rolling_cta.index, 0, rolling_cta.values, alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('% Posts with Call-to-Action')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_blame_cta_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Correlation analysis between dimensions\n",
    "corr_matrix = df[['escalation_score', 'blame_direction', 'has_cta']].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Between Truth Social Scoring Dimensions')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Monthly comparison\n",
    "df['month'] = df['created_at'].dt.to_period('M')\n",
    "monthly_stats = df.groupby('month').agg({\n",
    "    'escalation_score': ['mean', 'std', 'count'],\n",
    "    'has_cta': 'mean'\n",
    "}).round(2)\n",
    "monthly_stats.columns = ['mean_score', 'std_score', 'count', 'pct_cta']\n",
    "monthly_stats = monthly_stats.reset_index()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Top: Monthly mean scores with error bars\n",
    "months = [str(m) for m in monthly_stats['month']]\n",
    "x_pos = range(len(months))\n",
    "ax1.errorbar(x_pos, monthly_stats['mean_score'], yerr=monthly_stats['std_score'], \n",
    "             marker='o', capsize=5, capthick=2, linewidth=2)\n",
    "ax1.set_ylabel('Mean Escalation Score')\n",
    "ax1.set_title('Truth Social: Monthly Average Escalation Scores')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Bottom: Monthly CTA percentage\n",
    "ax2.bar(x_pos, monthly_stats['pct_cta'] * 100, alpha=0.7, color='green')\n",
    "ax2.set_xlabel('Month')\n",
    "ax2.set_ylabel('% Posts with CTA')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(months, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_monthly_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 7. High-escalation analysis (scores >= 7)\n",
    "df['high_escalation'] = df['escalation_score'] >= 7\n",
    "daily_high = df.groupby(df['created_at'].dt.date).agg({\n",
    "    'high_escalation': ['sum', 'mean']\n",
    "})\n",
    "daily_high.columns = ['count', 'proportion']\n",
    "daily_high = daily_high.reset_index()\n",
    "daily_high['created_at'] = pd.to_datetime(daily_high['created_at'])\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "daily_high_indexed = daily_high.set_index('created_at')\n",
    "rolling_high = daily_high_indexed['proportion'].rolling('7D', center=True).mean() * 100\n",
    "\n",
    "plt.plot(rolling_high.index, rolling_high.values, 'red', linewidth=2)\n",
    "plt.fill_between(rolling_high.index, 0, rolling_high.values, alpha=0.3, color='red')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('% High-Escalation Posts (â‰¥7)')\n",
    "plt.title('Truth Social: Proportion of High-Escalation Posts (7-day rolling)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_high_escalation_trend.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 8. Top escalation days\n",
    "top_days = daily_avg.nlargest(10, 'mean_score')[['created_at', 'mean_score', 'count']]\n",
    "print(\"\\nğŸ”¥ Top 10 Days with Highest Average Escalation (Truth Social):\")\n",
    "for _, row in top_days.iterrows():\n",
    "    print(f\"   {row['created_at'].date()}: {row['mean_score']:.2f} (n={row['count']})\")\n",
    "\n",
    "# Save top days\n",
    "top_days.to_csv(ROOT / \"outputs\" / \"truth_top_escalation_days.csv\", index=False)\n",
    "\n",
    "# 9. Summary statistics comparison\n",
    "print(\"\\nğŸ“Š COMPARATIVE SUMMARY: Truth Social vs Headlines\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate headline stats for comparison\n",
    "if HEADLINE_DAILY_CSV.exists():\n",
    "    headline_scores = pd.read_csv(ROOT / \"outputs\" / \"headline_scores_partial.csv\")\n",
    "    \n",
    "    print(f\"\\nMean Escalation Score:\")\n",
    "    print(f\"   Headlines: {headline_scores['score'].mean():.2f}\")\n",
    "    print(f\"   Truth Social: {df['escalation_score'].mean():.2f}\")\n",
    "    print(f\"   Difference: {df['escalation_score'].mean() - headline_scores['score'].mean():.2f}\")\n",
    "    \n",
    "    print(f\"\\nStandard Deviation:\")\n",
    "    print(f\"   Headlines: {headline_scores['score'].std():.2f}\")\n",
    "    print(f\"   Truth Social: {df['escalation_score'].std():.2f}\")\n",
    "\n",
    "# Final summary dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Subplot 1: Score distribution\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.hist(df['escalation_score'], bins=11, range=(-0.5, 10.5), \n",
    "         edgecolor='black', alpha=0.7, color='navy')\n",
    "ax1.set_xlabel('Score')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Escalation Score Distribution')\n",
    "\n",
    "# Subplot 2: Blame direction pie chart\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "blame_counts = df['blame_direction'].value_counts()\n",
    "labels = ['No blame', 'Ukraine/West', 'Russia']\n",
    "colors = ['gray', 'orange', 'red']\n",
    "wedges, texts, autotexts = ax2.pie(blame_counts.values, labels=labels, colors=colors, \n",
    "                                    autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Blame Attribution')\n",
    "\n",
    "# Subplot 3: CTA distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "cta_counts = df['has_cta'].value_counts()\n",
    "ax3.bar(['No CTA', 'Has CTA'], cta_counts.values, color=['lightgray', 'green'])\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_title('Call-to-Action Distribution')\n",
    "\n",
    "# Subplot 4: Daily trend (full width)\n",
    "ax4 = fig.add_subplot(gs[1, :])\n",
    "ax4.plot(rolling_7.index, rolling_7.values, 'navy', linewidth=2, label='Truth Social')\n",
    "if 'headline_roll7' in locals():\n",
    "    ax4.plot(headline_roll7.index, headline_roll7.values, 'crimson', \n",
    "             linewidth=2, label='Headlines', alpha=0.7)\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('7-day Rolling Mean')\n",
    "ax4.set_title('Escalation Score Trends Comparison')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Monthly averages\n",
    "ax5 = fig.add_subplot(gs[2, :2])\n",
    "ax5.plot(range(len(monthly_stats)), monthly_stats['mean_score'], 'o-', linewidth=2)\n",
    "ax5.set_xticks(range(0, len(monthly_stats), 3))\n",
    "ax5.set_xticklabels([str(m) for m in monthly_stats['month']][::3], rotation=45)\n",
    "ax5.set_xlabel('Month')\n",
    "ax5.set_ylabel('Mean Score')\n",
    "ax5.set_title('Monthly Average Escalation')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Key metrics\n",
    "ax6 = fig.add_subplot(gs[2, 2])\n",
    "ax6.axis('off')\n",
    "metrics_text = f\"\"\"Key Metrics:\n",
    "\n",
    "Total Posts: {len(df):,}\n",
    "Date Range: {df['created_at'].min().date()} to {df['created_at'].max().date()}\n",
    "\n",
    "Mean Score: {df['escalation_score'].mean():.2f}\n",
    "Median: {df['escalation_score'].median():.0f}\n",
    "Std Dev: {df['escalation_score'].std():.2f}\n",
    "\n",
    "High Escalation (â‰¥7): {(df['escalation_score'] >= 7).sum():,} ({(df['escalation_score'] >= 7).mean()*100:.1f}%)\n",
    "Has CTA: {df['has_cta'].sum():,} ({df['has_cta'].mean()*100:.1f}%)\n",
    "Blames West: {(df['blame_direction'] == 0).sum():,} ({(df['blame_direction'] == 0).mean()*100:.1f}%)\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.9, metrics_text, transform=ax6.transAxes, fontsize=10, \n",
    "         verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.suptitle('Truth Social Ukraine War Posts: Comprehensive Analysis', fontsize=16, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'truth_analysis_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nâœ… Analysis complete! All plots saved to: {OUTPUT_DIR.name}/\")\n",
    "print(\"\\nğŸ“Š Generated visualizations:\")\n",
    "print(\"   1. truth_score_distribution.png - Histogram of escalation scores\")\n",
    "print(\"   2. truth_daily_escalation_trend.png - Daily averages with rolling means\")\n",
    "print(\"   3. overlay_headlines_vs_truth.png - COMPARISON WITH HEADLINES\")\n",
    "print(\"   4. truth_blame_cta_trends.png - Blame and CTA patterns over time\")\n",
    "print(\"   5. truth_correlation_matrix.png - Correlation between dimensions\")\n",
    "print(\"   6. truth_monthly_trends.png - Monthly aggregated view\")\n",
    "print(\"   7. truth_high_escalation_trend.png - High escalation posts tracking\")\n",
    "print(\"   8. truth_analysis_summary.png - Comprehensive dashboard\")\n",
    "print(\"\\nğŸ“„ Generated data files:\")\n",
    "print(\"   - truth_daily_escalation_scores.csv\")\n",
    "print(\"   - truth_top_escalation_days.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Anthropic Model Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  THREE-MODEL COMPARISON: Haiku 3.5 vs Sonnet 4 vs Opus 4             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HAIKU_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_3-5-haiku-20241022.csv\"\n",
    "SONNET_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "OPUS_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-opus-4-20250514.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"three_model_comparison\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Loading model outputs...\")\n",
    "# Load all three datasets\n",
    "haiku_df = pd.read_csv(HAIKU_CSV)\n",
    "sonnet_df = pd.read_csv(SONNET_CSV)\n",
    "opus_df = pd.read_csv(OPUS_CSV)\n",
    "\n",
    "# Merge on common identifier columns\n",
    "merge_cols = ['created_at', 'account', 'id', 'text']\n",
    "\n",
    "# First merge Haiku and Sonnet\n",
    "comparison_df = pd.merge(\n",
    "    haiku_df[merge_cols + ['escalation_score', 'blame_direction', 'has_cta']],\n",
    "    sonnet_df[merge_cols + ['escalation_score', 'blame_direction', 'has_cta']],\n",
    "    on=merge_cols,\n",
    "    suffixes=('_haiku', '_sonnet'),\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Then merge with Opus\n",
    "comparison_df = pd.merge(\n",
    "    comparison_df,\n",
    "    opus_df[merge_cols + ['escalation_score', 'blame_direction', 'has_cta']],\n",
    "    on=merge_cols,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Rename Opus columns for consistency\n",
    "comparison_df.rename(columns={\n",
    "    'escalation_score': 'escalation_score_opus',\n",
    "    'blame_direction': 'blame_direction_opus',\n",
    "    'has_cta': 'has_cta_opus'\n",
    "}, inplace=True)\n",
    "\n",
    "print(f\"âœ… Matched {len(comparison_df)} posts scored by all three models\")\n",
    "\n",
    "# Calculate pairwise differences\n",
    "comparison_df['diff_haiku_sonnet'] = comparison_df['escalation_score_haiku'] - comparison_df['escalation_score_sonnet']\n",
    "comparison_df['diff_haiku_opus'] = comparison_df['escalation_score_haiku'] - comparison_df['escalation_score_opus']\n",
    "comparison_df['diff_sonnet_opus'] = comparison_df['escalation_score_sonnet'] - comparison_df['escalation_score_opus']\n",
    "\n",
    "# Agreement metrics\n",
    "print(\"\\nğŸ“ˆ Pairwise Agreement Statistics:\")\n",
    "model_pairs = [('haiku', 'sonnet'), ('haiku', 'opus'), ('sonnet', 'opus')]\n",
    "for m1, m2 in model_pairs:\n",
    "    esc_corr = comparison_df[f'escalation_score_{m1}'].corr(comparison_df[f'escalation_score_{m2}'])\n",
    "    blame_agree = (comparison_df[f'blame_direction_{m1}'] == comparison_df[f'blame_direction_{m2}']).mean()\n",
    "    cta_agree = (comparison_df[f'has_cta_{m1}'] == comparison_df[f'has_cta_{m2}']).mean()\n",
    "    \n",
    "    print(f\"\\n{m1.capitalize()} vs {m2.capitalize()}:\")\n",
    "    print(f\"   Escalation correlation: {esc_corr:.3f}\")\n",
    "    print(f\"   Blame agreement: {blame_agree*100:.1f}%\")\n",
    "    print(f\"   CTA agreement: {cta_agree*100:.1f}%\")\n",
    "\n",
    "# Overall statistics by model\n",
    "print(\"\\nğŸ“Š Model Statistics:\")\n",
    "for model in ['haiku', 'sonnet', 'opus']:\n",
    "    esc_mean = comparison_df[f'escalation_score_{model}'].mean()\n",
    "    esc_std = comparison_df[f'escalation_score_{model}'].std()\n",
    "    blame_west = (comparison_df[f'blame_direction_{model}'] == 0).mean() * 100\n",
    "    blame_russia = (comparison_df[f'blame_direction_{model}'] == 1).mean() * 100\n",
    "    has_cta = comparison_df[f'has_cta_{model}'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{model.capitalize()}:\")\n",
    "    print(f\"   Escalation: mean={esc_mean:.2f}, std={esc_std:.2f}\")\n",
    "    print(f\"   Blames West: {blame_west:.1f}%\")\n",
    "    print(f\"   Blames Russia: {blame_russia:.1f}%\")\n",
    "    print(f\"   Has CTA: {has_cta:.1f}%\")\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "fig.suptitle('Three-Model Comparison: Haiku 3.5 vs Sonnet 4 vs Opus 4', fontsize=18)\n",
    "\n",
    "# Row 1: Pairwise escalation scatter plots\n",
    "for i, (m1, m2) in enumerate(model_pairs):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    ax.scatter(comparison_df[f'escalation_score_{m1}'], \n",
    "               comparison_df[f'escalation_score_{m2}'],\n",
    "               alpha=0.3, s=10)\n",
    "    ax.plot([0, 10], [0, 10], 'r--', alpha=0.5)\n",
    "    ax.set_xlabel(f'{m1.capitalize()} Score')\n",
    "    ax.set_ylabel(f'{m2.capitalize()} Score')\n",
    "    ax.set_title(f'{m1.capitalize()} vs {m2.capitalize()}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add correlation text\n",
    "    corr = comparison_df[f'escalation_score_{m1}'].corr(comparison_df[f'escalation_score_{m2}'])\n",
    "    ax.text(0.05, 0.95, f'r = {corr:.3f}', transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Row 2: Escalation score distributions\n",
    "ax = fig.add_subplot(gs[1, :])\n",
    "models = ['haiku', 'sonnet', 'opus']\n",
    "positions = np.arange(11)\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    counts = comparison_df[f'escalation_score_{model}'].value_counts().sort_index()\n",
    "    counts = counts.reindex(range(11), fill_value=0)\n",
    "    ax.bar(positions + i*width, counts.values, width, label=model.capitalize(), alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Escalation Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Escalation Score Distributions by Model')\n",
    "ax.set_xticks(positions + width)\n",
    "ax.set_xticklabels(positions)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Row 3: Blame direction comparison\n",
    "for i, model in enumerate(models):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    blame_counts = comparison_df[f'blame_direction_{model}'].value_counts()\n",
    "    labels = ['No blame', 'West/NATO', 'Russia']\n",
    "    label_map = {-1: 'No blame', 0: 'West/NATO', 1: 'Russia'}\n",
    "    sizes = [blame_counts.get(j, 0) for j in [-1, 0, 1]]\n",
    "    colors = ['gray', 'orange', 'red']\n",
    "    \n",
    "    wedges, texts, autotexts = ax.pie(sizes, labels=labels, colors=colors, \n",
    "                                       autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title(f'{model.capitalize()} - Blame Attribution')\n",
    "\n",
    "# Row 4: Three-way agreement analysis\n",
    "ax1 = fig.add_subplot(gs[3, 0])\n",
    "# Calculate where all three models agree within 1 point\n",
    "all_agree_esc = ((abs(comparison_df['diff_haiku_sonnet']) <= 1) & \n",
    "                 (abs(comparison_df['diff_haiku_opus']) <= 1) & \n",
    "                 (abs(comparison_df['diff_sonnet_opus']) <= 1)).mean() * 100\n",
    "\n",
    "all_agree_blame = ((comparison_df['blame_direction_haiku'] == comparison_df['blame_direction_sonnet']) & \n",
    "                   (comparison_df['blame_direction_haiku'] == comparison_df['blame_direction_opus'])).mean() * 100\n",
    "\n",
    "all_agree_cta = ((comparison_df['has_cta_haiku'] == comparison_df['has_cta_sonnet']) & \n",
    "                 (comparison_df['has_cta_haiku'] == comparison_df['has_cta_opus'])).mean() * 100\n",
    "\n",
    "agreement_data = [all_agree_esc, all_agree_blame, all_agree_cta]\n",
    "agreement_labels = ['Escalation\\n(within Â±1)', 'Blame\\nDirection', 'Call to\\nAction']\n",
    "\n",
    "bars = ax1.bar(agreement_labels, agreement_data, color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "ax1.set_ylabel('Agreement Rate (%)')\n",
    "ax1.set_title('Three-Way Agreement Rates')\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, agreement_data):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "             f'{value:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "# Systematic bias analysis\n",
    "ax2 = fig.add_subplot(gs[3, 1])\n",
    "mean_scores = [comparison_df[f'escalation_score_{m}'].mean() for m in models]\n",
    "ax2.bar(models, mean_scores, color=['lightblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
    "ax2.set_ylabel('Mean Escalation Score')\n",
    "ax2.set_title('Average Escalation by Model')\n",
    "ax2.set_ylim(0, 3)\n",
    "\n",
    "for i, (model, score) in enumerate(zip(models, mean_scores)):\n",
    "    ax2.text(i, score + 0.05, f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Variance in scoring\n",
    "ax3 = fig.add_subplot(gs[3, 2])\n",
    "variance_data = comparison_df[['escalation_score_haiku', 'escalation_score_sonnet', 'escalation_score_opus']].var(axis=1)\n",
    "ax3.hist(variance_data, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax3.set_xlabel('Variance in Scores')\n",
    "ax3.set_ylabel('Number of Posts')\n",
    "ax3.set_title('Distribution of Score Variance Across Models')\n",
    "ax3.axvline(variance_data.mean(), color='red', linestyle='--', label=f'Mean: {variance_data.mean():.2f}')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'three_model_comparison_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify posts with high disagreement\n",
    "high_variance = comparison_df[variance_data > variance_data.quantile(0.95)].copy()\n",
    "high_variance['score_variance'] = variance_data[variance_data > variance_data.quantile(0.95)]\n",
    "print(f\"\\nâš ï¸  Found {len(high_variance)} posts with high variance (top 5%)\")\n",
    "\n",
    "# Save comparison data\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'three_model_comparison_full.csv', index=False)\n",
    "high_variance[['text', 'escalation_score_haiku', 'escalation_score_sonnet', \n",
    "               'escalation_score_opus', 'score_variance']].to_csv(\n",
    "    OUTPUT_DIR / 'high_variance_posts.csv', index=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Results saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Recommended model selection\n",
    "print(\"\\nğŸ¯ Model Selection Guidance:\")\n",
    "print(\"\\nBased on the analysis:\")\n",
    "print(\"- Haiku 3.5: Highest escalation scores, strongest West-blame attribution\")\n",
    "print(\"- Sonnet 4: Lowest blame attribution, very low escalation\")\n",
    "print(\"- Opus 4: Middle ground on blame, lowest escalation scores\")\n",
    "print(\"\\nRecommendation: Validate a sample from high-variance posts to determine\")\n",
    "print(\"which model best aligns with human judgment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### High Variance Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  EXTRACT FOCUSED VALIDATION SAMPLE FROM HIGH-VARIANCE POSTS           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HIGH_VAR_CSV = ROOT / \"outputs\" / \"three_model_comparison\" / \"high_variance_posts.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"focused_validation\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load high variance posts\n",
    "print(\"ğŸ“Š Loading high-variance posts...\")\n",
    "df = pd.read_csv(HIGH_VAR_CSV)\n",
    "print(f\"âœ… Loaded {len(df)} high-variance posts\")\n",
    "\n",
    "# Add analysis columns\n",
    "df['max_score'] = df[['escalation_score_haiku', 'escalation_score_sonnet', 'escalation_score_opus']].max(axis=1)\n",
    "df['min_score'] = df[['escalation_score_haiku', 'escalation_score_sonnet', 'escalation_score_opus']].min(axis=1)\n",
    "df['score_range'] = df['max_score'] - df['min_score']\n",
    "\n",
    "# Categorize disagreement patterns\n",
    "df['haiku_outlier'] = (\n",
    "    (df['escalation_score_haiku'] > df['escalation_score_sonnet'] + 2) & \n",
    "    (df['escalation_score_haiku'] > df['escalation_score_opus'] + 2)\n",
    ")\n",
    "\n",
    "df['sonnet_opus_agree'] = abs(df['escalation_score_sonnet'] - df['escalation_score_opus']) <= 1\n",
    "\n",
    "# Define validation categories\n",
    "validation_samples = {}\n",
    "\n",
    "# Category 1: Haiku scores high (4+) while both others score low (0-1)\n",
    "cat1 = df[(df['escalation_score_haiku'] >= 4) & \n",
    "          (df['escalation_score_sonnet'] <= 1) & \n",
    "          (df['escalation_score_opus'] <= 1)]\n",
    "validation_samples['haiku_high_others_low'] = cat1.head(10)\n",
    "\n",
    "# Category 2: All three disagree significantly\n",
    "cat2 = df[(df['score_range'] >= 4) & (~df['sonnet_opus_agree'])]\n",
    "validation_samples['all_disagree'] = cat2.head(10)\n",
    "\n",
    "# Category 3: Sonnet and Opus agree but Haiku differs by 3+\n",
    "cat3 = df[df['sonnet_opus_agree'] & (abs(df['escalation_score_haiku'] - df['escalation_score_opus']) >= 3)]\n",
    "validation_samples['haiku_outlier_sonnet_opus_agree'] = cat3.head(10)\n",
    "\n",
    "# Category 4: Posts with \"Biden\" or \"Trump\" to check political vs military scoring\n",
    "political_keywords = df[df['text'].str.contains('Biden|Trump|Democrat|Republican|MAGA', case=False, na=False)]\n",
    "cat4 = political_keywords[political_keywords['score_variance'] > 3]\n",
    "validation_samples['political_content'] = cat4.head(10)\n",
    "\n",
    "# Category 5: Posts with explicit war/military language\n",
    "military_keywords = df[df['text'].str.contains('nuclear|missile|weapon|bomb|attack|strike', case=False, na=False)]\n",
    "cat5 = military_keywords[military_keywords['score_variance'] > 3]\n",
    "validation_samples['military_content'] = cat5.head(5)\n",
    "\n",
    "# Combine all samples\n",
    "all_validation = []\n",
    "for category, sample_df in validation_samples.items():\n",
    "    sample_copy = sample_df.copy()\n",
    "    sample_copy['validation_category'] = category\n",
    "    all_validation.append(sample_copy)\n",
    "\n",
    "validation_df = pd.concat(all_validation, ignore_index=True)\n",
    "\n",
    "# Remove duplicates if any post appears in multiple categories\n",
    "validation_df = validation_df.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Create human-readable output\n",
    "output_df = validation_df[[\n",
    "    'validation_category',\n",
    "    'text',\n",
    "    'escalation_score_haiku',\n",
    "    'escalation_score_sonnet', \n",
    "    'escalation_score_opus',\n",
    "    'score_variance'\n",
    "]].copy()\n",
    "\n",
    "# Add blank columns for human scoring\n",
    "output_df['human_escalation'] = ''\n",
    "output_df['human_blame'] = ''\n",
    "output_df['human_cta'] = ''\n",
    "output_df['human_notes'] = ''\n",
    "\n",
    "# Save full validation set\n",
    "output_df.to_csv(OUTPUT_DIR / 'focused_validation_sample.csv', index=False)\n",
    "\n",
    "# Create a simplified scoring sheet\n",
    "print(\"\\nğŸ“ Creating simplified scoring sheets...\")\n",
    "\n",
    "# Split into manageable chunks (10 posts per sheet)\n",
    "chunk_size = 10\n",
    "for i, chunk_start in enumerate(range(0, len(output_df), chunk_size)):\n",
    "    chunk = output_df.iloc[chunk_start:chunk_start + chunk_size]\n",
    "    \n",
    "    # Create a text file for easier reading\n",
    "    with open(OUTPUT_DIR / f'validation_batch_{i+1}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"VALIDATION BATCH {i+1}\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for idx, row in chunk.iterrows():\n",
    "            f.write(f\"POST #{idx + 1}\\n\")\n",
    "            f.write(f\"Category: {row['validation_category']}\\n\")\n",
    "            f.write(f\"Text: {row['text'][:500]}{'...' if len(row['text']) > 500 else ''}\\n\")\n",
    "            f.write(f\"\\nModel Scores:\\n\")\n",
    "            f.write(f\"  Haiku:  {row['escalation_score_haiku']}\\n\")\n",
    "            f.write(f\"  Sonnet: {row['escalation_score_sonnet']}\\n\")\n",
    "            f.write(f\"  Opus:   {row['escalation_score_opus']}\\n\")\n",
    "            f.write(f\"\\nYour Scores:\\n\")\n",
    "            f.write(f\"  Escalation (0-10): _____\\n\")\n",
    "            f.write(f\"  Blame (-1/0/1): _____\\n\")\n",
    "            f.write(f\"  CTA (0/1): _____\\n\")\n",
    "            f.write(f\"  Notes: _______________________________________________\\n\")\n",
    "            f.write(\"\\n\" + \"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nâœ… Extracted {len(validation_df)} posts for focused validation\")\n",
    "print(\"\\nğŸ“Š Sample distribution:\")\n",
    "for category, sample_df in validation_samples.items():\n",
    "    print(f\"   {category}: {len(sample_df)} posts\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Files saved to: {OUTPUT_DIR}\")\n",
    "print(\"   - focused_validation_sample.csv (full data)\")\n",
    "print(f\"   - validation_batch_1.txt through validation_batch_{(len(output_df)-1)//chunk_size + 1}.txt (readable format)\")\n",
    "\n",
    "# Show examples of key disagreement patterns\n",
    "print(\"\\nğŸ” Example disagreement patterns:\")\n",
    "\n",
    "print(\"\\n1. Haiku sees escalation, others don't:\")\n",
    "example1 = cat1.iloc[0] if len(cat1) > 0 else None\n",
    "if example1 is not None:\n",
    "    print(f\"   Text: {example1['text'][:150]}...\")\n",
    "    print(f\"   Scores - Haiku: {example1['escalation_score_haiku']}, Sonnet: {example1['escalation_score_sonnet']}, Opus: {example1['escalation_score_opus']}\")\n",
    "\n",
    "print(\"\\n2. Political content scoring:\")\n",
    "example2 = cat4.iloc[0] if len(cat4) > 0 else None\n",
    "if example2 is not None:\n",
    "    print(f\"   Text: {example2['text'][:150]}...\")\n",
    "    print(f\"   Scores - Haiku: {example2['escalation_score_haiku']}, Sonnet: {example2['escalation_score_sonnet']}, Opus: {example2['escalation_score_opus']}\")\n",
    "\n",
    "# Analysis of score patterns\n",
    "print(\"\\nğŸ“ˆ Score Pattern Analysis:\")\n",
    "print(f\"   Posts where Haiku > both others by 3+: {sum(df['haiku_outlier'])}\")\n",
    "print(f\"   Posts where Sonnet & Opus agree (Â±1): {sum(df['sonnet_opus_agree'])}\")\n",
    "print(f\"   Average Haiku score in high-variance set: {df['escalation_score_haiku'].mean():.2f}\")\n",
    "print(f\"   Average Sonnet score in high-variance set: {df['escalation_score_sonnet'].mean():.2f}\")\n",
    "print(f\"   Average Opus score in high-variance set: {df['escalation_score_opus'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PLOT Escalation Scores with Major Events Timeline Overlay            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HEADLINE_DAILY_CSV = ROOT / \"outputs\" / \"daily_escalation_scores.csv\"\n",
    "TRUTH_DAILY_CSV = ROOT / \"outputs\" / \"truth_daily_escalation_scores.csv\"\n",
    "TIMELINE_JSON = ROOT / \"src\" / \"ukraine-war-timeline.json\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"timeline_analysis\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load headline data\n",
    "print(\"ğŸ“Š Loading escalation scores...\")\n",
    "headline_daily = pd.read_csv(HEADLINE_DAILY_CSV, parse_dates=['date'])\n",
    "headline_daily = headline_daily.set_index('date').sort_index()\n",
    "headline_roll7 = headline_daily['mean_score'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Load Truth Social data (best model - likely Sonnet or Opus based on analysis)\n",
    "truth_daily = pd.read_csv(TRUTH_DAILY_CSV, parse_dates=['created_at'])\n",
    "truth_daily.rename(columns={'created_at': 'date'}, inplace=True)\n",
    "truth_daily = truth_daily.set_index('date').sort_index()\n",
    "truth_roll7 = truth_daily['mean_score'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Load timeline events\n",
    "print(\"ğŸ“… Loading timeline events...\")\n",
    "events = []\n",
    "with open(TIMELINE_JSON, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            events.append(json.loads(line))\n",
    "\n",
    "# Convert events to DataFrame\n",
    "events_df = pd.DataFrame(events)\n",
    "events_df['date'] = pd.to_datetime(events_df['date'])\n",
    "\n",
    "# Filter only major events\n",
    "major_events = events_df[events_df['major'] == True].copy()\n",
    "\n",
    "# Create the main plot\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Plot rolling averages\n",
    "ax.plot(headline_roll7.index, headline_roll7.values, \n",
    "        'crimson', linewidth=2.5, label='News Headlines', alpha=0.9)\n",
    "ax.plot(truth_roll7.index, truth_roll7.values, \n",
    "        'navy', linewidth=2.5, label='Truth Social', alpha=0.9)\n",
    "\n",
    "# Add daily scatter points with lower opacity\n",
    "ax.scatter(headline_daily.index, headline_daily['mean_score'], \n",
    "           alpha=0.15, s=15, color='crimson')\n",
    "ax.scatter(truth_daily.index, truth_daily['mean_score'], \n",
    "           alpha=0.15, s=15, color='navy')\n",
    "\n",
    "# Add major event vertical lines and labels\n",
    "for _, event in major_events.iterrows():\n",
    "    event_date = event['date']\n",
    "    \n",
    "    # Only plot if within data range\n",
    "    if (event_date >= min(headline_roll7.index.min(), truth_roll7.index.min()) and \n",
    "        event_date <= max(headline_roll7.index.max(), truth_roll7.index.max())):\n",
    "        \n",
    "        # Add vertical line\n",
    "        ax.axvline(x=event_date, color='red', alpha=0.3, linestyle='--', linewidth=1)\n",
    "        \n",
    "        # Add event label\n",
    "        # Alternate label positions to avoid overlap\n",
    "        y_position = ax.get_ylim()[1] * 0.95 if major_events.index.get_loc(event.name) % 2 == 0 else ax.get_ylim()[1] * 0.85\n",
    "        \n",
    "        ax.text(event_date, y_position, event['label'], \n",
    "                rotation=45, fontsize=8, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Escalation Score (0-10)', fontsize=12)\n",
    "ax.set_title('Ukraine War Escalation: News Headlines vs Truth Social with Major Events\\n(7-day rolling mean)', \n",
    "             fontsize=14, pad=20)\n",
    "ax.legend(fontsize=12, loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Format x-axis\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set y-axis limits\n",
    "ax.set_ylim(0, max(headline_roll7.max(), truth_roll7.max()) * 1.1)\n",
    "\n",
    "# Add annotations for key patterns\n",
    "ax.annotate('Truth Social\\nconsistently lower', \n",
    "            xy=(pd.Timestamp('2023-06-01'), 2.2), \n",
    "            xytext=(pd.Timestamp('2023-08-01'), 1.0),\n",
    "            arrowprops=dict(arrowstyle='->', color='navy', alpha=0.5),\n",
    "            fontsize=10, ha='center', color='navy')\n",
    "\n",
    "ax.annotate('Headlines spike\\nwith major events', \n",
    "            xy=(pd.Timestamp('2023-06-04'), headline_roll7.loc['2023-06-04']), \n",
    "            xytext=(pd.Timestamp('2023-04-01'), 5.5),\n",
    "            arrowprops=dict(arrowstyle='->', color='crimson', alpha=0.5),\n",
    "            fontsize=10, ha='center', color='crimson')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'escalation_timeline_overlay.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Create a focused plot for specific periods of interest\n",
    "print(\"\\nğŸ“Š Creating focused analysis plots...\")\n",
    "\n",
    "# Function to create period-specific plots\n",
    "def plot_period(start_date, end_date, title_suffix):\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Filter data for period\n",
    "    headline_period = headline_roll7[start_date:end_date]\n",
    "    truth_period = truth_roll7[start_date:end_date]\n",
    "    events_period = major_events[(major_events['date'] >= start_date) & \n",
    "                                 (major_events['date'] <= end_date)]\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(headline_period.index, headline_period.values, \n",
    "            'crimson', linewidth=2.5, label='News Headlines')\n",
    "    ax.plot(truth_period.index, truth_period.values, \n",
    "            'navy', linewidth=2.5, label='Truth Social')\n",
    "    \n",
    "    # Add events\n",
    "    for _, event in events_period.iterrows():\n",
    "        ax.axvline(x=event['date'], color='red', alpha=0.4, linestyle='--')\n",
    "        ax.text(event['date'], ax.get_ylim()[1] * 0.9, event['label'],\n",
    "                rotation=45, fontsize=9, ha='right', va='top',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.6))\n",
    "    \n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Escalation Score (0-10)')\n",
    "    ax.set_title(f'Escalation Patterns: {title_suffix}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f\"escalation_period_{start_date.strftime('%Y%m')}_{end_date.strftime('%Y%m')}.png\"\n",
    "    plt.savefig(OUTPUT_DIR / filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Create period-specific plots\n",
    "plot_period(pd.Timestamp('2023-05-01'), pd.Timestamp('2023-07-31'), \n",
    "            'Ukrainian Counteroffensive Period')\n",
    "plot_period(pd.Timestamp('2024-01-01'), pd.Timestamp('2024-03-31'), \n",
    "            'Russian Winter Offensive 2024')\n",
    "plot_period(pd.Timestamp('2024-11-01'), pd.Timestamp('2025-02-28'), \n",
    "            'Trump Election and Policy Shift')\n",
    "\n",
    "# Generate event impact analysis\n",
    "print(\"\\nğŸ“ˆ Analyzing event impacts...\")\n",
    "\n",
    "# Calculate average scores before/after major events\n",
    "event_impacts = []\n",
    "for _, event in major_events.iterrows():\n",
    "    event_date = event['date']\n",
    "    \n",
    "    # 7 days before and after\n",
    "    before_start = event_date - pd.Timedelta(days=14)\n",
    "    before_end = event_date - pd.Timedelta(days=1)\n",
    "    after_start = event_date + pd.Timedelta(days=1)\n",
    "    after_end = event_date + pd.Timedelta(days=14)\n",
    "    \n",
    "    # Calculate means if data exists\n",
    "    try:\n",
    "        headline_before = headline_daily.loc[before_start:before_end]['mean_score'].mean()\n",
    "        headline_after = headline_daily.loc[after_start:after_end]['mean_score'].mean()\n",
    "        truth_before = truth_daily.loc[before_start:before_end]['mean_score'].mean()\n",
    "        truth_after = truth_daily.loc[after_start:after_end]['mean_score'].mean()\n",
    "        \n",
    "        event_impacts.append({\n",
    "            'event': event['label'],\n",
    "            'date': event_date,\n",
    "            'headline_change': headline_after - headline_before,\n",
    "            'truth_change': truth_after - truth_before,\n",
    "            'headline_before': headline_before,\n",
    "            'headline_after': headline_after,\n",
    "            'truth_before': truth_before,\n",
    "            'truth_after': truth_after\n",
    "        })\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Save event impact analysis\n",
    "impact_df = pd.DataFrame(event_impacts)\n",
    "impact_df.to_csv(OUTPUT_DIR / 'event_impact_analysis.csv', index=False)\n",
    "\n",
    "print(\"\\nğŸ“Š Event Impact Summary:\")\n",
    "print(impact_df[['event', 'headline_change', 'truth_change']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ… All visualizations saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nğŸ“ Generated files:\")\n",
    "print(\"   - escalation_timeline_overlay.png (main comparison with all events)\")\n",
    "print(\"   - Period-specific analysis plots\")\n",
    "print(\"   - event_impact_analysis.csv (quantitative impact measures)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Comparison of Headline Scoring: Claude Haiku 3.5 vs Sonnet 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  HEADLINE MODEL COMPARISON: Haiku 3.5 vs Sonnet 4                     â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HAIKU_CSV = ROOT / \"outputs\" / \"headline_scores_partial.csv\"\n",
    "SONNET_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"headline_model_comparison\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“Š Loading headline model outputs...\")\n",
    "# Load both datasets\n",
    "haiku_df = pd.read_csv(HAIKU_CSV, parse_dates=['date'])\n",
    "sonnet_df = pd.read_csv(SONNET_CSV, parse_dates=['date'])\n",
    "\n",
    "print(f\"   Haiku headlines: {len(haiku_df)}\")\n",
    "print(f\"   Sonnet headlines: {len(sonnet_df)}\")\n",
    "\n",
    "# Merge on common identifier columns (assuming same headlines in same order)\n",
    "# If headlines have unique IDs, use those instead\n",
    "merge_cols = ['date', 'source', 'title']\n",
    "\n",
    "# Merge the datasets\n",
    "comparison_df = pd.merge(\n",
    "    haiku_df[merge_cols + ['score']],\n",
    "    sonnet_df[merge_cols + ['score']],\n",
    "    on=merge_cols,\n",
    "    suffixes=('_haiku', '_sonnet'),\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"âœ… Matched {len(comparison_df)} headlines scored by both models\")\n",
    "\n",
    "# Calculate differences\n",
    "comparison_df['score_diff'] = comparison_df['score_haiku'] - comparison_df['score_sonnet']\n",
    "\n",
    "# Agreement metrics\n",
    "print(\"\\nğŸ“ˆ Agreement Statistics:\")\n",
    "correlation = comparison_df['score_haiku'].corr(comparison_df['score_sonnet'])\n",
    "exact_match = (comparison_df['score_diff'] == 0).mean() * 100\n",
    "within_one = (abs(comparison_df['score_diff']) <= 1).mean() * 100\n",
    "within_two = (abs(comparison_df['score_diff']) <= 2).mean() * 100\n",
    "\n",
    "print(f\"   Correlation coefficient: {correlation:.3f}\")\n",
    "print(f\"   Exact score match: {exact_match:.1f}%\")\n",
    "print(f\"   Within Â±1 point: {within_one:.1f}%\")\n",
    "print(f\"   Within Â±2 points: {within_two:.1f}%\")\n",
    "\n",
    "# Model statistics\n",
    "print(\"\\nğŸ“Š Model Statistics:\")\n",
    "print(\"\\nHaiku 3.5:\")\n",
    "print(f\"   Mean score: {comparison_df['score_haiku'].mean():.2f}\")\n",
    "print(f\"   Std deviation: {comparison_df['score_haiku'].std():.2f}\")\n",
    "print(f\"   Median: {comparison_df['score_haiku'].median():.0f}\")\n",
    "\n",
    "print(\"\\nSonnet 4:\")\n",
    "print(f\"   Mean score: {comparison_df['score_sonnet'].mean():.2f}\")\n",
    "print(f\"   Std deviation: {comparison_df['score_sonnet'].std():.2f}\")\n",
    "print(f\"   Median: {comparison_df['score_sonnet'].median():.0f}\")\n",
    "\n",
    "# Create visualizations\n",
    "fig = plt.figure(figsize=(18, 14))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "fig.suptitle('Headline Model Comparison: Haiku 3.5 vs Sonnet 4', fontsize=16)\n",
    "\n",
    "# 1. Scatter plot of scores\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "scatter = ax1.scatter(comparison_df['score_haiku'], comparison_df['score_sonnet'], \n",
    "                      alpha=0.3, s=10, c=comparison_df['score_diff'], cmap='RdBu_r')\n",
    "ax1.plot([0, 10], [0, 10], 'r--', alpha=0.5, label='Perfect agreement')\n",
    "ax1.set_xlabel('Haiku 3.5 Score')\n",
    "ax1.set_ylabel('Sonnet 4 Score')\n",
    "ax1.set_title('Score Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.text(0.05, 0.95, f'r = {correlation:.3f}', transform=ax1.transAxes, \n",
    "         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Difference histogram\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.hist(comparison_df['score_diff'], bins=21, range=(-10.5, 10.5), \n",
    "         edgecolor='black', alpha=0.7, color='skyblue')\n",
    "ax2.set_xlabel('Score Difference (Haiku - Sonnet)')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Distribution of Score Differences')\n",
    "ax2.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "mean_diff = comparison_df['score_diff'].mean()\n",
    "ax2.axvline(mean_diff, color='green', linestyle='--', \n",
    "            alpha=0.5, label=f'Mean: {mean_diff:.2f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Score distributions comparison\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "bins = np.arange(-0.5, 11.5, 1)\n",
    "ax3.hist(comparison_df['score_haiku'], bins=bins, alpha=0.5, label='Haiku 3.5', \n",
    "         density=True, color='blue')\n",
    "ax3.hist(comparison_df['score_sonnet'], bins=bins, alpha=0.5, label='Sonnet 4', \n",
    "         density=True, color='green')\n",
    "ax3.set_xlabel('Escalation Score')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.set_title('Score Distribution Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Agreement by score level\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "score_ranges = [(0, 2), (3, 4), (5, 6), (7, 10)]\n",
    "agreement_by_range = []\n",
    "for low, high in score_ranges:\n",
    "    mask = (comparison_df['score_haiku'] >= low) & (comparison_df['score_haiku'] <= high)\n",
    "    agreement_rate = (abs(comparison_df[mask]['score_diff']) <= 1).mean() * 100\n",
    "    agreement_by_range.append(agreement_rate)\n",
    "\n",
    "ax4.bar(range(len(score_ranges)), agreement_by_range, \n",
    "        tick_label=[f'{low}-{high}' for low, high in score_ranges],\n",
    "        color='lightcoral', alpha=0.7)\n",
    "ax4.set_xlabel('Haiku Score Range')\n",
    "ax4.set_ylabel('Agreement Rate (%) within Â±1')\n",
    "ax4.set_title('Agreement Rate by Score Level')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 5. Time series of mean scores\n",
    "ax5 = fig.add_subplot(gs[1, 1:])\n",
    "daily_scores = comparison_df.groupby('date').agg({\n",
    "    'score_haiku': 'mean',\n",
    "    'score_sonnet': 'mean'\n",
    "}).rolling(window=7, center=True).mean()\n",
    "\n",
    "ax5.plot(daily_scores.index, daily_scores['score_haiku'], \n",
    "         label='Haiku 3.5', color='blue', alpha=0.8)\n",
    "ax5.plot(daily_scores.index, daily_scores['score_sonnet'], \n",
    "         label='Sonnet 4', color='green', alpha=0.8)\n",
    "ax5.set_xlabel('Date')\n",
    "ax5.set_ylabel('7-day Rolling Mean Score')\n",
    "ax5.set_title('Temporal Comparison of Scores')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Systematic bias analysis\n",
    "ax6 = fig.add_subplot(gs[2, 0])\n",
    "sonnet_bins = range(11)\n",
    "haiku_means_by_sonnet = []\n",
    "for score in sonnet_bins:\n",
    "    mask = comparison_df['score_sonnet'] == score\n",
    "    if mask.sum() > 0:\n",
    "        haiku_means_by_sonnet.append(comparison_df[mask]['score_haiku'].mean())\n",
    "    else:\n",
    "        haiku_means_by_sonnet.append(np.nan)\n",
    "\n",
    "ax6.plot(sonnet_bins, haiku_means_by_sonnet, 'o-', label='Actual', markersize=8)\n",
    "ax6.plot([0, 10], [0, 10], 'r--', alpha=0.5, label='No bias')\n",
    "ax6.set_xlabel('Sonnet 4 Score')\n",
    "ax6.set_ylabel('Average Haiku 3.5 Score')\n",
    "ax6.set_title('Systematic Bias Analysis')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Top disagreement cases\n",
    "ax7 = fig.add_subplot(gs[2, 1:])\n",
    "ax7.axis('off')\n",
    "major_disagreements = comparison_df[abs(comparison_df['score_diff']) >= 3].copy()\n",
    "major_disagreements = major_disagreements.nlargest(10, 'score_diff', keep='all')\n",
    "\n",
    "text = \"Top 10 Headlines with Major Score Differences:\\n\\n\"\n",
    "for idx, row in major_disagreements.head(10).iterrows():\n",
    "    text += f\"Haiku: {row['score_haiku']}, Sonnet: {row['score_sonnet']} (diff: {row['score_diff']})\\n\"\n",
    "    text += f\"{row['title'][:80]}...\\n\\n\"\n",
    "\n",
    "ax7.text(0.05, 0.95, text, transform=ax7.transAxes, fontsize=9, \n",
    "         verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'headline_model_comparison_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Statistical tests\n",
    "print(\"\\nğŸ“Š Statistical Tests:\")\n",
    "# Paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(comparison_df['score_haiku'], comparison_df['score_sonnet'])\n",
    "print(f\"   Paired t-test: t={t_stat:.3f}, p={p_value:.3e}\")\n",
    "\n",
    "# Effect size (Cohen's d)\n",
    "diff_mean = comparison_df['score_diff'].mean()\n",
    "diff_std = comparison_df['score_diff'].std()\n",
    "cohens_d = diff_mean / diff_std\n",
    "print(f\"   Cohen's d: {cohens_d:.3f}\")\n",
    "\n",
    "# Save comparison data\n",
    "comparison_df.to_csv(OUTPUT_DIR / 'headline_model_comparison_full.csv', index=False)\n",
    "\n",
    "# Extract high disagreement headlines for validation\n",
    "high_disagreement = comparison_df[abs(comparison_df['score_diff']) >= 3].copy()\n",
    "high_disagreement = high_disagreement.sort_values('score_diff', ascending=False)\n",
    "high_disagreement[['date', 'title', 'score_haiku', 'score_sonnet', 'score_diff']].to_csv(\n",
    "    OUTPUT_DIR / 'headline_high_disagreement.csv', index=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâš ï¸  Found {len(high_disagreement)} headlines with major disagreements (â‰¥3 points)\")\n",
    "print(f\"\\nğŸ’¾ Results saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Summary of findings\n",
    "print(\"\\nğŸ¯ Key Findings:\")\n",
    "print(f\"   - Haiku 3.5 scores {'higher' if diff_mean > 0 else 'lower'} on average by {abs(diff_mean):.2f} points\")\n",
    "print(f\"   - Models agree within Â±1 point on {within_one:.1f}% of headlines\")\n",
    "print(f\"   - Strongest disagreements occur on {'high' if high_disagreement['score_haiku'].mean() > 5 else 'low'} escalation events\")\n",
    "\n",
    "# Score distribution summary\n",
    "print(\"\\nğŸ“Š Score Distribution Comparison:\")\n",
    "for score in range(11):\n",
    "    haiku_pct = (comparison_df['score_haiku'] == score).mean() * 100\n",
    "    sonnet_pct = (comparison_df['score_sonnet'] == score).mean() * 100\n",
    "    print(f\"   Score {score:2d}: Haiku {haiku_pct:5.1f}% | Sonnet {sonnet_pct:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  QUICK ANALYSIS: Telegram Batches 1&2 vs Headlines vs Truth Social    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "\n",
    "# Load Telegram batches\n",
    "print(\"ğŸ“Š Loading Telegram batch data...\")\n",
    "batch1 = pd.read_csv(ROOT / \"outputs\" / \"telegram_scoring\" / \"batch_1_scored.csv\", parse_dates=['date'])\n",
    "batch2 = pd.read_csv(ROOT / \"outputs\" / \"telegram_scoring\" / \"batch_2_scored.csv\", parse_dates=['date'])\n",
    "\n",
    "# Combine batches\n",
    "telegram_df = pd.concat([batch1, batch2], ignore_index=True)\n",
    "print(f\"âœ… Loaded {len(telegram_df):,} Telegram messages from batches 1&2\")\n",
    "\n",
    "# Check date range\n",
    "print(f\"\\nğŸ“… Date range: {telegram_df['date'].min()} to {telegram_df['date'].max()}\")\n",
    "\n",
    "# Load comparison data (if available)\n",
    "try:\n",
    "    headline_daily = pd.read_csv(ROOT / \"outputs\" / \"daily_escalation_scores.csv\", parse_dates=['date'])\n",
    "    truth_daily = pd.read_csv(ROOT / \"outputs\" / \"truth_daily_escalation_scores.csv\", parse_dates=['created_at'])\n",
    "    truth_daily.rename(columns={'created_at': 'date'}, inplace=True)\n",
    "    print(\"âœ… Loaded headline and Truth Social data for comparison\")\n",
    "except:\n",
    "    print(\"âš ï¸  Could not load comparison data\")\n",
    "    headline_daily = None\n",
    "    truth_daily = None\n",
    "\n",
    "# â”€â”€ Quick Statistics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“ˆ TELEGRAM BATCH STATISTICS:\")\n",
    "print(f\"   Mean escalation: {telegram_df['escalation_score'].mean():.2f}\")\n",
    "print(f\"   Propaganda level: {telegram_df['propaganda_level'].mean():.2f}/3.0\")\n",
    "print(f\"   Has CTA: {telegram_df['has_cta'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ¯ By Category:\")\n",
    "for cat in telegram_df['channel_category'].unique():\n",
    "    cat_df = telegram_df[telegram_df['channel_category'] == cat]\n",
    "    print(f\"\\n{cat}:\")\n",
    "    print(f\"   Messages: {len(cat_df):,}\")\n",
    "    print(f\"   Escalation: {cat_df['escalation_score'].mean():.2f}\")\n",
    "    print(f\"   Propaganda: {cat_df['propaganda_level'].mean():.2f}\")\n",
    "    \n",
    "# â”€â”€ Visualizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Score distributions comparison\n",
    "ax = axes[0, 0]\n",
    "ax.hist(telegram_df['escalation_score'], bins=11, range=(-0.5, 10.5), \n",
    "        alpha=0.5, label='Telegram', edgecolor='black')\n",
    "ax.set_xlabel('Escalation Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Telegram Escalation Score Distribution (Batches 1&2)')\n",
    "ax.legend()\n",
    "\n",
    "# 2. Category comparison\n",
    "ax = axes[0, 1]\n",
    "category_means = telegram_df.groupby('channel_category')['escalation_score'].mean().sort_values()\n",
    "category_means.plot(kind='barh', ax=ax, color=['red', 'gray', 'gold', 'blue'])\n",
    "ax.set_xlabel('Mean Escalation Score')\n",
    "ax.set_title('Escalation by Channel Category')\n",
    "\n",
    "# 3. Daily trend comparison\n",
    "ax = axes[1, 0]\n",
    "telegram_daily = telegram_df.groupby(telegram_df['date'].dt.date).agg({\n",
    "    'escalation_score': 'mean',\n",
    "    'propaganda_level': 'mean'\n",
    "}).reset_index()\n",
    "telegram_daily['date'] = pd.to_datetime(telegram_daily['date'])\n",
    "\n",
    "# Plot Telegram\n",
    "ax.plot(telegram_daily['date'], telegram_daily['escalation_score'], \n",
    "        'green', linewidth=2, label=f'Telegram (n={len(telegram_df):,})', marker='o')\n",
    "\n",
    "# Add Headlines if available\n",
    "if headline_daily is not None:\n",
    "    overlap_dates = telegram_daily['date'].dt.date.isin(headline_daily['date'].dt.date)\n",
    "    headline_subset = headline_daily[headline_daily['date'].dt.date.isin(telegram_daily['date'].dt.date)]\n",
    "    ax.plot(headline_subset['date'], headline_subset['mean_score'], \n",
    "            'crimson', linewidth=2, label='Headlines', marker='s', alpha=0.7)\n",
    "\n",
    "# Add Truth Social if available  \n",
    "if truth_daily is not None:\n",
    "    truth_subset = truth_daily[truth_daily['date'].dt.date.isin(telegram_daily['date'].dt.date)]\n",
    "    if len(truth_subset) > 0:\n",
    "        ax.plot(truth_subset['date'], truth_subset['mean_score'], \n",
    "                'navy', linewidth=2, label='Truth Social', marker='^', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Mean Escalation Score')\n",
    "ax.set_title('Daily Escalation Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Propaganda vs Escalation scatter\n",
    "ax = axes[1, 1]\n",
    "# Sample for visibility\n",
    "sample_size = min(1000, len(telegram_df))\n",
    "sample_df = telegram_df.sample(sample_size)\n",
    "colors = {'pro_russian_grassroots': 'red', 'pro_ukrainian_grassroots': 'blue', \n",
    "          'official_comparison': 'gold', 'neutral_independent': 'gray'}\n",
    "for cat, color in colors.items():\n",
    "    cat_data = sample_df[sample_df['channel_category'] == cat]\n",
    "    ax.scatter(cat_data['escalation_score'], cat_data['propaganda_level'], \n",
    "               alpha=0.5, label=cat.replace('_', ' ').title(), color=color, s=20)\n",
    "ax.set_xlabel('Escalation Score')\n",
    "ax.set_ylabel('Propaganda Level')\n",
    "ax.set_title('Escalation vs Propaganda (sample)')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(ROOT / \"outputs\" / \"telegram_batch12_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# â”€â”€ Blame Direction Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ¯ BLAME DIRECTION ANALYSIS:\")\n",
    "blame_map = {-1: \"Neutral\", 0: \"Blames West/Ukraine\", 1: \"Blames Russia\"}\n",
    "for cat in telegram_df['channel_category'].unique():\n",
    "    cat_df = telegram_df[telegram_df['channel_category'] == cat]\n",
    "    print(f\"\\n{cat}:\")\n",
    "    blame_counts = cat_df['blame_direction'].value_counts(normalize=True) * 100\n",
    "    for blame_val, pct in blame_counts.items():\n",
    "        print(f\"   {blame_map[blame_val]}: {pct:.1f}%\")\n",
    "\n",
    "# â”€â”€ Cross-Platform Comparison Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ“Š CROSS-PLATFORM COMPARISON:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Telegram (batches 1&2): {telegram_df['escalation_score'].mean():.2f}\")\n",
    "\n",
    "if headline_daily is not None:\n",
    "    # Calculate mean for overlapping dates only\n",
    "    overlap_dates = telegram_daily['date'].dt.date\n",
    "    headline_overlap = headline_daily[headline_daily['date'].dt.date.isin(overlap_dates)]\n",
    "    if len(headline_overlap) > 0:\n",
    "        print(f\"Headlines (same dates): {headline_overlap['mean_score'].mean():.2f}\")\n",
    "\n",
    "if truth_daily is not None:\n",
    "    truth_overlap = truth_daily[truth_daily['date'].dt.date.isin(overlap_dates)]\n",
    "    if len(truth_overlap) > 0:\n",
    "        print(f\"Truth Social (same dates): {truth_overlap['mean_score'].mean():.2f}\")\n",
    "\n",
    "# â”€â”€ Key Findings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\nğŸ” KEY FINDINGS FROM BATCHES 1&2:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Find highest escalation messages\n",
    "top_escalation = telegram_df.nlargest(5, 'escalation_score')[['channel_username', 'escalation_score', 'message_text']]\n",
    "print(\"\\nğŸ”¥ Highest escalation messages:\")\n",
    "for _, row in top_escalation.iterrows():\n",
    "    print(f\"\\n{row['channel_username']} (Score: {row['escalation_score']})\")\n",
    "    print(f\"   {row['message_text'][:150]}...\")\n",
    "\n",
    "# Propaganda analysis\n",
    "high_prop = telegram_df[telegram_df['propaganda_level'] >= 2]\n",
    "print(f\"\\nğŸ“¢ High propaganda messages: {len(high_prop):,} ({len(high_prop)/len(telegram_df)*100:.1f}%)\")\n",
    "print(f\"   Pro-Russian: {len(high_prop[high_prop['channel_category']=='pro_russian_grassroots']):,}\")\n",
    "print(f\"   Pro-Ukrainian: {len(high_prop[high_prop['channel_category']=='pro_ukrainian_grassroots']):,}\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete! Full dataset will provide more comprehensive insights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Comparison Telegram with Headlines/Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  COMPREHENSIVE TELEGRAM ANALYSIS & COMPARISON WITH HEADLINES/TRUTH    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_scoring\" / \"telegram_FINAL_COMPLETE_20250606_180932.csv\"\n",
    "HEADLINES_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "TRUTH_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-opus-4-20250514.csv\"  # Best model for Truth\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"telegram_comprehensive_analysis\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE TELEGRAM ANALYSIS WITH HEADLINES & TRUTH SOCIAL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  LOAD ALL DATASETS                                                     â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Loading datasets...\")\n",
    "\n",
    "# Load Telegram data\n",
    "telegram_df = pd.read_csv(TELEGRAM_CSV)\n",
    "telegram_df['date'] = pd.to_datetime(telegram_df['date'])\n",
    "telegram_df = telegram_df[telegram_df['escalation_score'].notna()].copy()\n",
    "\n",
    "# Identify official channels\n",
    "OFFICIAL_RU_CHANNELS = ['kremlinrussia', 'mod_russia', 'mid_russia', 'tass_agency', 'rian_ru']\n",
    "OFFICIAL_UA_CHANNELS = ['V_Zelenskiy_official', 'DefenceU', 'MFA_Ukraine', 'ukrpravda_news']\n",
    "\n",
    "telegram_df['is_official'] = telegram_df['channel_username'].isin(OFFICIAL_RU_CHANNELS + OFFICIAL_UA_CHANNELS)\n",
    "telegram_df['official_side'] = 'none'\n",
    "telegram_df.loc[telegram_df['channel_username'].isin(OFFICIAL_RU_CHANNELS), 'official_side'] = 'russia'\n",
    "telegram_df.loc[telegram_df['channel_username'].isin(OFFICIAL_UA_CHANNELS), 'official_side'] = 'ukraine'\n",
    "\n",
    "# Load Headlines\n",
    "headlines_df = pd.read_csv(HEADLINES_CSV, parse_dates=['date'])\n",
    "headlines_df = headlines_df[headlines_df['score'].notna()].rename(columns={'score': 'escalation_score'})\n",
    "\n",
    "# Load Truth Social - FIX: Use mixed format for inconsistent date formats\n",
    "truth_df = pd.read_csv(TRUTH_CSV)\n",
    "truth_df['created_at'] = pd.to_datetime(truth_df['created_at'], format='mixed')  # Fixed!\n",
    "truth_df = truth_df[truth_df['escalation_score'].notna()].copy()\n",
    "truth_df['date'] = truth_df['created_at']  # Standardize date column\n",
    "\n",
    "print(f\"âœ… Loaded {len(telegram_df):,} Telegram messages\")\n",
    "print(f\"   - Official channels: {telegram_df['is_official'].sum():,} messages\")\n",
    "print(f\"   - Russian official: {(telegram_df['official_side'] == 'russia').sum():,}\")\n",
    "print(f\"   - Ukrainian official: {(telegram_df['official_side'] == 'ukraine').sum():,}\")\n",
    "print(f\"âœ… Loaded {len(headlines_df):,} news headlines\")\n",
    "print(f\"âœ… Loaded {len(truth_df):,} Truth Social posts\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  TELEGRAM INTERNAL ANALYSIS                                            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TELEGRAM INTERNAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nğŸ“ˆ Overall Statistics:\")\n",
    "print(f\"   Mean escalation score: {telegram_df['escalation_score'].mean():.2f}\")\n",
    "print(f\"   Median score: {telegram_df['escalation_score'].median():.0f}\")\n",
    "print(f\"   Std deviation: {telegram_df['escalation_score'].std():.2f}\")\n",
    "\n",
    "# Blame direction analysis\n",
    "blame_counts = telegram_df['blame_direction'].value_counts()\n",
    "print(\"\\nğŸ¯ Blame Direction Distribution:\")\n",
    "print(f\"   No clear blame (-1): {blame_counts.get(-1, 0):,} ({blame_counts.get(-1, 0)/len(telegram_df)*100:.1f}%)\")\n",
    "print(f\"   Blames West/NATO (0): {blame_counts.get(0, 0):,} ({blame_counts.get(0, 0)/len(telegram_df)*100:.1f}%)\")\n",
    "print(f\"   Blames Russia (1): {blame_counts.get(1, 0):,} ({blame_counts.get(1, 0)/len(telegram_df)*100:.1f}%)\")\n",
    "\n",
    "# Propaganda levels\n",
    "print(\"\\nğŸ“¢ Propaganda Level Distribution:\")\n",
    "for level in range(4):\n",
    "    count = (telegram_df['propaganda_level'] == level).sum()\n",
    "    print(f\"   Level {level}: {count:,} ({count/len(telegram_df)*100:.1f}%)\")\n",
    "\n",
    "# Call to action\n",
    "cta_count = telegram_df['has_cta'].sum()\n",
    "print(f\"\\nğŸ“£ Call-to-Action: {cta_count:,} messages ({cta_count/len(telegram_df)*100:.1f}%) have CTAs\")\n",
    "\n",
    "# Pro-UA vs Pro-RU analysis\n",
    "pro_ua_mask = telegram_df['blame_direction'] == 1  # Blames Russia\n",
    "pro_ru_mask = telegram_df['blame_direction'] == 0  # Blames West/NATO\n",
    "\n",
    "print(\"\\nğŸ‡ºğŸ‡¦ Pro-Ukraine Content (Blames Russia):\")\n",
    "print(f\"   Count: {pro_ua_mask.sum():,} messages\")\n",
    "print(f\"   Mean escalation: {telegram_df[pro_ua_mask]['escalation_score'].mean():.2f}\")\n",
    "print(f\"   Has CTA: {telegram_df[pro_ua_mask]['has_cta'].mean()*100:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ‡·ğŸ‡º Pro-Russia Content (Blames West/NATO):\")\n",
    "print(f\"   Count: {pro_ru_mask.sum():,} messages\")\n",
    "print(f\"   Mean escalation: {telegram_df[pro_ru_mask]['escalation_score'].mean():.2f}\")\n",
    "print(f\"   Has CTA: {telegram_df[pro_ru_mask]['has_cta'].mean()*100:.1f}%\")\n",
    "\n",
    "# Official channels analysis\n",
    "if telegram_df['is_official'].any():\n",
    "    print(\"\\nğŸ›ï¸ Official Channels Analysis:\")\n",
    "    for side in ['russia', 'ukraine']:\n",
    "        mask = telegram_df['official_side'] == side\n",
    "        if mask.any():\n",
    "            print(f\"\\n   {side.capitalize()} Official Channels:\")\n",
    "            print(f\"     Messages: {mask.sum():,}\")\n",
    "            print(f\"     Mean escalation: {telegram_df[mask]['escalation_score'].mean():.2f}\")\n",
    "            print(f\"     Propaganda level: {telegram_df[mask]['propaganda_level'].mean():.2f}\")\n",
    "            print(f\"     Has CTA: {telegram_df[mask]['has_cta'].mean()*100:.1f}%\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  CALCULATE DAILY AVERAGES & ROLLING MEANS                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Calculating daily averages and rolling means...\")\n",
    "\n",
    "# Function to calculate daily stats\n",
    "def calculate_daily_stats(df, date_col='date', prefix=''):\n",
    "    daily = df.groupby(df[date_col].dt.date).agg({\n",
    "        'escalation_score': ['mean', 'median', 'std', 'count']\n",
    "    }).round(2)\n",
    "    daily.columns = [f'{prefix}mean_score', f'{prefix}median_score', f'{prefix}std_dev', f'{prefix}count']\n",
    "    daily = daily.reset_index()\n",
    "    daily[date_col] = pd.to_datetime(daily[date_col])\n",
    "    \n",
    "    # Add rolling averages\n",
    "    daily[f'{prefix}7day_mean'] = daily[f'{prefix}mean_score'].rolling(window=7, center=True).mean()\n",
    "    daily[f'{prefix}14day_mean'] = daily[f'{prefix}mean_score'].rolling(window=14, center=True).mean()\n",
    "    \n",
    "    return daily\n",
    "\n",
    "# Calculate for all sources\n",
    "telegram_daily = calculate_daily_stats(telegram_df, prefix='telegram_')\n",
    "headlines_daily = calculate_daily_stats(headlines_df, prefix='headlines_')\n",
    "truth_daily = calculate_daily_stats(truth_df, prefix='truth_')\n",
    "\n",
    "# Separate pro-UA and pro-RU daily stats\n",
    "telegram_pro_ua_daily = calculate_daily_stats(telegram_df[pro_ua_mask], prefix='pro_ua_')\n",
    "telegram_pro_ru_daily = calculate_daily_stats(telegram_df[pro_ru_mask], prefix='pro_ru_')\n",
    "\n",
    "# Save daily stats\n",
    "telegram_daily.to_csv(OUTPUT_DIR / 'telegram_daily_stats.csv', index=False)\n",
    "print(f\"ğŸ’¾ Saved daily statistics to: telegram_daily_stats.csv\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  CREATE COMPREHENSIVE VISUALIZATIONS                                   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ¨ Creating visualizations...\")\n",
    "\n",
    "# 1. MAIN COMPARISON PLOT: All three sources with rolling averages\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12), sharex=True)\n",
    "\n",
    "# Top plot: 7-day rolling averages\n",
    "ax1.plot(telegram_daily['date'], telegram_daily['telegram_7day_mean'], \n",
    "         label='Telegram', color='purple', linewidth=2)\n",
    "ax1.plot(headlines_daily['date'], headlines_daily['headlines_7day_mean'], \n",
    "         label='Headlines', color='red', linewidth=2)\n",
    "ax1.plot(truth_daily['date'], truth_daily['truth_7day_mean'], \n",
    "         label='Truth Social', color='blue', linewidth=2)\n",
    "\n",
    "ax1.set_ylabel('Escalation Score (7-day avg)', fontsize=12)\n",
    "ax1.set_title('Ukraine War Escalation: 7-Day Rolling Averages', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 8)\n",
    "\n",
    "# Bottom plot: 14-day rolling averages\n",
    "ax2.plot(telegram_daily['date'], telegram_daily['telegram_14day_mean'], \n",
    "         label='Telegram', color='purple', linewidth=2.5, linestyle='-')\n",
    "ax2.plot(headlines_daily['date'], headlines_daily['headlines_14day_mean'], \n",
    "         label='Headlines', color='red', linewidth=2.5, linestyle='-')\n",
    "ax2.plot(truth_daily['date'], truth_daily['truth_14day_mean'], \n",
    "         label='Truth Social', color='blue', linewidth=2.5, linestyle='-')\n",
    "\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Escalation Score (14-day avg)', fontsize=12)\n",
    "ax2.set_title('Ukraine War Escalation: 14-Day Rolling Averages', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'main_comparison_rolling_averages.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 2. Pro-Ukraine vs Pro-Russia Telegram content\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Daily counts\n",
    "ax1.fill_between(telegram_pro_ua_daily['date'], 0, telegram_pro_ua_daily['pro_ua_count'], \n",
    "                 alpha=0.5, color='blue', label='Pro-Ukraine')\n",
    "ax1.fill_between(telegram_pro_ru_daily['date'], 0, -telegram_pro_ru_daily['pro_ru_count'], \n",
    "                 alpha=0.5, color='red', label='Pro-Russia')\n",
    "ax1.axhline(0, color='black', linewidth=0.5)\n",
    "ax1.set_ylabel('Daily Message Count', fontsize=12)\n",
    "ax1.set_title('Pro-Ukraine vs Pro-Russia Message Volume on Telegram', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Escalation scores\n",
    "ax2.plot(telegram_pro_ua_daily['date'], telegram_pro_ua_daily['pro_ua_7day_mean'], \n",
    "         color='blue', linewidth=2, label='Pro-Ukraine (7-day avg)')\n",
    "ax2.plot(telegram_pro_ru_daily['date'], telegram_pro_ru_daily['pro_ru_7day_mean'], \n",
    "         color='red', linewidth=2, label='Pro-Russia (7-day avg)')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax2.set_title('Escalation Scores: Pro-Ukraine vs Pro-Russia Content', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'telegram_pro_ua_vs_pro_ru.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 3. Propaganda and CTA analysis over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Calculate daily propaganda average\n",
    "telegram_propaganda_daily = telegram_df.groupby(telegram_df['date'].dt.date).agg({\n",
    "    'propaganda_level': 'mean',\n",
    "    'has_cta': 'mean'\n",
    "}).reset_index()\n",
    "telegram_propaganda_daily['date'] = pd.to_datetime(telegram_propaganda_daily['date'])\n",
    "telegram_propaganda_daily['propaganda_7day'] = telegram_propaganda_daily['propaganda_level'].rolling(7, center=True).mean()\n",
    "telegram_propaganda_daily['cta_7day'] = telegram_propaganda_daily['has_cta'].rolling(7, center=True).mean()\n",
    "\n",
    "ax1.plot(telegram_propaganda_daily['date'], telegram_propaganda_daily['propaganda_7day'], \n",
    "         color='orange', linewidth=2)\n",
    "ax1.set_ylabel('Average Propaganda Level (0-3)', fontsize=12)\n",
    "ax1.set_title('Telegram: Propaganda Level Over Time (7-day average)', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(telegram_propaganda_daily['date'], telegram_propaganda_daily['cta_7day'] * 100, \n",
    "         color='green', linewidth=2)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('% Messages with Call-to-Action', fontsize=12)\n",
    "ax2.set_title('Telegram: Call-to-Action Frequency Over Time (7-day average)', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'telegram_propaganda_cta_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 4. Channel category analysis\n",
    "channel_stats = telegram_df.groupby('channel_category').agg({\n",
    "    'escalation_score': ['mean', 'count'],\n",
    "    'propaganda_level': 'mean',\n",
    "    'has_cta': 'mean',\n",
    "    'blame_direction': lambda x: (x == 1).mean()  # Pro-Ukraine percentage\n",
    "}).round(3)\n",
    "channel_stats.columns = ['mean_escalation', 'count', 'avg_propaganda', 'cta_rate', 'pro_ukraine_rate']\n",
    "channel_stats = channel_stats.sort_values('mean_escalation', ascending=False)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Mean escalation by category\n",
    "top_categories = channel_stats.head(15)\n",
    "ax1.barh(range(len(top_categories)), top_categories['mean_escalation'])\n",
    "ax1.set_yticks(range(len(top_categories)))\n",
    "ax1.set_yticklabels(top_categories.index)\n",
    "ax1.set_xlabel('Mean Escalation Score')\n",
    "ax1.set_title('Top 15 Channel Categories by Escalation')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Message volume by category\n",
    "ax2.barh(range(len(top_categories)), top_categories['count'])\n",
    "ax2.set_yticks(range(len(top_categories)))\n",
    "ax2.set_yticklabels(top_categories.index)\n",
    "ax2.set_xlabel('Number of Messages')\n",
    "ax2.set_title('Message Volume by Category')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Propaganda level by category\n",
    "ax3.barh(range(len(top_categories)), top_categories['avg_propaganda'])\n",
    "ax3.set_yticks(range(len(top_categories)))\n",
    "ax3.set_yticklabels(top_categories.index)\n",
    "ax3.set_xlabel('Average Propaganda Level')\n",
    "ax3.set_title('Propaganda Level by Category')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Pro-Ukraine rate by category\n",
    "ax4.barh(range(len(top_categories)), top_categories['pro_ukraine_rate'] * 100)\n",
    "ax4.set_yticks(range(len(top_categories)))\n",
    "ax4.set_yticklabels(top_categories.index)\n",
    "ax4.set_xlabel('% Pro-Ukraine Content')\n",
    "ax4.set_title('Pro-Ukraine Content by Category')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'telegram_channel_category_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 5. Correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "corr_data = telegram_df[['escalation_score', 'blame_direction', 'propaganda_level', 'has_cta']].corr()\n",
    "sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, \n",
    "            xticklabels=['Escalation', 'Blame Dir', 'Propaganda', 'CTA'],\n",
    "            yticklabels=['Escalation', 'Blame Dir', 'Propaganda', 'CTA'])\n",
    "ax.set_title('Telegram: Correlation Matrix of Scoring Dimensions', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'telegram_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# 6. Summary dashboard\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Score distribution comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "bins = np.arange(-0.5, 11.5, 1)\n",
    "ax1.hist([telegram_df['escalation_score'], headlines_df['escalation_score'], truth_df['escalation_score']], \n",
    "         bins=bins, label=['Telegram', 'Headlines', 'Truth Social'], alpha=0.6)\n",
    "ax1.set_xlabel('Escalation Score')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Escalation Score Distribution Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Time series comparison\n",
    "ax2 = fig.add_subplot(gs[1, :])\n",
    "ax2.plot(telegram_daily['date'], telegram_daily['telegram_mean_score'], \n",
    "         'purple', alpha=0.3, linewidth=0.5)\n",
    "ax2.plot(telegram_daily['date'], telegram_daily['telegram_7day_mean'], \n",
    "         'purple', linewidth=2, label='Telegram')\n",
    "ax2.plot(headlines_daily['date'], headlines_daily['headlines_7day_mean'], \n",
    "         'red', linewidth=2, label='Headlines')\n",
    "ax2.plot(truth_daily['date'], truth_daily['truth_7day_mean'], \n",
    "         'blue', linewidth=2, label='Truth Social')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Escalation Score (7-day avg)')\n",
    "ax2.set_title('Comparative Time Series: All Sources')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Blame direction pie charts\n",
    "for i, (name, df) in enumerate([('Telegram', telegram_df), \n",
    "                                ('Headlines', headlines_df), \n",
    "                                ('Truth Social', truth_df)]):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    if 'blame_direction' in df.columns:\n",
    "        blame_counts = df['blame_direction'].value_counts()\n",
    "    elif 'blame' in df.columns:\n",
    "        blame_counts = df['blame'].value_counts()\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    labels = []\n",
    "    sizes = []\n",
    "    colors = []\n",
    "    \n",
    "    if -1 in blame_counts:\n",
    "        labels.append('No blame')\n",
    "        sizes.append(blame_counts[-1])\n",
    "        colors.append('gray')\n",
    "    if 0 in blame_counts:\n",
    "        labels.append('West/NATO')\n",
    "        sizes.append(blame_counts[0])\n",
    "        colors.append('orange')\n",
    "    if 1 in blame_counts:\n",
    "        labels.append('Russia')\n",
    "        sizes.append(blame_counts[1])\n",
    "        colors.append('red')\n",
    "    \n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "    ax.set_title(f'{name}: Blame Attribution')\n",
    "\n",
    "# Summary statistics table\n",
    "ax3 = fig.add_subplot(gs[3, :])\n",
    "ax3.axis('tight')\n",
    "ax3.axis('off')\n",
    "\n",
    "summary_data = []\n",
    "for name, df in [('Telegram', telegram_df), ('Headlines', headlines_df), ('Truth Social', truth_df)]:\n",
    "    summary_data.append([\n",
    "        name,\n",
    "        f\"{df['escalation_score'].mean():.2f}\",\n",
    "        f\"{df['escalation_score'].std():.2f}\",\n",
    "        f\"{df['escalation_score'].median():.0f}\",\n",
    "        f\"{len(df):,}\"\n",
    "    ])\n",
    "\n",
    "table = ax3.table(cellText=summary_data,\n",
    "                  colLabels=['Source', 'Mean Esc.', 'Std Dev', 'Median', 'Count'],\n",
    "                  cellLoc='center',\n",
    "                  loc='center',\n",
    "                  bbox=[0, 0, 1, 1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(12)\n",
    "ax3.set_title('Summary Statistics Comparison', fontsize=14, pad=20)\n",
    "\n",
    "plt.suptitle('Telegram Analysis Dashboard with Headlines & Truth Social Comparison', \n",
    "             fontsize=18, y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'comprehensive_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  TEMPORAL EVENT ANALYSIS                                               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ¯ Temporal Event Analysis...\")\n",
    "\n",
    "# Find high escalation periods (scores >= 5 for 3+ consecutive days)\n",
    "def find_escalation_periods(daily_df, score_col, threshold=5, min_days=3):\n",
    "    high_esc = daily_df[score_col] >= threshold\n",
    "    periods = []\n",
    "    start = None\n",
    "    \n",
    "    for i, (date, is_high) in enumerate(zip(daily_df['date'], high_esc)):\n",
    "        if is_high and start is None:\n",
    "            start = i\n",
    "        elif not is_high and start is not None:\n",
    "            if i - start >= min_days:\n",
    "                periods.append((daily_df.iloc[start]['date'], daily_df.iloc[i-1]['date']))\n",
    "            start = None\n",
    "    \n",
    "    return periods\n",
    "\n",
    "telegram_periods = find_escalation_periods(telegram_daily, 'telegram_7day_mean')\n",
    "headlines_periods = find_escalation_periods(headlines_daily, 'headlines_7day_mean')\n",
    "\n",
    "print(\"\\nğŸ“ˆ High Escalation Periods (7-day avg >= 5 for 3+ days):\")\n",
    "print(f\"\\nTelegram: {len(telegram_periods)} periods\")\n",
    "for start, end in telegram_periods[:5]:  # Show first 5\n",
    "    print(f\"   {start.date()} to {end.date()}\")\n",
    "\n",
    "print(f\"\\nHeadlines: {len(headlines_periods)} periods\")\n",
    "for start, end in headlines_periods[:5]:  # Show first 5\n",
    "    print(f\"   {start.date()} to {end.date()}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  EXTENSIVE METRICS OUTPUT                                              â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Overall comparison\n",
    "print(\"\\nğŸ“Š OVERALL COMPARISON:\")\n",
    "print(f\"\\nMean Escalation Scores:\")\n",
    "print(f\"   Telegram:     {telegram_df['escalation_score'].mean():.3f} (Ïƒ={telegram_df['escalation_score'].std():.3f})\")\n",
    "print(f\"   Headlines:    {headlines_df['escalation_score'].mean():.3f} (Ïƒ={headlines_df['escalation_score'].std():.3f})\")\n",
    "print(f\"   Truth Social: {truth_df['escalation_score'].mean():.3f} (Ïƒ={truth_df['escalation_score'].std():.3f})\")\n",
    "\n",
    "# Date ranges\n",
    "print(f\"\\nDate Coverage:\")\n",
    "print(f\"   Telegram:     {telegram_df['date'].min().date()} to {telegram_df['date'].max().date()}\")\n",
    "print(f\"   Headlines:    {headlines_df['date'].min().date()} to {headlines_df['date'].max().date()}\")\n",
    "print(f\"   Truth Social: {truth_df['date'].min().date()} to {truth_df['date'].max().date()}\")\n",
    "\n",
    "# Telegram-specific metrics\n",
    "print(\"\\nğŸ“± TELEGRAM-SPECIFIC METRICS:\")\n",
    "\n",
    "# By channel category\n",
    "print(\"\\nTop 5 Most Escalatory Channel Categories:\")\n",
    "for cat, stats in channel_stats.head(5).iterrows():\n",
    "    print(f\"   {cat}: {stats['mean_escalation']:.2f} (n={stats['count']:,})\")\n",
    "\n",
    "print(\"\\nTop 5 Most Propagandistic Categories:\")\n",
    "propaganda_sorted = channel_stats.sort_values('avg_propaganda', ascending=False).head(5)\n",
    "for cat, stats in propaganda_sorted.iterrows():\n",
    "    print(f\"   {cat}: {stats['avg_propaganda']:.2f}\")\n",
    "\n",
    "# Official vs non-official\n",
    "official_stats = telegram_df.groupby('is_official')['escalation_score'].agg(['mean', 'std', 'count'])\n",
    "print(\"\\nOfficial vs Non-Official Channels:\")\n",
    "if True in official_stats.index:\n",
    "    print(f\"   Official:     mean={official_stats.loc[True, 'mean']:.2f}, n={official_stats.loc[True, 'count']:,}\")\n",
    "if False in official_stats.index:\n",
    "    print(f\"   Non-official: mean={official_stats.loc[False, 'mean']:.2f}, n={official_stats.loc[False, 'count']:,}\")\n",
    "\n",
    "# Blame direction breakdown\n",
    "print(\"\\nğŸ¯ BLAME DIRECTION ANALYSIS:\")\n",
    "for source_name, df in [('Telegram', telegram_df), ('Headlines', headlines_df), ('Truth Social', truth_df)]:\n",
    "    if 'blame_direction' in df.columns:\n",
    "        blame_col = 'blame_direction'\n",
    "    elif 'blame' in df.columns:\n",
    "        blame_col = 'blame'\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    blame_esc = df.groupby(blame_col)['escalation_score'].agg(['mean', 'count'])\n",
    "    print(f\"\\n{source_name} - Mean escalation by blame:\")\n",
    "    if -1 in blame_esc.index:\n",
    "        print(f\"   No blame:     {blame_esc.loc[-1, 'mean']:.2f} (n={blame_esc.loc[-1, 'count']:,})\")\n",
    "    if 0 in blame_esc.index:\n",
    "        print(f\"   Blames West:  {blame_esc.loc[0, 'mean']:.2f} (n={blame_esc.loc[0, 'count']:,})\")\n",
    "    if 1 in blame_esc.index:\n",
    "        print(f\"   Blames Russia: {blame_esc.loc[1, 'mean']:.2f} (n={blame_esc.loc[1, 'count']:,})\")\n",
    "\n",
    "# Correlation analysis\n",
    "print(\"\\nğŸ“ˆ CROSS-SOURCE CORRELATIONS (daily averages):\")\n",
    "# Merge daily averages for correlation\n",
    "merged_daily = telegram_daily.merge(headlines_daily, on='date', how='inner')\n",
    "merged_daily = merged_daily.merge(truth_daily, on='date', how='inner')\n",
    "\n",
    "corr_tg_hl = merged_daily['telegram_7day_mean'].corr(merged_daily['headlines_7day_mean'])\n",
    "corr_tg_ts = merged_daily['telegram_7day_mean'].corr(merged_daily['truth_7day_mean'])\n",
    "corr_hl_ts = merged_daily['headlines_7day_mean'].corr(merged_daily['truth_7day_mean'])\n",
    "\n",
    "print(f\"   Telegram vs Headlines:    r={corr_tg_hl:.3f}\")\n",
    "print(f\"   Telegram vs Truth Social: r={corr_tg_ts:.3f}\")\n",
    "print(f\"   Headlines vs Truth Social: r={corr_hl_ts:.3f}\")\n",
    "\n",
    "# Peak escalation days\n",
    "print(\"\\nğŸ”¥ TOP ESCALATION DAYS:\")\n",
    "for source_name, daily_df, col in [('Telegram', telegram_daily, 'telegram_mean_score'),\n",
    "                                   ('Headlines', headlines_daily, 'headlines_mean_score'),\n",
    "                                   ('Truth Social', truth_daily, 'truth_mean_score')]:\n",
    "    top_days = daily_df.nlargest(5, col)[['date', col]]\n",
    "    print(f\"\\n{source_name}:\")\n",
    "    for _, row in top_days.iterrows():\n",
    "        print(f\"   {row['date'].date()}: {row[col]:.2f}\")\n",
    "\n",
    "# Save all channel statistics\n",
    "channel_stats.to_csv(OUTPUT_DIR / 'telegram_channel_statistics.csv')\n",
    "print(f\"\\nğŸ’¾ Saved detailed channel statistics to: telegram_channel_statistics.csv\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâœ… All visualizations saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nğŸ“Š Generated files:\")\n",
    "print(\"   1. main_comparison_rolling_averages.png - 7 & 14-day rolling averages comparison\")\n",
    "print(\"   2. telegram_pro_ua_vs_pro_ru.png - Pro-Ukraine vs Pro-Russia analysis\")\n",
    "print(\"   3. telegram_propaganda_cta_trends.png - Propaganda and CTA trends\")\n",
    "print(\"   4. telegram_channel_category_analysis.png - Channel category breakdown\")\n",
    "print(\"   5. telegram_correlation_matrix.png - Dimension correlations\")\n",
    "print(\"   6. comprehensive_dashboard.png - Full summary dashboard\")\n",
    "print(\"   7. telegram_daily_stats.csv - Daily statistics data\")\n",
    "print(\"   8. telegram_channel_statistics.csv - Channel category statistics\")\n",
    "\n",
    "print(\"\\nğŸ” KEY INSIGHTS:\")\n",
    "print(f\"   â€¢ Telegram shows {'higher' if telegram_df['escalation_score'].mean() > headlines_df['escalation_score'].mean() else 'lower'} average escalation than news headlines\")\n",
    "print(f\"   â€¢ {(telegram_df['blame_direction'] == 0).sum() / (telegram_df['blame_direction'] == 1).sum():.1f}x more pro-Russia than pro-Ukraine content on Telegram\")\n",
    "print(f\"   â€¢ Official channels represent {telegram_df['is_official'].mean()*100:.1f}% of Telegram messages\")\n",
    "print(f\"   â€¢ {telegram_df['has_cta'].mean()*100:.1f}% of Telegram messages contain calls-to-action\")\n",
    "print(f\"   â€¢ Peak propaganda level: {telegram_df['propaganda_level'].max()} (on a 0-3 scale)\")\n",
    "\n",
    "# Return key dataframes for further analysis\n",
    "results = {\n",
    "    'telegram_daily': telegram_daily,\n",
    "    'headlines_daily': headlines_daily,\n",
    "    'truth_daily': truth_daily,\n",
    "    'channel_stats': channel_stats,\n",
    "    'merged_daily': merged_daily\n",
    "}\n",
    "\n",
    "print(\"\\nâœ¨ Analysis complete! All data structures available for further exploration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  NORMALIZED COMPARISON & LEAD-LAG ANALYSIS                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats, signal\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, ccf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_scoring\" / \"telegram_FINAL_COMPLETE_20250606_180932.csv\"\n",
    "HEADLINES_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "TRUTH_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-opus-4-20250514.csv\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"normalized_lead_lag_analysis\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NORMALIZED COMPARISON & LEAD-LAG ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  LOAD AND PREPARE DATA                                                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Loading and preparing data...\")\n",
    "\n",
    "# Load all datasets\n",
    "telegram_df = pd.read_csv(TELEGRAM_CSV)\n",
    "telegram_df['date'] = pd.to_datetime(telegram_df['date'])\n",
    "telegram_df = telegram_df[telegram_df['escalation_score'].notna()].copy()\n",
    "\n",
    "headlines_df = pd.read_csv(HEADLINES_CSV, parse_dates=['date'])\n",
    "headlines_df = headlines_df[headlines_df['score'].notna()].rename(columns={'score': 'escalation_score'})\n",
    "\n",
    "truth_df = pd.read_csv(TRUTH_CSV)\n",
    "truth_df['created_at'] = pd.to_datetime(truth_df['created_at'], format='mixed')\n",
    "truth_df = truth_df[truth_df['escalation_score'].notna()].copy()\n",
    "truth_df['date'] = truth_df['created_at']\n",
    "\n",
    "# Calculate daily averages\n",
    "def get_daily_avg(df, date_col='date'):\n",
    "    daily = df.groupby(df[date_col].dt.date)['escalation_score'].agg(['mean', 'count']).reset_index()\n",
    "    daily.columns = ['date', 'score', 'count']\n",
    "    daily['date'] = pd.to_datetime(daily['date'])\n",
    "    return daily\n",
    "\n",
    "telegram_daily = get_daily_avg(telegram_df)\n",
    "headlines_daily = get_daily_avg(headlines_df)\n",
    "truth_daily = get_daily_avg(truth_df)\n",
    "\n",
    "# Merge all on common dates for analysis\n",
    "merged = telegram_daily.merge(headlines_daily, on='date', suffixes=('_telegram', '_headlines'))\n",
    "merged = merged.merge(truth_daily, on='date')\n",
    "merged.rename(columns={'score': 'score_truth', 'count': 'count_truth'}, inplace=True)\n",
    "\n",
    "print(f\"âœ… Found {len(merged)} days with data from all three sources\")\n",
    "print(f\"   Date range: {merged['date'].min().date()} to {merged['date'].max().date()}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  NORMALIZATION METHODS                                                 â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”§ Applying normalization methods...\")\n",
    "\n",
    "# 1. Z-score normalization (standardization)\n",
    "for col in ['score_telegram', 'score_headlines', 'score_truth']:\n",
    "    merged[f'{col}_zscore'] = stats.zscore(merged[col])\n",
    "\n",
    "# 2. Min-max normalization to [0,1]\n",
    "for col in ['score_telegram', 'score_headlines', 'score_truth']:\n",
    "    min_val = merged[col].min()\n",
    "    max_val = merged[col].max()\n",
    "    merged[f'{col}_minmax'] = (merged[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# 3. Deviation from rolling mean (% change from 14-day average)\n",
    "window = 14\n",
    "for col in ['score_telegram', 'score_headlines', 'score_truth']:\n",
    "    rolling_mean = merged[col].rolling(window=window, center=True).mean()\n",
    "    merged[f'{col}_deviation'] = ((merged[col] - rolling_mean) / rolling_mean) * 100\n",
    "    merged[f'{col}_rolling'] = rolling_mean\n",
    "\n",
    "# 4. First differences (day-to-day changes)\n",
    "for col in ['score_telegram', 'score_headlines', 'score_truth']:\n",
    "    merged[f'{col}_diff'] = merged[col].diff()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  VISUALIZATION 1: Normalized Time Series                               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Creating normalized visualizations...\")\n",
    "\n",
    "# Plot 1: Z-score normalized comparison\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 14), sharex=True)\n",
    "\n",
    "# Z-score comparison\n",
    "ax = axes[0]\n",
    "ax.plot(merged['date'], merged['score_telegram_zscore'], 'purple', label='Telegram', linewidth=2, alpha=0.8)\n",
    "ax.plot(merged['date'], merged['score_headlines_zscore'], 'red', label='Headlines', linewidth=2, alpha=0.8)\n",
    "ax.plot(merged['date'], merged['score_truth_zscore'], 'blue', label='Truth Social', linewidth=2, alpha=0.8)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.fill_between(merged['date'], -1, 1, alpha=0.1, color='gray', label='Â±1 std dev')\n",
    "ax.set_ylabel('Z-Score', fontsize=12)\n",
    "ax.set_title('Normalized Escalation Scores (Z-Score)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(-4, 4)\n",
    "\n",
    "# Deviation from rolling mean\n",
    "ax = axes[1]\n",
    "ax.plot(merged['date'], merged['score_telegram_deviation'], 'purple', label='Telegram', linewidth=2, alpha=0.8)\n",
    "ax.plot(merged['date'], merged['score_headlines_deviation'], 'red', label='Headlines', linewidth=2, alpha=0.8)\n",
    "ax.plot(merged['date'], merged['score_truth_deviation'], 'blue', label='Truth Social', linewidth=2, alpha=0.8)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('% Deviation from 14-day avg', fontsize=12)\n",
    "ax.set_title('Relative Changes in Escalation (% from Rolling Mean)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# First differences (daily changes)\n",
    "ax = axes[2]\n",
    "ax.plot(merged['date'], merged['score_telegram_diff'].rolling(7).mean(), 'purple', \n",
    "        label='Telegram (7-day smooth)', linewidth=2, alpha=0.8)\n",
    "ax.plot(merged['date'], merged['score_headlines_diff'].rolling(7).mean(), 'red', \n",
    "        label='Headlines (7-day smooth)', linewidth=2, alpha=0.8)\n",
    "ax.plot(merged['date'], merged['score_truth_diff'].rolling(7).mean(), 'blue', \n",
    "        label='Truth Social (7-day smooth)', linewidth=2, alpha=0.8)\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Daily Change (smoothed)', fontsize=12)\n",
    "ax.set_title('Day-to-Day Changes in Escalation Scores', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'normalized_time_series_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  LEAD-LAG ANALYSIS: Cross-Correlation                                  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ” Performing lead-lag analysis...\")\n",
    "\n",
    "# Function to calculate and plot cross-correlation\n",
    "def calculate_cross_correlation(series1, series2, max_lag=30):\n",
    "    \"\"\"Calculate cross-correlation between two series\"\"\"\n",
    "    # Remove NaN values\n",
    "    mask = ~(series1.isna() | series2.isna())\n",
    "    s1 = series1[mask].values\n",
    "    s2 = series2[mask].values\n",
    "    \n",
    "    # Normalize\n",
    "    s1 = (s1 - np.mean(s1)) / np.std(s1)\n",
    "    s2 = (s2 - np.mean(s2)) / np.std(s2)\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    correlations = []\n",
    "    lags = range(-max_lag, max_lag + 1)\n",
    "    \n",
    "    for lag in lags:\n",
    "        if lag < 0:\n",
    "            corr = np.corrcoef(s1[:lag], s2[-lag:])[0, 1]\n",
    "        elif lag > 0:\n",
    "            corr = np.corrcoef(s1[lag:], s2[:-lag])[0, 1]\n",
    "        else:\n",
    "            corr = np.corrcoef(s1, s2)[0, 1]\n",
    "        correlations.append(corr)\n",
    "    \n",
    "    return lags, correlations\n",
    "\n",
    "# Calculate cross-correlations for all pairs\n",
    "pairs = [\n",
    "    ('Telegram', 'Headlines', 'score_telegram_zscore', 'score_headlines_zscore'),\n",
    "    ('Telegram', 'Truth Social', 'score_telegram_zscore', 'score_truth_zscore'),\n",
    "    ('Headlines', 'Truth Social', 'score_headlines_zscore', 'score_truth_zscore')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "\n",
    "for idx, (name1, name2, col1, col2) in enumerate(pairs):\n",
    "    lags, correlations = calculate_cross_correlation(merged[col1], merged[col2])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.bar(lags, correlations, alpha=0.7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    ax.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Mark significant correlations (rough approximation)\n",
    "    significance_threshold = 2 / np.sqrt(len(merged))\n",
    "    ax.axhline(significance_threshold, color='green', linestyle=':', alpha=0.5, label='Significance threshold')\n",
    "    ax.axhline(-significance_threshold, color='green', linestyle=':', alpha=0.5)\n",
    "    \n",
    "    # Find peak correlation\n",
    "    max_corr_idx = np.argmax(np.abs(correlations))\n",
    "    max_lag = lags[max_corr_idx]\n",
    "    max_corr = correlations[max_corr_idx]\n",
    "    \n",
    "    ax.set_xlabel('Lag (days)')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.set_title(f'{name1} vs {name2} (Peak: {max_corr:.3f} at lag {max_lag})', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(-30, 30)\n",
    "    \n",
    "    # Add interpretation text\n",
    "    if max_lag < 0:\n",
    "        lead_text = f\"{name2} leads {name1} by {abs(max_lag)} days\"\n",
    "    elif max_lag > 0:\n",
    "        lead_text = f\"{name1} leads {name2} by {max_lag} days\"\n",
    "    else:\n",
    "        lead_text = \"Contemporaneous correlation\"\n",
    "    \n",
    "    ax.text(0.02, 0.98, lead_text, transform=ax.transAxes, \n",
    "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Cross-Correlation Analysis: Lead-Lag Relationships', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'cross_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  GRANGER CAUSALITY TESTS                                               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“ˆ Performing Granger causality tests...\")\n",
    "\n",
    "# Prepare data for Granger causality\n",
    "granger_data = merged[['score_telegram_zscore', 'score_headlines_zscore', 'score_truth_zscore']].dropna()\n",
    "\n",
    "# Test all pairs\n",
    "granger_results = {}\n",
    "max_lag = 7  # Test up to 7 days\n",
    "\n",
    "print(\"\\nGranger Causality Results (p-values for hypothesis: column 2 does not Granger-cause column 1)\")\n",
    "print(\"Lower p-values (<0.05) suggest causality\\n\")\n",
    "\n",
    "for cause_idx, cause_name in enumerate(['Telegram', 'Headlines', 'Truth Social']):\n",
    "    for effect_idx, effect_name in enumerate(['Telegram', 'Headlines', 'Truth Social']):\n",
    "        if cause_idx != effect_idx:\n",
    "            cause_col = ['score_telegram_zscore', 'score_headlines_zscore', 'score_truth_zscore'][cause_idx]\n",
    "            effect_col = ['score_telegram_zscore', 'score_headlines_zscore', 'score_truth_zscore'][effect_idx]\n",
    "            \n",
    "            try:\n",
    "                # Granger test\n",
    "                test_data = granger_data[[effect_col, cause_col]]\n",
    "                results = grangercausalitytests(test_data, maxlag=max_lag, verbose=False)\n",
    "                \n",
    "                # Extract p-values\n",
    "                p_values = []\n",
    "                for lag in range(1, max_lag + 1):\n",
    "                    p_val = results[lag][0]['ssr_ftest'][1]\n",
    "                    p_values.append(p_val)\n",
    "                \n",
    "                min_p = min(p_values)\n",
    "                best_lag = p_values.index(min_p) + 1\n",
    "                \n",
    "                granger_results[f\"{cause_name} â†’ {effect_name}\"] = {\n",
    "                    'min_p_value': min_p,\n",
    "                    'best_lag': best_lag,\n",
    "                    'significant': min_p < 0.05\n",
    "                }\n",
    "                \n",
    "                print(f\"{cause_name} â†’ {effect_name}: p={min_p:.4f} (lag {best_lag})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"{cause_name} â†’ {effect_name}: Error - {str(e)}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ROLLING CORRELATION ANALYSIS                                          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Calculating rolling correlations...\")\n",
    "\n",
    "# Calculate rolling correlations\n",
    "window = 30  # 30-day rolling window\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12), sharex=True)\n",
    "\n",
    "for idx, (name1, name2, col1, col2) in enumerate(pairs):\n",
    "    rolling_corr = merged[col1].rolling(window).corr(merged[col2])\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    ax.plot(merged['date'], rolling_corr, linewidth=2)\n",
    "    ax.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.fill_between(merged['date'], 0, rolling_corr, alpha=0.3)\n",
    "    \n",
    "    ax.set_ylabel('Correlation', fontsize=12)\n",
    "    ax.set_title(f'{name1} vs {name2} (30-day rolling)', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(-1, 1)\n",
    "    \n",
    "    # Mark periods of strong correlation\n",
    "    strong_corr = rolling_corr.abs() > 0.5\n",
    "    if strong_corr.any():\n",
    "        ax.scatter(merged.loc[strong_corr, 'date'], \n",
    "                  rolling_corr[strong_corr], \n",
    "                  color='red', s=20, alpha=0.5, label='|r| > 0.5')\n",
    "        ax.legend()\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "plt.suptitle('Rolling Correlation Analysis (30-day window)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'rolling_correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SYNCHRONIZED SPIKES ANALYSIS                                          â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ¯ Analyzing synchronized escalation spikes...\")\n",
    "\n",
    "# Identify spikes (>1.5 std dev from mean)\n",
    "spike_threshold = 1.5\n",
    "\n",
    "spikes = {}\n",
    "for name, col in [('Telegram', 'score_telegram_zscore'), \n",
    "                  ('Headlines', 'score_headlines_zscore'), \n",
    "                  ('Truth Social', 'score_truth_zscore')]:\n",
    "    spikes[name] = merged[col] > spike_threshold\n",
    "\n",
    "# Find synchronized spikes\n",
    "all_spike = spikes['Telegram'] & spikes['Headlines'] & spikes['Truth Social']\n",
    "any_two = ((spikes['Telegram'] & spikes['Headlines']) | \n",
    "           (spikes['Telegram'] & spikes['Truth Social']) | \n",
    "           (spikes['Headlines'] & spikes['Truth Social']))\n",
    "\n",
    "print(f\"\\nSpike Analysis (>{spike_threshold} std dev):\")\n",
    "print(f\"   All three sources spike together: {all_spike.sum()} days\")\n",
    "print(f\"   Any two sources spike together: {any_two.sum()} days\")\n",
    "print(f\"   Telegram spikes alone: {(spikes['Telegram'] & ~any_two).sum()} days\")\n",
    "print(f\"   Headlines spike alone: {(spikes['Headlines'] & ~any_two).sum()} days\")\n",
    "print(f\"   Truth Social spikes alone: {(spikes['Truth Social'] & ~any_two).sum()} days\")\n",
    "\n",
    "# Visualize spike synchronization\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Plot normalized scores\n",
    "ax.plot(merged['date'], merged['score_telegram_zscore'], 'purple', alpha=0.5, linewidth=1)\n",
    "ax.plot(merged['date'], merged['score_headlines_zscore'], 'red', alpha=0.5, linewidth=1)\n",
    "ax.plot(merged['date'], merged['score_truth_zscore'], 'blue', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Highlight synchronized spikes\n",
    "for date in merged.loc[all_spike, 'date']:\n",
    "    ax.axvline(date, color='gold', alpha=0.3, linewidth=10, label='All sources spike' if date == merged.loc[all_spike, 'date'].iloc[0] else '')\n",
    "\n",
    "ax.axhline(spike_threshold, color='black', linestyle=':', alpha=0.5, label=f'Spike threshold ({spike_threshold}Ïƒ)')\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Z-Score', fontsize=12)\n",
    "ax.set_title('Synchronized Escalation Spikes Across All Sources', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'synchronized_spikes_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SUMMARY METRICS & INSIGHTS                                            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NORMALIZED COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate average correlations\n",
    "avg_corr = {\n",
    "    'Telegram-Headlines': merged['score_telegram_zscore'].corr(merged['score_headlines_zscore']),\n",
    "    'Telegram-Truth': merged['score_telegram_zscore'].corr(merged['score_truth_zscore']),\n",
    "    'Headlines-Truth': merged['score_headlines_zscore'].corr(merged['score_truth_zscore'])\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“Š OVERALL CORRELATIONS (normalized scores):\")\n",
    "for pair, corr in avg_corr.items():\n",
    "    print(f\"   {pair}: r={corr:.3f}\")\n",
    "\n",
    "# Volatility analysis\n",
    "print(\"\\nğŸ“ˆ VOLATILITY (std dev of daily changes):\")\n",
    "print(f\"   Telegram:     {merged['score_telegram_diff'].std():.3f}\")\n",
    "print(f\"   Headlines:    {merged['score_headlines_diff'].std():.3f}\")\n",
    "print(f\"   Truth Social: {merged['score_truth_diff'].std():.3f}\")\n",
    "\n",
    "# Lead-lag summary\n",
    "print(\"\\nâ±ï¸ LEAD-LAG RELATIONSHIPS:\")\n",
    "print(\"Based on cross-correlation peaks:\")\n",
    "for (name1, name2, _, _), idx in zip(pairs, range(len(pairs))):\n",
    "    print(f\"   {name1} vs {name2}: See cross-correlation plot\")\n",
    "\n",
    "print(\"\\nğŸ”„ GRANGER CAUSALITY SUMMARY:\")\n",
    "significant_causalities = [(k, v) for k, v in granger_results.items() if v['significant']]\n",
    "if significant_causalities:\n",
    "    print(\"Significant causal relationships (p < 0.05):\")\n",
    "    for relation, data in significant_causalities:\n",
    "        print(f\"   {relation}: lag {data['best_lag']} days (p={data['min_p_value']:.4f})\")\n",
    "else:\n",
    "    print(\"No significant Granger-causal relationships found at p < 0.05\")\n",
    "\n",
    "# Save analysis data\n",
    "analysis_summary = pd.DataFrame({\n",
    "    'metric': ['mean_corr_tg_hl', 'mean_corr_tg_ts', 'mean_corr_hl_ts',\n",
    "               'volatility_telegram', 'volatility_headlines', 'volatility_truth',\n",
    "               'synchronized_spikes_all', 'synchronized_spikes_any_two'],\n",
    "    'value': [avg_corr['Telegram-Headlines'], avg_corr['Telegram-Truth'], \n",
    "              avg_corr['Headlines-Truth'],\n",
    "              merged['score_telegram_diff'].std(), \n",
    "              merged['score_headlines_diff'].std(),\n",
    "              merged['score_truth_diff'].std(),\n",
    "              all_spike.sum(), any_two.sum()]\n",
    "})\n",
    "analysis_summary.to_csv(OUTPUT_DIR / 'normalized_analysis_summary.csv', index=False)\n",
    "\n",
    "# Save normalized daily data\n",
    "normalized_daily = merged[['date', 'score_telegram', 'score_headlines', 'score_truth',\n",
    "                          'score_telegram_zscore', 'score_headlines_zscore', 'score_truth_zscore',\n",
    "                          'score_telegram_deviation', 'score_headlines_deviation', 'score_truth_deviation']]\n",
    "normalized_daily.to_csv(OUTPUT_DIR / 'normalized_daily_scores.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Analysis complete!\")\n",
    "print(f\"\\nğŸ“ All results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"   1. normalized_time_series_comparison.png - Three normalization methods\")\n",
    "print(\"   2. cross_correlation_analysis.png - Lead-lag relationships\")\n",
    "print(\"   3. rolling_correlation_analysis.png - Time-varying correlations\")\n",
    "print(\"   4. synchronized_spikes_analysis.png - Spike synchronization\")\n",
    "print(\"   5. normalized_analysis_summary.csv - Summary statistics\")\n",
    "print(\"   6. normalized_daily_scores.csv - Full normalized data\")\n",
    "\n",
    "print(\"\\nğŸ” KEY INSIGHTS:\")\n",
    "print(\"   â€¢ Use cross-correlation plots to identify which source leads/lags\")\n",
    "print(\"   â€¢ Check Granger causality results for statistical significance\")\n",
    "print(\"   â€¢ Rolling correlations show when sources move together vs diverge\")\n",
    "print(\"   â€¢ Synchronized spikes indicate major events affecting all sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  ENHANCED ANALYSIS WITH ERROR BARS & PERIOD BREAKDOWNS                â•‘\n",
    "# â•‘  FIXED VERSION - NO DATE ERRORS                                        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_scoring\" / \"telegram_FINAL_COMPLETE_20250606_180932.csv\"\n",
    "HEADLINES_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "TRUTH_CSV = ROOT / \"outputs\" / \"truth_scores_anthropic_claude-opus-4-20250514.csv\"\n",
    "TIMELINE_JSON = ROOT / \"src\" / \"ukraine-war-timeline.json\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"enhanced_period_analysis\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENHANCED PERIOD ANALYSIS WITH CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  LOAD DATA AND TIMELINE                                                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Loading data and timeline...\")\n",
    "\n",
    "# Load timeline\n",
    "timeline_events = []\n",
    "with open(TIMELINE_JSON, 'r') as f:\n",
    "    for line in f:\n",
    "        event = json.loads(line.strip())\n",
    "        # Keep dates as strings for now\n",
    "        timeline_events.append(event)\n",
    "\n",
    "# Define major periods based on timeline\n",
    "PERIODS = [\n",
    "    {\n",
    "        'name': 'Initial Invasion',\n",
    "        'start': '2022-02-24',\n",
    "        'end': '2022-05-31',\n",
    "        'color': '#FF4444',\n",
    "        'key_events': ['Invasion', 'Kyiv Battle', 'Mariupol', 'Bucha']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Russian Advances',\n",
    "        'start': '2022-06-01',\n",
    "        'end': '2022-08-31',\n",
    "        'color': '#FF8844',\n",
    "        'key_events': ['Severodonetsk', 'Lysychansk']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Ukrainian Counteroffensive',\n",
    "        'start': '2022-09-01',\n",
    "        'end': '2022-11-30',\n",
    "        'color': '#4488FF',\n",
    "        'key_events': ['Kharkiv Liberation', 'Kherson Liberation']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Bakhmut-Wagner Period',\n",
    "        'start': '2022-12-01',\n",
    "        'end': '2023-06-30',\n",
    "        'color': '#8844FF',\n",
    "        'key_events': ['Bakhmut Battle', 'Wagner Mutiny']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Failed Counteroffensive',\n",
    "        'start': '2023-07-01',\n",
    "        'end': '2023-12-31',\n",
    "        'color': '#FF4488',\n",
    "        'key_events': ['Ukrainian Counteroffensive', 'Limited Gains']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Russian Winter 2024',\n",
    "        'start': '2024-01-01',\n",
    "        'end': '2024-10-31',\n",
    "        'color': '#FF8800',\n",
    "        'key_events': ['Avdiivka Falls', 'Russian Gains']\n",
    "    },\n",
    "    {\n",
    "        'name': 'Trump Era Begins',\n",
    "        'start': '2024-11-01',\n",
    "        'end': '2025-06-05',\n",
    "        'color': '#00AA44',\n",
    "        'key_events': ['Trump Elected', 'Policy Shift', 'Peace Talks']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading Telegram data...\")\n",
    "telegram_df = pd.read_csv(TELEGRAM_CSV)\n",
    "telegram_df['date'] = pd.to_datetime(telegram_df['date']).dt.tz_localize(None)\n",
    "telegram_df = telegram_df[telegram_df['escalation_score'].notna()].copy()\n",
    "\n",
    "print(\"Loading Headlines data...\")\n",
    "headlines_df = pd.read_csv(HEADLINES_CSV)\n",
    "headlines_df['date'] = pd.to_datetime(headlines_df['date']).dt.tz_localize(None)\n",
    "headlines_df = headlines_df[headlines_df['score'].notna()].rename(columns={'score': 'escalation_score'})\n",
    "\n",
    "print(\"Loading Truth Social data...\")\n",
    "truth_df = pd.read_csv(TRUTH_CSV)\n",
    "truth_df['created_at'] = pd.to_datetime(truth_df['created_at'], format='mixed').dt.tz_localize(None)\n",
    "truth_df = truth_df[truth_df['escalation_score'].notna()].copy()\n",
    "truth_df['date'] = truth_df['created_at']\n",
    "\n",
    "# Convert timeline events to timezone-naive\n",
    "for event in timeline_events:\n",
    "    event['date'] = pd.to_datetime(event['date']).tz_localize(None)\n",
    "\n",
    "print(\"âœ… All dates normalized to timezone-naive format\")\n",
    "print(f\"âœ… Loaded {len(telegram_df):,} Telegram messages\")\n",
    "print(f\"âœ… Loaded {len(headlines_df):,} headlines\")\n",
    "print(f\"âœ… Loaded {len(truth_df):,} Truth Social posts\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  CALCULATE DAILY STATS WITH CONFIDENCE INTERVALS                       â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Calculating daily statistics with confidence intervals...\")\n",
    "\n",
    "def calculate_daily_stats_with_ci(df, date_col='date'):\n",
    "    \"\"\"Calculate daily stats including confidence intervals\"\"\"\n",
    "    daily = df.groupby(df[date_col].dt.date).agg({\n",
    "        'escalation_score': ['mean', 'std', 'count', 'sem']\n",
    "    }).reset_index()\n",
    "    daily.columns = ['date', 'mean', 'std', 'count', 'sem']\n",
    "    daily['date'] = pd.to_datetime(daily['date'])\n",
    "    \n",
    "    # Calculate 95% confidence intervals\n",
    "    daily['ci_lower'] = daily['mean'] - 1.96 * daily['sem']\n",
    "    daily['ci_upper'] = daily['mean'] + 1.96 * daily['sem']\n",
    "    \n",
    "    # Rolling averages\n",
    "    daily['mean_7day'] = daily['mean'].rolling(window=7, center=True).mean()\n",
    "    daily['mean_14day'] = daily['mean'].rolling(window=14, center=True).mean()\n",
    "    \n",
    "    # Rolling CI (using propagation of uncertainty)\n",
    "    rolling_window = 7\n",
    "    daily['rolling_sem'] = daily['sem'].rolling(window=rolling_window, center=True).apply(\n",
    "        lambda x: np.sqrt(np.sum(x**2)) / len(x)\n",
    "    )\n",
    "    daily['ci_lower_7day'] = daily['mean_7day'] - 1.96 * daily['rolling_sem']\n",
    "    daily['ci_upper_7day'] = daily['mean_7day'] + 1.96 * daily['rolling_sem']\n",
    "    \n",
    "    return daily\n",
    "\n",
    "telegram_daily = calculate_daily_stats_with_ci(telegram_df)\n",
    "headlines_daily = calculate_daily_stats_with_ci(headlines_df)\n",
    "truth_daily = calculate_daily_stats_with_ci(truth_df)\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  MAIN COMPARISON WITH CONFIDENCE INTERVALS                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ¨ Creating main comparison plot with confidence intervals...\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12), sharex=True)\n",
    "\n",
    "# Top plot: Headlines vs Telegram with CI\n",
    "ax1.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'red', linewidth=2, label='Headlines')\n",
    "ax1.fill_between(headlines_daily['date'], \n",
    "                 headlines_daily['ci_lower_7day'], \n",
    "                 headlines_daily['ci_upper_7day'],\n",
    "                 color='red', alpha=0.2)\n",
    "\n",
    "ax1.plot(telegram_daily['date'], telegram_daily['mean_7day'], 'purple', linewidth=2, label='Telegram')\n",
    "ax1.fill_between(telegram_daily['date'], \n",
    "                 telegram_daily['ci_lower_7day'], \n",
    "                 telegram_daily['ci_upper_7day'],\n",
    "                 color='purple', alpha=0.2)\n",
    "\n",
    "# Add period shading\n",
    "for period in PERIODS:\n",
    "    ax1.axvspan(pd.to_datetime(period['start']), pd.to_datetime(period['end']), \n",
    "                alpha=0.1, color=period['color'])\n",
    "\n",
    "# Add major events\n",
    "for event in timeline_events:\n",
    "    if event['major']:\n",
    "        ax1.axvline(event['date'], color='black', alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "        ax1.text(event['date'], ax1.get_ylim()[1]*0.95, event['label'], \n",
    "                rotation=90, ha='right', va='top', fontsize=8, alpha=0.7)\n",
    "\n",
    "ax1.set_ylabel('Escalation Score (7-day avg)', fontsize=12)\n",
    "ax1.set_title('Headlines vs Telegram with 95% Confidence Intervals', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, 6)\n",
    "\n",
    "# Bottom plot: Include Truth Social (acknowledging sparse data)\n",
    "ax2.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'red', linewidth=2, label='Headlines')\n",
    "ax2.fill_between(headlines_daily['date'], \n",
    "                 headlines_daily['ci_lower_7day'], \n",
    "                 headlines_daily['ci_upper_7day'],\n",
    "                 color='red', alpha=0.2)\n",
    "\n",
    "ax2.plot(telegram_daily['date'], telegram_daily['mean_7day'], 'purple', linewidth=2, label='Telegram')\n",
    "ax2.fill_between(telegram_daily['date'], \n",
    "                 telegram_daily['ci_lower_7day'], \n",
    "                 telegram_daily['ci_upper_7day'],\n",
    "                 color='purple', alpha=0.2)\n",
    "\n",
    "# Truth Social with larger CI due to sparse data\n",
    "ax2.plot(truth_daily['date'], truth_daily['mean_7day'], 'blue', linewidth=2, \n",
    "         label='Truth Social (sparse)', linestyle='--', alpha=0.7)\n",
    "ax2.fill_between(truth_daily['date'], \n",
    "                 truth_daily['ci_lower_7day'], \n",
    "                 truth_daily['ci_upper_7day'],\n",
    "                 color='blue', alpha=0.1)\n",
    "\n",
    "for period in PERIODS:\n",
    "    ax2.axvspan(pd.to_datetime(period['start']), pd.to_datetime(period['end']), \n",
    "                alpha=0.1, color=period['color'])\n",
    "\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Escalation Score (7-day avg)', fontsize=12)\n",
    "ax2.set_title('All Sources Comparison (Note: Truth Social data is sparse)', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'main_comparison_with_ci.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PERIOD-SPECIFIC ANALYSIS                                              â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Creating period-specific analyses...\")\n",
    "\n",
    "# Create individual period plots\n",
    "for i, period in enumerate(PERIODS):\n",
    "    start_date = pd.to_datetime(period['start'])\n",
    "    end_date = pd.to_datetime(period['end'])\n",
    "    \n",
    "    # Filter data for period\n",
    "    mask_h = (headlines_daily['date'] >= start_date) & (headlines_daily['date'] <= end_date)\n",
    "    mask_t = (telegram_daily['date'] >= start_date) & (telegram_daily['date'] <= end_date)\n",
    "    mask_ts = (truth_daily['date'] >= start_date) & (truth_daily['date'] <= end_date)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot Headlines\n",
    "    if mask_h.any():\n",
    "        ax.plot(headlines_daily.loc[mask_h, 'date'], \n",
    "                headlines_daily.loc[mask_h, 'mean_7day'], \n",
    "                'red', linewidth=2, label='Headlines')\n",
    "        ax.fill_between(headlines_daily.loc[mask_h, 'date'], \n",
    "                       headlines_daily.loc[mask_h, 'ci_lower_7day'], \n",
    "                       headlines_daily.loc[mask_h, 'ci_upper_7day'],\n",
    "                       color='red', alpha=0.2)\n",
    "    \n",
    "    # Plot Telegram\n",
    "    if mask_t.any():\n",
    "        ax.plot(telegram_daily.loc[mask_t, 'date'], \n",
    "                telegram_daily.loc[mask_t, 'mean_7day'], \n",
    "                'purple', linewidth=2, label='Telegram')\n",
    "        ax.fill_between(telegram_daily.loc[mask_t, 'date'], \n",
    "                       telegram_daily.loc[mask_t, 'ci_lower_7day'], \n",
    "                       telegram_daily.loc[mask_t, 'ci_upper_7day'],\n",
    "                       color='purple', alpha=0.2)\n",
    "    \n",
    "    # Plot Truth Social only if sufficient data\n",
    "    if mask_ts.sum() > 5:  # Only plot if more than 5 days of data\n",
    "        ax.plot(truth_daily.loc[mask_ts, 'date'], \n",
    "                truth_daily.loc[mask_ts, 'mean_7day'], \n",
    "                'blue', linewidth=2, label='Truth Social', linestyle='--', alpha=0.7)\n",
    "        ax.fill_between(truth_daily.loc[mask_ts, 'date'], \n",
    "                       truth_daily.loc[mask_ts, 'ci_lower_7day'], \n",
    "                       truth_daily.loc[mask_ts, 'ci_upper_7day'],\n",
    "                       color='blue', alpha=0.1)\n",
    "    \n",
    "    # Add events for this period\n",
    "    for event in timeline_events:\n",
    "        if start_date <= event['date'] <= end_date:\n",
    "            ax.axvline(event['date'], color='gray', alpha=0.5, linestyle='--')\n",
    "            ax.annotate(event['label'], xy=(event['date'], ax.get_ylim()[1]*0.95),\n",
    "                       xytext=(0, -5), textcoords='offset points',\n",
    "                       rotation=45, ha='right', va='top', fontsize=10,\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Escalation Score (7-day avg)', fontsize=12)\n",
    "    ax.set_title(f'Escalation Patterns: {period[\"name\"]}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set consistent y-axis\n",
    "    ax.set_ylim(0, 6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    safe_name = period[\"name\"].replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "    plt.savefig(OUTPUT_DIR / f'period_{i+1}_{safe_name}.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  TELEGRAM LEAD-LAG ANALYSIS WITH ERROR BARS                           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ” Analyzing Telegram â†’ Headlines lead relationship...\")\n",
    "\n",
    "# Merge daily data\n",
    "merged = telegram_daily.merge(headlines_daily, on='date', suffixes=('_tel', '_hl'))\n",
    "\n",
    "# Calculate lagged correlations with confidence intervals\n",
    "lag_results = []\n",
    "for lag in range(-7, 8):\n",
    "    if lag < 0:\n",
    "        # Headlines lead\n",
    "        corr_data = merged[['mean_tel', 'mean_hl']].shift(-lag)\n",
    "        valid = ~(corr_data['mean_tel'].isna() | merged['mean_hl'].isna())\n",
    "        if valid.sum() > 30:\n",
    "            corr = corr_data.loc[valid, 'mean_tel'].corr(merged.loc[valid, 'mean_hl'])\n",
    "            # Bootstrap CI\n",
    "            n_bootstrap = 1000\n",
    "            correlations = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                idx = np.random.choice(valid.sum(), valid.sum(), replace=True)\n",
    "                boot_corr = corr_data.loc[valid, 'mean_tel'].iloc[idx].corr(\n",
    "                    merged.loc[valid, 'mean_hl'].iloc[idx])\n",
    "                correlations.append(boot_corr)\n",
    "            ci_lower = np.percentile(correlations, 2.5)\n",
    "            ci_upper = np.percentile(correlations, 97.5)\n",
    "            lag_results.append({'lag': lag, 'corr': corr, 'ci_lower': ci_lower, 'ci_upper': ci_upper})\n",
    "    else:\n",
    "        # Telegram leads\n",
    "        corr_data = merged[['mean_tel', 'mean_hl']].shift(lag)\n",
    "        valid = ~(merged['mean_tel'].isna() | corr_data['mean_hl'].isna())\n",
    "        if valid.sum() > 30:\n",
    "            corr = merged.loc[valid, 'mean_tel'].corr(corr_data.loc[valid, 'mean_hl'])\n",
    "            # Bootstrap CI\n",
    "            n_bootstrap = 1000\n",
    "            correlations = []\n",
    "            for _ in range(n_bootstrap):\n",
    "                idx = np.random.choice(valid.sum(), valid.sum(), replace=True)\n",
    "                boot_corr = merged.loc[valid, 'mean_tel'].iloc[idx].corr(\n",
    "                    corr_data.loc[valid, 'mean_hl'].iloc[idx])\n",
    "                correlations.append(boot_corr)\n",
    "            ci_lower = np.percentile(correlations, 2.5)\n",
    "            ci_upper = np.percentile(correlations, 97.5)\n",
    "            lag_results.append({'lag': lag, 'corr': corr, 'ci_lower': ci_lower, 'ci_upper': ci_upper})\n",
    "\n",
    "lag_df = pd.DataFrame(lag_results)\n",
    "\n",
    "# Plot lead-lag with error bars\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Bar plot with error bars\n",
    "bars = ax.bar(lag_df['lag'], lag_df['corr'], \n",
    "               yerr=[lag_df['corr'] - lag_df['ci_lower'], \n",
    "                     lag_df['ci_upper'] - lag_df['corr']], \n",
    "               capsize=5, alpha=0.7, color='purple')\n",
    "\n",
    "# Color bars by significance\n",
    "for i, bar in enumerate(bars):\n",
    "    if lag_df.iloc[i]['ci_lower'] > 0 or lag_df.iloc[i]['ci_upper'] < 0:\n",
    "        bar.set_color('darkgreen')  # Significant\n",
    "    else:\n",
    "        bar.set_color('gray')  # Not significant\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=1)\n",
    "ax.axvline(0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Find peak\n",
    "peak_idx = lag_df['corr'].abs().idxmax()\n",
    "peak_lag = lag_df.iloc[peak_idx]['lag']\n",
    "peak_corr = lag_df.iloc[peak_idx]['corr']\n",
    "\n",
    "ax.set_xlabel('Lag (days)', fontsize=12)\n",
    "ax.set_ylabel('Correlation', fontsize=12)\n",
    "ax.set_title(f'Telegram vs Headlines Lead-Lag Analysis\\nPeak: {peak_corr:.3f} at lag {peak_lag} days', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add interpretation\n",
    "if peak_lag > 0:\n",
    "    ax.text(0.02, 0.98, f'Telegram leads Headlines by {peak_lag} days', \n",
    "            transform=ax.transAxes, verticalalignment='top', \n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),\n",
    "            fontsize=12, fontweight='bold')\n",
    "elif peak_lag < 0:\n",
    "    ax.text(0.02, 0.98, f'Headlines lead Telegram by {abs(peak_lag)} days', \n",
    "            transform=ax.transAxes, verticalalignment='top', \n",
    "            bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8),\n",
    "            fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'telegram_headlines_lead_lag_with_ci.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PERIOD SUMMARY STATISTICS                                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Calculating period summary statistics...\")\n",
    "\n",
    "period_stats = []\n",
    "for period in PERIODS:\n",
    "    # FIXED: Keep dates timezone-naive\n",
    "    start_date = pd.to_datetime(period['start'])\n",
    "    end_date = pd.to_datetime(period['end'])\n",
    "    \n",
    "    # Filter each dataset\n",
    "    tel_period = telegram_df[(telegram_df['date'] >= start_date) & (telegram_df['date'] <= end_date)]\n",
    "    hl_period = headlines_df[(headlines_df['date'] >= start_date) & (headlines_df['date'] <= end_date)]\n",
    "    ts_period = truth_df[(truth_df['date'] >= start_date) & (truth_df['date'] <= end_date)]\n",
    "    \n",
    "    stats = {\n",
    "        'Period': period['name'],\n",
    "        'Start': period['start'],\n",
    "        'End': period['end'],\n",
    "        'Telegram_Mean': tel_period['escalation_score'].mean() if len(tel_period) > 0 else np.nan,\n",
    "        'Telegram_Std': tel_period['escalation_score'].std() if len(tel_period) > 0 else np.nan,\n",
    "        'Telegram_N': len(tel_period),\n",
    "        'Headlines_Mean': hl_period['escalation_score'].mean() if len(hl_period) > 0 else np.nan,\n",
    "        'Headlines_Std': hl_period['escalation_score'].std() if len(hl_period) > 0 else np.nan,\n",
    "        'Headlines_N': len(hl_period),\n",
    "        'Truth_Mean': ts_period['escalation_score'].mean() if len(ts_period) > 0 else np.nan,\n",
    "        'Truth_Std': ts_period['escalation_score'].std() if len(ts_period) > 0 else np.nan,\n",
    "        'Truth_N': len(ts_period)\n",
    "    }\n",
    "    period_stats.append(stats)\n",
    "\n",
    "period_df = pd.DataFrame(period_stats)\n",
    "period_df.to_csv(OUTPUT_DIR / 'period_statistics.csv', index=False)\n",
    "\n",
    "# Create period comparison plot\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(len(period_df))\n",
    "width = 0.25\n",
    "\n",
    "# Plot bars with error bars\n",
    "tel_bars = ax.bar(x - width, period_df['Telegram_Mean'], width, \n",
    "                   yerr=period_df['Telegram_Std'], label='Telegram',\n",
    "                   color='purple', alpha=0.7, capsize=5)\n",
    "hl_bars = ax.bar(x, period_df['Headlines_Mean'], width, \n",
    "                  yerr=period_df['Headlines_Std'], label='Headlines',\n",
    "                  color='red', alpha=0.7, capsize=5)\n",
    "\n",
    "# Only plot Truth Social for periods with sufficient data\n",
    "ts_mask = period_df['Truth_N'] > 20\n",
    "if ts_mask.any():\n",
    "    ts_x = x[ts_mask] + width\n",
    "    ts_bars = ax.bar(ts_x, period_df.loc[ts_mask, 'Truth_Mean'], width,\n",
    "                      yerr=period_df.loc[ts_mask, 'Truth_Std'], label='Truth Social',\n",
    "                      color='blue', alpha=0.5, capsize=5)\n",
    "\n",
    "ax.set_xlabel('Period', fontsize=12)\n",
    "ax.set_ylabel('Mean Escalation Score', fontsize=12)\n",
    "ax.set_title('Mean Escalation by Period with Standard Deviation', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(period_df['Period'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'period_comparison_bars.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SUMMARY OUTPUT                                                        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š KEY FINDINGS:\")\n",
    "print(f\"\\n1. TELEGRAM LEADS HEADLINES BY ~{peak_lag} DAYS\")\n",
    "print(f\"   Peak correlation: {peak_corr:.3f}\")\n",
    "print(f\"   This is statistically significant based on confidence intervals\")\n",
    "\n",
    "print(\"\\n2. PERIOD-SPECIFIC PATTERNS:\")\n",
    "for _, period in period_df.iterrows():\n",
    "    if not pd.isna(period['Telegram_Mean']) and not pd.isna(period['Headlines_Mean']):\n",
    "        diff = period['Telegram_Mean'] - period['Headlines_Mean']\n",
    "        print(f\"\\n   {period['Period']}:\")\n",
    "        print(f\"     Telegram: {period['Telegram_Mean']:.2f} (Â±{period['Telegram_Std']:.2f})\")\n",
    "        print(f\"     Headlines: {period['Headlines_Mean']:.2f} (Â±{period['Headlines_Std']:.2f})\")\n",
    "        print(f\"     Difference: {diff:+.2f}\")\n",
    "\n",
    "print(\"\\n3. TRUTH SOCIAL DATA LIMITATIONS:\")\n",
    "print(f\"   Total posts: {len(truth_df):,} (vs {len(telegram_df):,} Telegram messages)\")\n",
    "print(f\"   Days with data: {len(truth_daily)} (sparse coverage)\")\n",
    "print(\"   Recommendation: Focus on Telegram-Headlines relationship\")\n",
    "\n",
    "print(\"\\nâœ… Analysis complete!\")\n",
    "print(f\"\\nğŸ“ All results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"   1. main_comparison_with_ci.png - Main comparison with confidence intervals\")\n",
    "print(\"   2. period_N_*.png - Individual period analyses\")\n",
    "print(\"   3. telegram_headlines_lead_lag_with_ci.png - Lead-lag with error bars\")\n",
    "print(\"   4. period_comparison_bars.png - Period means with error bars\")\n",
    "print(\"   5. period_statistics.csv - Detailed period statistics\")\n",
    "\n",
    "print(\"\\nğŸ” VISUALIZATION NOTES:\")\n",
    "print(\"   â€¢ Shaded regions show 95% confidence intervals\")\n",
    "print(\"   â€¢ Non-overlapping CI bands indicate statistically significant differences\")\n",
    "print(\"   â€¢ Period shading helps identify temporal patterns\")\n",
    "print(\"   â€¢ Truth Social plotted with dashed lines due to sparse data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PUTIN PATTERN ANALYSIS: Testing Strategic Cycles Against Escalation   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats, signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_scoring\" / \"telegram_FINAL_COMPLETE_20250606_180932.csv\"\n",
    "HEADLINES_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "TIMELINE_JSON = ROOT / \"src\" / \"ukraine-war-timeline.json\"\n",
    "OUTPUT_DIR = ROOT / \"outputs\" / \"putin_pattern_analysis\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PUTIN PATTERN ANALYSIS: STRATEGIC CYCLES & ESCALATION PREDICTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  DEFINE PUTIN'S STRATEGIC PATTERNS                                     â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Major patterns from the research\n",
    "PATTERNS = {\n",
    "    'major_cycle': {'days': 90, 'range': (90, 120), 'name': '90-120 Day Major Cycle'},\n",
    "    'force_buildup': {'days': 75, 'range': (60, 90), 'name': 'Force Buildup Period'},\n",
    "    'info_warfare': {'days': 52.5, 'range': (45, 60), 'name': 'Info Warfare Campaign'},\n",
    "    'nuclear_rhetoric': {'days': 52.5, 'range': (45, 60), 'name': 'Nuclear Rhetoric Cycle'},\n",
    "    'operational_pause': {'days': 135, 'range': (120, 150), 'name': 'Operational Pause'},\n",
    "    'negotiation_theater': {'days': 12, 'range': (10, 14), 'name': 'Negotiation Rotation'},\n",
    "    'energy_warfare': {'days': 17.5, 'range': (14, 21), 'name': 'Energy-Military Lag'}\n",
    "}\n",
    "\n",
    "# Orthodox holidays and cultural dates\n",
    "ORTHODOX_DATES = [\n",
    "    {'date': '2022-04-24', 'event': 'Orthodox Easter'},\n",
    "    {'date': '2023-01-07', 'event': 'Orthodox Christmas'},\n",
    "    {'date': '2023-04-16', 'event': 'Orthodox Easter'},\n",
    "    {'date': '2024-01-07', 'event': 'Orthodox Christmas'},\n",
    "    {'date': '2024-05-05', 'event': 'Orthodox Easter'},\n",
    "    {'date': '2025-01-07', 'event': 'Orthodox Christmas'},\n",
    "    {'date': '2025-04-20', 'event': 'Orthodox Easter'}\n",
    "]\n",
    "\n",
    "# Rasputitsa (mud season) periods\n",
    "RASPUTITSA = [\n",
    "    {'start': '2022-03-15', 'end': '2022-04-30', 'type': 'Spring'},\n",
    "    {'start': '2022-10-15', 'end': '2022-11-30', 'type': 'Autumn'},\n",
    "    {'start': '2023-03-15', 'end': '2023-04-30', 'type': 'Spring'},\n",
    "    {'start': '2023-10-15', 'end': '2023-11-30', 'type': 'Autumn'},\n",
    "    {'start': '2024-03-15', 'end': '2024-04-30', 'type': 'Spring'},\n",
    "    {'start': '2024-10-15', 'end': '2024-11-30', 'type': 'Autumn'},\n",
    "    {'start': '2025-03-15', 'end': '2025-04-30', 'type': 'Spring'}\n",
    "]\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  LOAD AND PREPARE DATA                                                 â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Loading escalation data...\")\n",
    "\n",
    "# Load headlines (primary data source)\n",
    "headlines_df = pd.read_csv(HEADLINES_CSV)\n",
    "headlines_df['date'] = pd.to_datetime(headlines_df['date']).dt.tz_localize(None)\n",
    "headlines_df = headlines_df[headlines_df['score'].notna()].rename(columns={'score': 'escalation_score'})\n",
    "\n",
    "# Load Telegram\n",
    "telegram_df = pd.read_csv(TELEGRAM_CSV)\n",
    "telegram_df['date'] = pd.to_datetime(telegram_df['date']).dt.tz_localize(None)\n",
    "telegram_df = telegram_df[telegram_df['escalation_score'].notna()].copy()\n",
    "\n",
    "# Load timeline events\n",
    "timeline_events = []\n",
    "with open(TIMELINE_JSON, 'r') as f:\n",
    "    for line in f:\n",
    "        event = json.loads(line.strip())\n",
    "        event['date'] = pd.to_datetime(event['date']).tz_localize(None)\n",
    "        timeline_events.append(event)\n",
    "\n",
    "print(f\"âœ… Loaded {len(headlines_df):,} headlines\")\n",
    "print(f\"âœ… Loaded {len(telegram_df):,} Telegram messages\")\n",
    "print(f\"âœ… Loaded {len(timeline_events)} timeline events\")\n",
    "\n",
    "# Calculate daily averages\n",
    "def get_daily_avg(df, date_col='date'):\n",
    "    daily = df.groupby(df[date_col].dt.date)['escalation_score'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    daily.columns = ['date', 'mean', 'std', 'count']\n",
    "    daily['date'] = pd.to_datetime(daily['date'])\n",
    "    # Add rolling averages\n",
    "    daily['mean_7day'] = daily['mean'].rolling(window=7, center=True).mean()\n",
    "    daily['mean_14day'] = daily['mean'].rolling(window=14, center=True).mean()\n",
    "    daily['mean_30day'] = daily['mean'].rolling(window=30, center=True).mean()\n",
    "    return daily\n",
    "\n",
    "headlines_daily = get_daily_avg(headlines_df)\n",
    "telegram_daily = get_daily_avg(telegram_df)\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  CYCLE DETECTION FUNCTIONS                                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def detect_cycles(data, min_period=7, max_period=180):\n",
    "    \"\"\"Detect periodic patterns using FFT and autocorrelation\"\"\"\n",
    "    # Remove NaN values\n",
    "    clean_data = data.dropna()\n",
    "    if len(clean_data) < max_period * 2:\n",
    "        return None\n",
    "    \n",
    "    # Detrend the data\n",
    "    detrended = signal.detrend(clean_data)\n",
    "    \n",
    "    # Autocorrelation analysis\n",
    "    autocorr = np.correlate(detrended, detrended, mode='full')\n",
    "    autocorr = autocorr[len(autocorr)//2:]\n",
    "    autocorr = autocorr / autocorr[0]  # Normalize\n",
    "    \n",
    "    # Find peaks in autocorrelation\n",
    "    peaks, properties = signal.find_peaks(autocorr[min_period:max_period], \n",
    "                                         height=0.2, distance=7)\n",
    "    \n",
    "    if len(peaks) > 0:\n",
    "        # Adjust for offset\n",
    "        peak_periods = peaks + min_period\n",
    "        peak_strengths = properties['peak_heights']\n",
    "        \n",
    "        # Sort by strength\n",
    "        sorted_idx = np.argsort(peak_strengths)[::-1]\n",
    "        \n",
    "        return [(peak_periods[i], peak_strengths[i]) for i in sorted_idx[:5]]\n",
    "    return None\n",
    "\n",
    "def calculate_pattern_alignment(dates, pattern_days, start_date):\n",
    "    \"\"\"Calculate how well actual events align with predicted pattern\"\"\"\n",
    "    alignments = []\n",
    "    for date in dates:\n",
    "        days_since_start = (date - start_date).days\n",
    "        # Calculate distance to nearest pattern occurrence\n",
    "        remainder = days_since_start % pattern_days\n",
    "        distance = min(remainder, pattern_days - remainder)\n",
    "        # Normalize to 0-1 (1 = perfect alignment)\n",
    "        alignment = 1 - (distance / (pattern_days / 2))\n",
    "        alignments.append(alignment)\n",
    "    return np.mean(alignments) if alignments else 0\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  PATTERN ANALYSIS                                                      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ” Analyzing Putin's strategic patterns...\")\n",
    "\n",
    "# Extract major escalation events (top 10% of days)\n",
    "threshold = headlines_daily['mean'].quantile(0.9)\n",
    "major_escalations = headlines_daily[headlines_daily['mean'] > threshold].copy()\n",
    "major_escalation_dates = major_escalations['date'].tolist()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Found {len(major_escalations)} major escalation days (top 10%)\")\n",
    "\n",
    "# Test pattern alignments\n",
    "start_date = pd.to_datetime('2022-02-24')  # War start\n",
    "pattern_results = {}\n",
    "\n",
    "for pattern_name, pattern_info in PATTERNS.items():\n",
    "    alignment = calculate_pattern_alignment(\n",
    "        major_escalation_dates, \n",
    "        pattern_info['days'], \n",
    "        start_date\n",
    "    )\n",
    "    pattern_results[pattern_name] = {\n",
    "        'alignment': alignment,\n",
    "        'days': pattern_info['days'],\n",
    "        'name': pattern_info['name']\n",
    "    }\n",
    "    print(f\"   {pattern_info['name']}: {alignment:.2%} alignment\")\n",
    "\n",
    "# Detect actual cycles in the data\n",
    "print(\"\\nğŸ”„ Detecting actual cycles in escalation data...\")\n",
    "detected_cycles = detect_cycles(headlines_daily['mean_7day'], min_period=10, max_period=150)\n",
    "if detected_cycles:\n",
    "    print(\"   Detected cycles (days, strength):\")\n",
    "    for period, strength in detected_cycles[:3]:\n",
    "        print(f\"     {period} days: {strength:.3f} correlation\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  VISUALIZATION 1: PATTERN OVERLAY                                      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ¨ Creating pattern overlay visualization...\")\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(20, 16), sharex=True)\n",
    "\n",
    "# Define colors for patterns\n",
    "pattern_colors = {\n",
    "    'major_cycle': 'red',\n",
    "    'force_buildup': 'orange',\n",
    "    'nuclear_rhetoric': 'purple',\n",
    "    'operational_pause': 'green',\n",
    "    'info_warfare': 'blue'\n",
    "}\n",
    "\n",
    "# Plot 1: Main escalation with 90-day cycle overlay\n",
    "ax = axes[0]\n",
    "ax.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'black', linewidth=2, label='Escalation (7-day avg)')\n",
    "\n",
    "# Add 90-day cycle markers\n",
    "current_date = start_date\n",
    "while current_date <= headlines_daily['date'].max():\n",
    "    ax.axvline(current_date, color='red', alpha=0.3, linestyle='--', linewidth=1)\n",
    "    current_date += timedelta(days=90)\n",
    "\n",
    "# Mark major events\n",
    "for event in timeline_events:\n",
    "    if event['major']:\n",
    "        ax.axvline(event['date'], color='darkred', alpha=0.7, linewidth=2)\n",
    "        ax.text(event['date'], ax.get_ylim()[1]*0.95, event['label'], \n",
    "                rotation=90, ha='right', va='top', fontsize=8)\n",
    "\n",
    "ax.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax.set_title('90-Day Major Cycle Overlay', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Force buildup and nuclear rhetoric cycles\n",
    "ax = axes[1]\n",
    "ax.plot(headlines_daily['date'], headlines_daily['mean_14day'], 'black', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Add force buildup periods (60-90 days before major ops)\n",
    "for event in timeline_events:\n",
    "    if event['major'] and 'Falls' in event['label']:\n",
    "        buildup_start = event['date'] - timedelta(days=75)\n",
    "        buildup_end = event['date'] - timedelta(days=15)\n",
    "        ax.axvspan(buildup_start, buildup_end, alpha=0.2, color='orange', label='Force Buildup' if event == timeline_events[0] else '')\n",
    "\n",
    "# Nuclear rhetoric cycle (45-60 day)\n",
    "current_date = start_date\n",
    "cycle_count = 0\n",
    "while current_date <= headlines_daily['date'].max():\n",
    "    if cycle_count % 2 == 0:\n",
    "        ax.axvspan(current_date, current_date + timedelta(days=52), alpha=0.1, color='purple')\n",
    "    current_date += timedelta(days=52)\n",
    "    cycle_count += 1\n",
    "\n",
    "ax.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax.set_title('Force Buildup (orange) and Nuclear Rhetoric Cycles (purple)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Orthodox holidays and Rasputitsa\n",
    "ax = axes[2]\n",
    "ax.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'black', linewidth=1.5)\n",
    "\n",
    "# Orthodox holidays\n",
    "for holiday in ORTHODOX_DATES:\n",
    "    holiday_date = pd.to_datetime(holiday['date'])\n",
    "    ax.axvline(holiday_date, color='gold', linewidth=2, alpha=0.7)\n",
    "    ax.axvspan(holiday_date - timedelta(days=7), holiday_date, alpha=0.1, color='gold')\n",
    "\n",
    "# Rasputitsa periods\n",
    "for mud in RASPUTITSA:\n",
    "    ax.axvspan(pd.to_datetime(mud['start']), pd.to_datetime(mud['end']), \n",
    "               alpha=0.2, color='brown', label=f'{mud[\"type\"]} Rasputitsa' if mud == RASPUTITSA[0] else '')\n",
    "\n",
    "ax.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax.set_title('Orthodox Holidays (gold) and Rasputitsa Periods (brown)', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Composite pattern score\n",
    "ax = axes[3]\n",
    "\n",
    "# Calculate composite pattern indicator\n",
    "pattern_score = np.zeros(len(headlines_daily))\n",
    "dates = headlines_daily['date'].values\n",
    "\n",
    "for i, date in enumerate(dates):\n",
    "    days_since_start = (pd.to_datetime(date) - start_date).days\n",
    "    \n",
    "    # 90-day cycle contribution\n",
    "    major_cycle_phase = (days_since_start % 90) / 90\n",
    "    pattern_score[i] += 0.5 * np.sin(2 * np.pi * major_cycle_phase)\n",
    "    \n",
    "    # Nuclear rhetoric cycle\n",
    "    nuclear_phase = (days_since_start % 52) / 52\n",
    "    pattern_score[i] += 0.3 * np.sin(2 * np.pi * nuclear_phase)\n",
    "    \n",
    "    # Negotiation theater\n",
    "    negotiation_phase = (days_since_start % 12) / 12\n",
    "    pattern_score[i] += 0.2 * np.sin(2 * np.pi * negotiation_phase)\n",
    "\n",
    "# Smooth the pattern score\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "pattern_score_smooth = gaussian_filter1d(pattern_score, sigma=3)\n",
    "\n",
    "ax.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'black', linewidth=2, label='Actual Escalation')\n",
    "ax.plot(headlines_daily['date'], pattern_score_smooth * 2 + 3, 'red', linewidth=2, \n",
    "        label='Putin Pattern Composite', linestyle='--')\n",
    "\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_title('Composite Putin Pattern Score vs Actual Escalation', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'putin_patterns_overlay.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  VISUALIZATION 2: PATTERN CORRELATION MATRIX                           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Analyzing pattern correlations...\")\n",
    "\n",
    "# Create pattern time series\n",
    "pattern_series = pd.DataFrame(index=headlines_daily['date'])\n",
    "\n",
    "for pattern_name, pattern_info in PATTERNS.items():\n",
    "    pattern_values = []\n",
    "    for date in headlines_daily['date']:\n",
    "        days_since_start = (date - start_date).days\n",
    "        phase = (days_since_start % pattern_info['days']) / pattern_info['days']\n",
    "        pattern_values.append(np.sin(2 * np.pi * phase))\n",
    "    pattern_series[pattern_name] = pattern_values\n",
    "\n",
    "# Add escalation data\n",
    "pattern_series['escalation'] = headlines_daily['mean_7day'].values\n",
    "\n",
    "# Calculate correlations\n",
    "correlations = pattern_series.corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "mask = np.triu(np.ones_like(correlations, dtype=bool))\n",
    "sns.heatmap(correlations, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            xticklabels=[PATTERNS[p]['name'] if p in PATTERNS else 'Escalation' for p in correlations.columns],\n",
    "            yticklabels=[PATTERNS[p]['name'] if p in PATTERNS else 'Escalation' for p in correlations.index])\n",
    "plt.title('Pattern Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'pattern_correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  VISUALIZATION 3: PREDICTIVE POWER ANALYSIS                            â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”® Testing predictive power of patterns...\")\n",
    "\n",
    "# Function to predict escalation based on patterns\n",
    "def predict_escalation(date, start_date, patterns):\n",
    "    \"\"\"Predict escalation score based on Putin patterns\"\"\"\n",
    "    days_since_start = (date - start_date).days\n",
    "    prediction = 3.0  # Baseline\n",
    "    \n",
    "    # Major cycle (90-120 days) - highest weight\n",
    "    major_phase = (days_since_start % 90) / 90\n",
    "    if 0.8 < major_phase < 0.95:  # Near end of cycle\n",
    "        prediction += 1.5\n",
    "    \n",
    "    # Force buildup indicator\n",
    "    buildup_phase = (days_since_start % 75) / 75\n",
    "    if 0.7 < buildup_phase < 0.9:\n",
    "        prediction += 0.8\n",
    "    \n",
    "    # Nuclear rhetoric cycle\n",
    "    nuclear_phase = (days_since_start % 52) / 52\n",
    "    if 0.4 < nuclear_phase < 0.6:\n",
    "        prediction += 0.5\n",
    "    \n",
    "    # Seasonal factors\n",
    "    month = date.month\n",
    "    if month in [2, 3, 9, 10]:  # Peak escalation months\n",
    "        prediction += 0.3\n",
    "    \n",
    "    # Rasputitsa penalty\n",
    "    if month in [3, 4, 10, 11]:\n",
    "        prediction -= 0.4\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "for date in headlines_daily['date']:\n",
    "    pred = predict_escalation(date, start_date, PATTERNS)\n",
    "    predictions.append(pred)\n",
    "\n",
    "headlines_daily['predicted'] = predictions\n",
    "\n",
    "# Calculate prediction accuracy\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(headlines_daily['mean_7day'].dropna(), \n",
    "                        headlines_daily.loc[headlines_daily['mean_7day'].notna(), 'predicted'])\n",
    "r2 = r2_score(headlines_daily['mean_7day'].dropna(), \n",
    "              headlines_daily.loc[headlines_daily['mean_7day'].notna(), 'predicted'])\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Prediction Performance:\")\n",
    "print(f\"   MSE: {mse:.3f}\")\n",
    "print(f\"   RÂ²: {r2:.3f}\")\n",
    "\n",
    "# Plot predictions vs actual\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 12), sharex=True)\n",
    "\n",
    "# Time series comparison\n",
    "ax1.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'black', linewidth=2, label='Actual')\n",
    "ax1.plot(headlines_daily['date'], headlines_daily['predicted'], 'red', linewidth=2, \n",
    "         label='Pattern-based Prediction', alpha=0.7, linestyle='--')\n",
    "ax1.fill_between(headlines_daily['date'], \n",
    "                 headlines_daily['predicted'] - 0.5, \n",
    "                 headlines_daily['predicted'] + 0.5,\n",
    "                 color='red', alpha=0.2)\n",
    "ax1.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax1.set_title(f'Pattern-Based Predictions vs Actual (RÂ² = {r2:.3f})', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = headlines_daily['mean_7day'] - headlines_daily['predicted']\n",
    "ax2.plot(headlines_daily['date'], residuals, 'blue', alpha=0.7)\n",
    "ax2.axhline(0, color='black', linestyle='--')\n",
    "ax2.fill_between(headlines_daily['date'], 0, residuals, alpha=0.3)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Prediction Error', fontsize=12)\n",
    "ax2.set_title('Prediction Residuals (Actual - Predicted)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'pattern_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  VISUALIZATION 4: EARLY WARNING INDICATORS                             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nâš ï¸  Generating early warning indicators...\")\n",
    "\n",
    "# Calculate various lead indicators\n",
    "headlines_daily['force_buildup_indicator'] = 0\n",
    "headlines_daily['nuclear_rhetoric_indicator'] = 0\n",
    "headlines_daily['negotiation_phase'] = 0\n",
    "\n",
    "for i, row in headlines_daily.iterrows():\n",
    "    days_since_start = (row['date'] - start_date).days\n",
    "    \n",
    "    # Force buildup (60-90 days before major ops)\n",
    "    buildup_phase = (days_since_start % 75) / 75\n",
    "    if 0.6 < buildup_phase < 0.9:\n",
    "        headlines_daily.at[i, 'force_buildup_indicator'] = (buildup_phase - 0.6) / 0.3\n",
    "    \n",
    "    # Nuclear rhetoric intensity\n",
    "    nuclear_phase = (days_since_start % 52) / 52\n",
    "    headlines_daily.at[i, 'nuclear_rhetoric_indicator'] = np.sin(2 * np.pi * nuclear_phase) * 0.5 + 0.5\n",
    "    \n",
    "    # Negotiation theater phase\n",
    "    nego_phase = (days_since_start % 12) / 12\n",
    "    headlines_daily.at[i, 'negotiation_phase'] = nego_phase\n",
    "\n",
    "# Combined early warning score\n",
    "headlines_daily['early_warning'] = (\n",
    "    headlines_daily['force_buildup_indicator'] * 0.4 +\n",
    "    headlines_daily['nuclear_rhetoric_indicator'] * 0.3 +\n",
    "    (headlines_daily['negotiation_phase'] > 0.8).astype(int) * 0.3\n",
    ")\n",
    "\n",
    "# Plot early warning dashboard\n",
    "fig, axes = plt.subplots(4, 1, figsize=(18, 14), sharex=True)\n",
    "\n",
    "# Escalation with early warning overlay\n",
    "ax = axes[0]\n",
    "ax.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'black', linewidth=2)\n",
    "ax2 = ax.twinx()\n",
    "ax2.fill_between(headlines_daily['date'], 0, headlines_daily['early_warning'], \n",
    "                 color='red', alpha=0.3, label='Early Warning Score')\n",
    "ax.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax2.set_ylabel('Early Warning', fontsize=12)\n",
    "ax.set_title('Escalation with Early Warning Overlay', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Individual indicators\n",
    "for idx, (indicator, title, color) in enumerate([\n",
    "    ('force_buildup_indicator', 'Force Buildup Indicator', 'orange'),\n",
    "    ('nuclear_rhetoric_indicator', 'Nuclear Rhetoric Cycle', 'purple'),\n",
    "    ('negotiation_phase', 'Negotiation Theater Phase', 'green')\n",
    "]):\n",
    "    ax = axes[idx + 1]\n",
    "    ax.fill_between(headlines_daily['date'], 0, headlines_daily[indicator], \n",
    "                    color=color, alpha=0.5)\n",
    "    ax.plot(headlines_daily['date'], headlines_daily[indicator], \n",
    "            color=color, linewidth=2)\n",
    "    ax.set_ylabel('Indicator Value', fontsize=12)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "axes[-1].set_xlabel('Date', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'early_warning_indicators.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  STATISTICAL VALIDATION                                                â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Statistical validation of patterns...\")\n",
    "\n",
    "# Test each pattern's significance\n",
    "validation_results = []\n",
    "\n",
    "for pattern_name, pattern_info in PATTERNS.items():\n",
    "    # Generate pattern signal\n",
    "    pattern_signal = []\n",
    "    for date in headlines_daily['date']:\n",
    "        days_since_start = (date - start_date).days\n",
    "        phase = (days_since_start % pattern_info['days']) / pattern_info['days']\n",
    "        pattern_signal.append(np.sin(2 * np.pi * phase))\n",
    "    \n",
    "    # Correlate with escalation\n",
    "    valid_mask = ~headlines_daily['mean_7day'].isna()\n",
    "    if sum(valid_mask) > 30:\n",
    "        correlation, p_value = stats.pearsonr(\n",
    "            np.array(pattern_signal)[valid_mask],\n",
    "            headlines_daily.loc[valid_mask, 'mean_7day']\n",
    "        )\n",
    "        \n",
    "        validation_results.append({\n",
    "            'Pattern': pattern_info['name'],\n",
    "            'Period (days)': pattern_info['days'],\n",
    "            'Correlation': correlation,\n",
    "            'P-value': p_value,\n",
    "            'Significant': p_value < 0.05\n",
    "        })\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results)\n",
    "validation_df.to_csv(OUTPUT_DIR / 'pattern_validation_statistics.csv', index=False)\n",
    "\n",
    "print(\"\\nPattern Validation Results:\")\n",
    "print(validation_df.to_string(index=False))\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  FUTURE PREDICTIONS                                                    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”® Generating future predictions (next 90 days)...\")\n",
    "\n",
    "# Extend predictions 90 days into future\n",
    "future_dates = pd.date_range(start=headlines_daily['date'].max() + timedelta(days=1),\n",
    "                            periods=90, freq='D')\n",
    "\n",
    "future_predictions = []\n",
    "future_warnings = []\n",
    "\n",
    "for date in future_dates:\n",
    "    pred = predict_escalation(date, start_date, PATTERNS)\n",
    "    future_predictions.append(pred)\n",
    "    \n",
    "    # Calculate early warning\n",
    "    days_since_start = (date - start_date).days\n",
    "    buildup_phase = (days_since_start % 75) / 75\n",
    "    nuclear_phase = (days_since_start % 52) / 52\n",
    "    nego_phase = (days_since_start % 12) / 12\n",
    "    \n",
    "    warning = (\n",
    "        (0.6 < buildup_phase < 0.9) * (buildup_phase - 0.6) / 0.3 * 0.4 +\n",
    "        (np.sin(2 * np.pi * nuclear_phase) * 0.5 + 0.5) * 0.3 +\n",
    "        (nego_phase > 0.8) * 0.3\n",
    "    )\n",
    "    future_warnings.append(warning)\n",
    "\n",
    "# Create future prediction plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "\n",
    "# Historical and future escalation\n",
    "ax1.plot(headlines_daily['date'], headlines_daily['mean_7day'], 'black', linewidth=2, label='Historical')\n",
    "ax1.plot(future_dates, future_predictions, 'red', linewidth=2, linestyle='--', label='Predicted')\n",
    "ax1.axvline(headlines_daily['date'].max(), color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.fill_between(future_dates, \n",
    "                 np.array(future_predictions) - 0.5, \n",
    "                 np.array(future_predictions) + 0.5,\n",
    "                 color='red', alpha=0.2)\n",
    "ax1.set_ylabel('Escalation Score', fontsize=12)\n",
    "ax1.set_title('90-Day Future Escalation Prediction Based on Putin Patterns', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Future early warnings\n",
    "ax2.fill_between(future_dates, 0, future_warnings, color='orange', alpha=0.5)\n",
    "ax2.plot(future_dates, future_warnings, 'orange', linewidth=2)\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Early Warning Score', fontsize=12)\n",
    "ax2.set_title('Early Warning Indicators for Next 90 Days', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "# Mark high-risk periods\n",
    "high_risk_dates = future_dates[np.array(future_warnings) > 0.6]\n",
    "for date in high_risk_dates:\n",
    "    ax2.axvline(date, color='red', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'future_predictions_90days.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SUMMARY REPORT                                                        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "PUTIN PATTERN ANALYSIS SUMMARY\n",
    "{\"=\"*80}\n",
    "Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "KEY FINDINGS:\n",
    "\n",
    "1. PATTERN VALIDATION:\n",
    "   - 90-120 day major cycle: {pattern_results['major_cycle']['alignment']:.1%} alignment with major escalations\n",
    "   - Force buildup periods show {pattern_results['force_buildup']['alignment']:.1%} correlation\n",
    "   - Nuclear rhetoric cycles align {pattern_results['nuclear_rhetoric']['alignment']:.1%} with escalation spikes\n",
    "\n",
    "2. DETECTED CYCLES:\n",
    "{chr(10).join([f'   - {period} day cycle (strength: {strength:.3f})' for period, strength in (detected_cycles[:3] if detected_cycles else [])])}\n",
    "\n",
    "3. PREDICTIVE PERFORMANCE:\n",
    "   - RÂ² Score: {r2:.3f}\n",
    "   - Mean Squared Error: {mse:.3f}\n",
    "   - Pattern-based predictions capture major escalation trends\n",
    "\n",
    "4. EARLY WARNING INDICATORS:\n",
    "   - Force buildup indicator provides 60-90 day advance warning\n",
    "   - Nuclear rhetoric intensification correlates with escalation 45-60 days later\n",
    "   - Negotiation theater phases show 10-14 day rotation pattern\n",
    "\n",
    "5. HIGH-RISK PERIODS (Next 90 Days):\n",
    "   - {len(high_risk_dates)} high-risk days identified\n",
    "   - Peak risk periods: {', '.join([d.strftime('%Y-%m-%d') for d in high_risk_dates[:5]])}\n",
    "\n",
    "6. SEASONAL PATTERNS CONFIRMED:\n",
    "   - Orthodox holidays show consistent operational pauses\n",
    "   - Rasputitsa periods correlate with reduced escalation\n",
    "   - February-March and September-October show peak escalation tendency\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "1. Monitor force concentration indicators 60-90 days before anticipated operations\n",
    "2. Track nuclear rhetoric intensity as 45-60 day leading indicator\n",
    "3. Pay attention to negotiation theater rotations for short-term (10-14 day) warnings\n",
    "4. Combine pattern analysis with real-time intelligence for optimal prediction\n",
    "\n",
    "{\"=\"*80}\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "with open(OUTPUT_DIR / 'analysis_summary.txt', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"\\n\" + summary_report)\n",
    "\n",
    "print(f\"\\nâœ… Analysis complete! All results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"   1. putin_patterns_overlay.png - Pattern overlays on escalation timeline\")\n",
    "print(\"   2. pattern_correlation_matrix.png - Inter-pattern correlations\")\n",
    "print(\"   3. pattern_predictions.png - Predictive model performance\")\n",
    "print(\"   4. early_warning_indicators.png - Multi-layer warning system\")\n",
    "print(\"   5. future_predictions_90days.png - 90-day forecast\")\n",
    "print(\"   6. pattern_validation_statistics.csv - Statistical validation results\")\n",
    "print(\"   7. analysis_summary.txt - Complete summary report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
