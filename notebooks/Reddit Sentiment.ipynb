{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Reddit Sentiment Analysis Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv, os\n",
    "dotenv.load_dotenv()        # picks up .env in the working dir (or parent)\n",
    "print(os.getenv(\"REDDIT_CLIENT_ID\")[:4])   # sanity-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- .env loader that works in notebooks ------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def find_repo_root(start: Path, marker=\".git\") -> Path:\n",
    "    \"\"\"Walk up until we see a folder containing the given marker ('.git' or '.env').\"\"\"\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(f\"Repository root with {marker} not found from {start}\")\n",
    "\n",
    "# 1) locate repo root (folder that has .env **or** .git)\n",
    "repo_root = find_repo_root(Path.cwd(), \".env\")\n",
    "\n",
    "# 2) load environment variables\n",
    "load_dotenv(repo_root / \".env\")\n",
    "\n",
    "# 3) add src/ to Python path (optional, if you’ll import from src/)\n",
    "src_path = repo_root / \"src\"\n",
    "if src_path.exists():\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# 4) fetch secrets (raise fast if any missing)\n",
    "REQUIRED = [\"OPENAI_API_KEY\", \"NEWSAPI_KEY\"]\n",
    "CREDS = {k: os.getenv(k) for k in REQUIRED}\n",
    "missing = [k for k, v in CREDS.items() if not v]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing secrets in .env: {', '.join(missing)}\")\n",
    "\n",
    "# handy variables\n",
    "OPENAI_KEY       = CREDS[\"OPENAI_API_KEY\"]\n",
    "NEWSAPI_KEY      = CREDS[\"NEWSAPI_KEY\"]\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_SECRET    = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_AGENT     = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "print(f\"✅  .env loaded from {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime as dt, json, time, textwrap, math\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import openai, praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"REDDIT_USER_AGENT\"),\n",
    ")\n",
    "\n",
    "# ----------  OpenAI prompt formatter ----------\n",
    "SYSTEM_PROMPT = \"\"\"You are an analyst measuring public sentiment toward\n",
    "U.S./NATO military support for Ukraine. \n",
    "Rate the text on a 5-point scale:\n",
    " -2 = strongly against Western aid,\n",
    " -1 = somewhat against,\n",
    "  0 = neutral/unclear,\n",
    " +1 = somewhat supportive,\n",
    " +2 = strongly supportive.\n",
    "Return ONLY the integer.\"\"\"\n",
    "\n",
    "def get_sentiment(text: str, model=\"gpt-4o-mini\") -> int:\n",
    "    resp = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                  {\"role\":\"user\",\"content\":text[:4000]}],   # keep under limit\n",
    "        temperature=0,\n",
    "        max_tokens=2,\n",
    "    )\n",
    "    return int(resp.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Pull Small Trial of Posts & Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- parameters you can tweak ----------\n",
    "SUBREDDITS   = [\"ukraine\", \"worldnews\", \"geopolitics\"]\n",
    "KEYWORDS     = [\"nato\", \"western aid\", \"military support\", \"leopard\", \"patriot\"]\n",
    "START = dt.datetime.utcnow() - dt.timedelta(days=3)   # last 3 days\n",
    "END   = dt.datetime.utcnow()\n",
    "LIMIT_PER_SUB = 150                          # stays under Reddit's 100 QPM cap :contentReference[oaicite:2]{index=2}&#8203;:contentReference[oaicite:3]{index=3}\n",
    "# --------------------------------------------\n",
    "\n",
    "for k in (\"REDDIT_CLIENT_ID\", \"REDDIT_CLIENT_SECRET\", \"REDDIT_USER_AGENT\"):\n",
    "    print(f\"{k} →\", repr(os.getenv(k)))\n",
    "\n",
    "for p in reddit.subreddit(\"worldnews\").search(\n",
    "        '\"military support\"', sort=\"new\", limit=3):\n",
    "    print(p.created_utc, p.title[:80])\n",
    "\n",
    "rows = []\n",
    "for sub in SUBREDDITS:\n",
    "    sr = reddit.subreddit(sub)\n",
    "    query = \" OR \".join(f'\"{k}\"' for k in KEYWORDS)\n",
    "    for post in sr.search(query, sort=\"new\", time_filter=\"all\", limit=LIMIT_PER_SUB):\n",
    "        if not (START <= dt.datetime.utcfromtimestamp(post.created_utc) < END):\n",
    "            continue\n",
    "        rows.append(\n",
    "            {\"id\": post.id,\n",
    "             \"created\": dt.datetime.utcfromtimestamp(post.created_utc),\n",
    "             \"sub\": sub,\n",
    "             \"type\": \"post\",\n",
    "             \"text\": post.title + \"\\n\" + (post.selftext or \"\")}\n",
    "        )\n",
    "        post.comments.replace_more(limit=0)\n",
    "        for com in post.comments.list()[:50]:          # cap for demo\n",
    "            rows.append(\n",
    "                {\"id\": com.id,\n",
    "                 \"created\": dt.datetime.utcfromtimestamp(com.created_utc),\n",
    "                 \"sub\": sub,\n",
    "                 \"type\": \"comment\",\n",
    "                 \"text\": com.body}\n",
    "            )\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Collected {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)[[\"created\",\"sub\",\"type\",\"text\"]]\n",
    "df[\"sub\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import time, datetime as dt\n",
    "\n",
    "ONE_MONTH = timedelta(days=31)\n",
    "\n",
    "def month_windows(start, end):\n",
    "    t = end\n",
    "    while t > start:\n",
    "        yield (t - dt.timedelta(days=31), t)\n",
    "        t -= dt.timedelta(days=31)\n",
    "\n",
    "def collect_window(sub, query, t0, t1, limit=500):\n",
    "    rows, after = [], None\n",
    "    with tqdm(total=limit,\n",
    "              desc=f\"{sub:<15} {t0:%Y-%m}\",\n",
    "              leave=False,\n",
    "              unit=\"row\") as bar:\n",
    "        while True:\n",
    "            batch = list(\n",
    "                reddit.subreddit(sub).search(\n",
    "                    query,\n",
    "                    sort=\"new\",\n",
    "                    limit=100,\n",
    "                    params={\"after\": after} if after else {}\n",
    "                )\n",
    "            )\n",
    "            if not batch:\n",
    "                break\n",
    "            for post in batch:\n",
    "                ts = dt.datetime.utcfromtimestamp(post.created_utc)\n",
    "                if not (t0 <= ts < t1):\n",
    "                    continue\n",
    "                rows.append({\n",
    "                    \"id\": post.id,\n",
    "                    \"created\": ts,\n",
    "                    \"sub\": sub,\n",
    "                    \"type\": \"post\",\n",
    "                    \"text\": post.title + \"\\n\" + (post.selftext or \"\")\n",
    "                })\n",
    "                bar.update(1)\n",
    "                if len(rows) >= limit:\n",
    "                    break\n",
    "            after = batch[-1].fullname\n",
    "            if len(rows) >= limit:\n",
    "                break\n",
    "    return rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS = [\n",
    "    \"worldnews\",\"geopolitics\",\"news\",\n",
    "    \"ukraine\",\"RussiaUkraineWar2022\",\n",
    "    \"UkrainianConflict\",\"RussiaLago\"\n",
    "]\n",
    "KEYWORDS = [\"nato\",\"western aid\",\"military aid\",\"f-16\",\"leopard\",\"patriot\"]\n",
    "query = \" OR \".join(f'\"{k}\"' for k in KEYWORDS)\n",
    "\n",
    "START = datetime.utcnow() - timedelta(days=365)\n",
    "END   = datetime.utcnow()\n",
    "\n",
    "rows = []\n",
    "for sub in tqdm(SUBREDDITS, desc=\"Subreddits\"):\n",
    "    for t0, t1 in tqdm(list(month_windows(START, END)),\n",
    "                       desc=f\"{sub} months\", leave=False):\n",
    "        rows.extend(collect_window(sub, query, t0, t1))\n",
    "        time.sleep(0.5)          # polite pause\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"reddit_raw_v1.csv\", index=False)\n",
    "print(\"Total rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))        # how many rows survived\n",
    "df.head(5)            # first five\n",
    "df.tail(5)            # last five\n",
    "df[\"sub\"].value_counts()\n",
    "\n",
    "# 1.  give the current DataFrame a permanent home\n",
    "df.to_parquet(\"reddit_raw_v1.parquet\")      # works now that pyarrow is installed\n",
    "# or, if you prefer plain CSV\n",
    "# df.to_csv(\"reddit_raw_v1.csv\", index=False)\n",
    "\n",
    "# 2.  quick sanity-peek\n",
    "import pandas as pd\n",
    "peek = pd.read_parquet(\"reddit_raw_v1.parquet\")\n",
    "print(len(peek), \"rows on disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Grab Quick Review Set for Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Reload clean file so we start from the same baseline every session\n",
    "# ------------------------------------------------------------------\n",
    "import pandas as pd, pathlib\n",
    "\n",
    "DATA_FNAME = \"reddit_raw_v1.csv\"          # or .parquet if you installed pyarrow\n",
    "if DATA_FNAME.endswith(\".csv\"):\n",
    "    df = pd.read_csv(DATA_FNAME)\n",
    "else:\n",
    "    df = pd.read_parquet(DATA_FNAME)\n",
    "\n",
    "print(\"Loaded\", len(df), \"rows from disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. make sure dates are real datetime ------------------------\n",
    "df[\"created\"] = pd.to_datetime(df[\"created\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"created\"])          # drop any bad rows\n",
    "\n",
    "# plain string “YYYY-MM” for robust comparisons\n",
    "df[\"ym\"] = df[\"created\"].dt.strftime(\"%Y-%m\")\n",
    "\n",
    "# quick overview\n",
    "print(\"unique subs :\", df['sub'].unique())\n",
    "print(\"months span:\", df['ym'].min(), \"→\", df['ym'].max())\n",
    "print(\"rows        :\", len(df))\n",
    "\n",
    "# ---- 2. balanced 50-row sampler (fixed) -------------------------\n",
    "TARGET = 50\n",
    "strata = df.groupby([\"sub\", \"ym\"]).size().reset_index(name=\"n\")\n",
    "\n",
    "rng      = np.random.default_rng(42)\n",
    "indices  = []\n",
    "\n",
    "N = len(df)                       # 19 194 rows\n",
    "\n",
    "for _, row in strata.iterrows():\n",
    "    grp_mask = (df[\"sub\"] == row[\"sub\"]) & (df[\"ym\"] == row[\"ym\"])\n",
    "    grp_idx  = df.index[grp_mask]\n",
    "\n",
    "    if grp_idx.empty:\n",
    "        continue\n",
    "\n",
    "    share = row[\"n\"] / N\n",
    "    want  = max(1, int(np.ceil(TARGET * share)))\n",
    "    want  = min(want, len(grp_idx))          # len() is an int → safe\n",
    "\n",
    "    pick  = rng.choice(grp_idx, size=want, replace=False)\n",
    "    indices.extend(pick)\n",
    "\n",
    "print(\"collected\", len(indices), \"rows before trim\")\n",
    "\n",
    "review_df = (df.loc[indices]\n",
    "               .sample(n=TARGET, random_state=1)     # final trim/shuffle\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "print(\"✅ review_df length =\", len(review_df))\n",
    "\n",
    "review_df[[\"created\",\"sub\",\"type\",\"text\"]].to_csv(\"reddit_review_set.csv\", index=False)\n",
    "           \n",
    "print(\"✅ review set saved → reddit_review_set.csv\")\n",
    "\n",
    "\n",
    "# --- 3. quick bar-chart -----------------------------------------\n",
    "(review_df.groupby(\"sub\").size()\n",
    "          .sort_values()\n",
    "          .plot.barh(figsize=(6,3), alpha=.7))\n",
    "plt.title(\"Rows per subreddit in review sample\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Quick Pipeline for Tagging Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: write review set for manual labeling -----------------\n",
    "REVIEW_FPATH = \"reddit_review_set.csv\"   # already on disk – just remind yourself\n",
    "print(\"➡️  Open this file in Excel/Sheets, add a column 'label_human' (–2…+2),\",\n",
    "      \"and save.  15–20 min of quick coding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
