{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  STEP 1 â€“ Load Claude-labelled data                                   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd, numpy as np, os, json, datetime as dt\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "CLAUDE_CSV = ROOT / \"outputs\" / \"telegram_scoring\" / \"telegram_scored_COMBINED_20250606_124931.csv\"\n",
    "\n",
    "df = pd.read_csv(CLAUDE_CSV)\n",
    "assert {\"escalation_score\", \"blame_direction\", \"propaganda_level\", \"has_cta\"}.issubset(df.columns), \\\n",
    "       \"CSV missing one of the required score columns.\"\n",
    "\n",
    "# Parse a date column if present; fallback to row index\n",
    "for col in [\"created_at\", \"date\", \"timestamp\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        DATE_COL = col\n",
    "        break\n",
    "else:\n",
    "    df[\"row_idx_as_date\"] = pd.to_datetime(df.index, unit=\"D\", origin=\"2022-01-01\")\n",
    "    DATE_COL = \"row_idx_as_date\"\n",
    "\n",
    "# â”€â”€â”€ parameters â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SAMPLE_PER_SCORE = 20        # 11Ã—20 = 220 total (adjust as desired)\n",
    "RANDOM_SEED      = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  STEP 2 â€“ Stratified sampling across escalation + date               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "sampled_frames = []\n",
    "\n",
    "for esc in range(11):                     # 0 â€¦ 10 inclusive\n",
    "    sub = df[df.escalation_score == esc].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    \n",
    "    # split that subset into 4 date quartiles, take roughly equal chunks\n",
    "    sub[\"q\"] = pd.qcut(sub[DATE_COL], 4, labels=False, duplicates=\"drop\")\n",
    "    n_per_quartile = max(1, SAMPLE_PER_SCORE // 4)\n",
    "    \n",
    "    picks = []\n",
    "    for q, g in sub.groupby(\"q\"):\n",
    "        picks.append(g.sample(min(n_per_quartile, len(g)),\n",
    "                              random_state=RANDOM_SEED))\n",
    "    picked = pd.concat(picks)\n",
    "    \n",
    "    # if we still need more to hit SAMPLE_PER_SCORE, fill randomly\n",
    "    if len(picked) < SAMPLE_PER_SCORE:\n",
    "        remaining = sub.drop(picked.index)\n",
    "        if not remaining.empty:\n",
    "            picked = pd.concat([\n",
    "                picked,\n",
    "                remaining.sample(min(SAMPLE_PER_SCORE - len(picked), len(remaining)),\n",
    "                                 random_state=RANDOM_SEED)\n",
    "            ])\n",
    "    sampled_frames.append(picked)\n",
    "\n",
    "sampled = pd.concat(sampled_frames).sample(frac=1, random_state=RANDOM_SEED)  # shuffle\n",
    "\n",
    "print(f\"Sampled {len(sampled):,} rows across all escalation levels.\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  STEP 3 â€“ Add empty â€œhuman-verifyâ€ columns                           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "for col in [\"E_verified\", \"B_verified\", \"P_verified\", \"C_verified\"]:\n",
    "    sampled[col] = \"\"\n",
    "\n",
    "# Optional: keep only the columns you care about\n",
    "cols_to_keep = [\"message_text\", \"escalation_score\", \"blame_direction\",\n",
    "                \"propaganda_level\", \"has_cta\",\n",
    "                \"channel_username\", DATE_COL] + [c for c in sampled.columns if c.endswith(\"_verified\")]\n",
    "sampled = sampled[cols_to_keep]\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  STEP 4 â€“ Save for Google Sheets / Streamlit review                  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "OUT_DIR = ROOT / \"outputs\" / \"labeling\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "out_path = OUT_DIR / f\"telegram_sample_for_review_{dt.datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "sampled.to_csv(out_path, index=False)\n",
    "print(\"âœ… Saved sample to\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, json, time\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "anthropic_client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "LABELS = ROOT / \"outputs\" / \"labeling\" / \"telegram_sample_for_review_20250606_154749.csv\"\n",
    "df = pd.read_csv(LABELS)\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "You are analysing Telegram posts about the Russia-Ukraine war.\n",
    "\n",
    "Return **FOUR integers** (E,B,P,C) with NO other text:\n",
    "\n",
    "E Escalation 0-10\n",
    "      0 humanitarian / diplomacy\n",
    "      1 peace-talks mention\n",
    "      2 political / information warfare\n",
    "      3 routine battlefield update\n",
    "      4 major aid package\n",
    "      5 major weapon systems\n",
    "      6 advanced / long-range systems\n",
    "      7 nationwide strikes\n",
    "      8 strategic-infrastructure attacks\n",
    "      9 nuclear threats\n",
    "     10 explicit nuclear-war rhetoric\n",
    "\n",
    "B Blame\n",
    " â€“1 neutral | 0 blames UA/NATO | 1 blames RU\n",
    "\n",
    "P Propaganda 0 factual Â· 1 mild spin Â· 2 clear propaganda Â· 3 extreme/false\n",
    "\n",
    "C Call-to-action 0 none Â· 1 urges action (donate, enlist, protest, etc.)\n",
    "\n",
    "**Format**: \"E,B,P,C\" (e.g. `7,0,2,1`). Nothing else.\n",
    "\"\"\"\n",
    "\n",
    "def ask_openai(text: str) -> str:\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": RUBRIC},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n",
    "\n",
    "def ask_opus(text: str) -> str:\n",
    "    resp = anthropic_client.messages.create(\n",
    "        model=\"claude-opus-4-20250514\",\n",
    "        max_tokens=10,\n",
    "        temperature=0,\n",
    "        system=RUBRIC,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    )\n",
    "    return resp.content[0].text.strip()\n",
    "\n",
    "def process_row(idx, text):\n",
    "    \"\"\"Process a single row with both models\"\"\"\n",
    "    results = {\"idx\": idx, \"openai\": None, \"opus\": None}\n",
    "    \n",
    "    try:\n",
    "        results[\"openai\"] = ask_openai(text)\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI error on row {idx}: {e}\")\n",
    "        results[\"openai\"] = \"ERROR\"\n",
    "    \n",
    "    try:\n",
    "        results[\"opus\"] = ask_opus(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Opus error on row {idx}: {e}\")\n",
    "        results[\"opus\"] = \"ERROR\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process in parallel with thread pool\n",
    "results_dict = {}\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {\n",
    "        executor.submit(process_row, idx, row.message_text[:2000]): idx \n",
    "        for idx, row in df.iterrows()\n",
    "    }\n",
    "    \n",
    "    # Process completed tasks with progress bar\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n",
    "        result = future.result()\n",
    "        results_dict[result[\"idx\"]] = result\n",
    "\n",
    "# Sort results by index and add to dataframe\n",
    "for idx in sorted(results_dict.keys()):\n",
    "    df.at[idx, \"openai\"] = results_dict[idx][\"openai\"]\n",
    "    df.at[idx, \"opus\"] = results_dict[idx][\"opus\"]\n",
    "\n",
    "# Save results\n",
    "output_path = ROOT / \"outputs\" / \"labeling\" / \"validation_results.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "LAB_DIR = ROOT / \"outputs\" / \"labeling\"\n",
    "INFILE = LAB_DIR / \"validation_results.csv\"\n",
    "\n",
    "df = pd.read_csv(INFILE)\n",
    "\n",
    "# Parse the model outputs (they come as \"E,B,P,C\" strings)\n",
    "def parse_scores(score_str):\n",
    "    \"\"\"Parse 'E,B,P,C' format into individual values\"\"\"\n",
    "    if pd.isna(score_str) or score_str == \"ERROR\":\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        parts = score_str.strip().split(',')\n",
    "        if len(parts) == 4:\n",
    "            return int(parts[0]), int(parts[1]), int(parts[2]), int(parts[3])\n",
    "    except:\n",
    "        pass\n",
    "    return None, None, None, None\n",
    "\n",
    "# Parse OpenAI scores\n",
    "df[['E_openai', 'B_openai', 'P_openai', 'C_openai']] = df['openai'].apply(\n",
    "    lambda x: pd.Series(parse_scores(x))\n",
    ")\n",
    "\n",
    "# Parse Opus scores  \n",
    "df[['E_opus', 'B_opus', 'P_opus', 'C_opus']] = df['opus'].apply(\n",
    "    lambda x: pd.Series(parse_scores(x))\n",
    ")\n",
    "\n",
    "# If you already have claude-sonnet scores from earlier, rename them\n",
    "if 'escalation_score' in df.columns:\n",
    "    df = df.rename(columns={\n",
    "        'escalation_score': 'E_sonnet',\n",
    "        'blame_direction': 'B_sonnet',\n",
    "        'propaganda_level': 'P_sonnet',\n",
    "        'has_cta': 'C_sonnet'\n",
    "    })\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows with valid OpenAI scores: {df['E_openai'].notna().sum()}\")\n",
    "print(f\"Rows with valid Opus scores: {df['E_opus'].notna().sum()}\")\n",
    "if 'E_sonnet' in df.columns:\n",
    "    print(f\"Rows with valid Sonnet scores: {df['E_sonnet'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define consensus functions\n",
    "def majority_vote(values):\n",
    "    \"\"\"Get majority vote from list of values, handling None\"\"\"\n",
    "    valid_vals = [v for v in values if v is not None]\n",
    "    if not valid_vals:\n",
    "        return None\n",
    "    return max(set(valid_vals), key=valid_vals.count)\n",
    "\n",
    "def mean_round(values):\n",
    "    \"\"\"Get mean and round, handling None\"\"\"\n",
    "    valid_vals = [v for v in values if v is not None]\n",
    "    if not valid_vals:\n",
    "        return None\n",
    "    return int(np.round(np.mean(valid_vals)))\n",
    "\n",
    "# Compute consensus for each dimension\n",
    "models = ['openai', 'opus']\n",
    "if 'E_sonnet' in df.columns:\n",
    "    models.append('sonnet')\n",
    "\n",
    "# Escalation: use mean (continuous scale)\n",
    "df['E_consensus'] = df[[f'E_{m}' for m in models]].apply(\n",
    "    lambda row: mean_round(row.values), axis=1\n",
    ")\n",
    "\n",
    "# Binary/categorical: use majority vote\n",
    "for dim in ['B', 'P', 'C']:\n",
    "    df[f'{dim}_consensus'] = df[[f'{dim}_{m}' for m in models]].apply(\n",
    "        lambda row: majority_vote(row.values), axis=1\n",
    "    )\n",
    "\n",
    "# Calculate disagreement metrics\n",
    "def check_disagreement(row):\n",
    "    \"\"\"Check if models disagree significantly\"\"\"\n",
    "    # Escalation: difference > 2 points\n",
    "    e_vals = [row[f'E_{m}'] for m in models if row[f'E_{m}'] is not None]\n",
    "    if len(e_vals) >= 2:\n",
    "        if max(e_vals) - min(e_vals) > 2:\n",
    "            return True\n",
    "    \n",
    "    # Other dimensions: any disagreement\n",
    "    for dim in ['B', 'P', 'C']:\n",
    "        vals = [row[f'{dim}_{m}'] for m in models if row[f'{dim}_{m}'] is not None]\n",
    "        if len(set(vals)) > 1:  # More than one unique value = disagreement\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "df['needs_review'] = df.apply(check_disagreement, axis=1)\n",
    "\n",
    "# Split into auto-accepted and needs-review\n",
    "auto_accepted = df[~df['needs_review'].fillna(True)].copy()\n",
    "needs_review = df[df['needs_review'].fillna(True)].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Results:\")\n",
    "print(f\"âœ… Auto-accepted (high agreement): {len(auto_accepted)} rows\")\n",
    "print(f\"âš ï¸  Needs review (disagreement): {len(needs_review)} rows\")\n",
    "print(f\"ğŸš« Invalid (missing scores): {df['E_consensus'].isna().sum()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze where models disagree most\n",
    "if len(needs_review) > 0:\n",
    "    print(\"\\nğŸ” Disagreement Analysis:\")\n",
    "    \n",
    "    # Escalation disagreements\n",
    "    e_disagree = needs_review[[f'E_{m}' for m in models]].copy()\n",
    "    e_disagree['range'] = e_disagree.max(axis=1) - e_disagree.min(axis=1)\n",
    "    print(f\"\\nEscalation score ranges:\")\n",
    "    print(e_disagree['range'].value_counts().sort_index())\n",
    "    \n",
    "    # Show examples of high disagreement\n",
    "    high_disagree = needs_review[needs_review.apply(\n",
    "        lambda r: max([r[f'E_{m}'] for m in models if r[f'E_{m}'] is not None] or [0]) - \n",
    "                  min([r[f'E_{m}'] for m in models if r[f'E_{m}'] is not None] or [0]) > 3, \n",
    "        axis=1\n",
    "    )]\n",
    "    \n",
    "    if len(high_disagree) > 0:\n",
    "        print(f\"\\nğŸ“ Example of high disagreement (showing first):\")\n",
    "        row = high_disagree.iloc[0]\n",
    "        print(f\"Text: {row['message_text'][:200]}...\")\n",
    "        print(f\"Scores: OpenAI={row['E_openai']}, Opus={row['E_opus']}\", end=\"\")\n",
    "        if 'E_sonnet' in df.columns:\n",
    "            print(f\", Sonnet={row['E_sonnet']}\")\n",
    "        else:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Model Agreement Analysis Across All Dimensions', fontsize=16)\n",
    "\n",
    "# 1. Escalation Score Comparisons (continuous)\n",
    "models = ['openai', 'opus', 'sonnet'] if 'E_sonnet' in df.columns else ['openai', 'opus']\n",
    "model_pairs = [(models[i], models[j]) for i in range(len(models)) for j in range(i+1, len(models))]\n",
    "\n",
    "for idx, (m1, m2) in enumerate(model_pairs[:3]):  # Max 3 pairs\n",
    "    ax = axes[0, idx]\n",
    "    \n",
    "    # Filter valid scores\n",
    "    valid_mask = df[f'E_{m1}'].notna() & df[f'E_{m2}'].notna()\n",
    "    x = df.loc[valid_mask, f'E_{m1}']\n",
    "    y = df.loc[valid_mask, f'E_{m2}']\n",
    "    \n",
    "    # Scatter plot with jitter\n",
    "    ax.scatter(x + np.random.normal(0, 0.1, len(x)), \n",
    "               y + np.random.normal(0, 0.1, len(y)), \n",
    "               alpha=0.5)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    ax.plot([0, 10], [0, 10], 'r--', alpha=0.5)\n",
    "    \n",
    "    # Calculate correlation\n",
    "    corr, _ = pearsonr(x, y)\n",
    "    \n",
    "    ax.set_xlabel(f'{m1.capitalize()} Escalation')\n",
    "    ax.set_ylabel(f'{m2.capitalize()} Escalation')\n",
    "    ax.set_title(f'{m1.capitalize()} vs {m2.capitalize()}\\n(r = {corr:.3f})')\n",
    "    ax.set_xlim(-0.5, 10.5)\n",
    "    ax.set_ylim(-0.5, 10.5)\n",
    "\n",
    "# 2. Agreement Heatmaps for Binary/Categorical\n",
    "dims = ['B', 'P', 'C']\n",
    "dim_names = {'B': 'Blame', 'P': 'Propaganda', 'C': 'Call-to-Action'}\n",
    "\n",
    "for idx, dim in enumerate(dims):\n",
    "    ax = axes[1, idx]\n",
    "    \n",
    "    # Create agreement matrix\n",
    "    agreement_matrix = np.zeros((len(models), len(models)))\n",
    "    \n",
    "    for i, m1 in enumerate(models):\n",
    "        for j, m2 in enumerate(models):\n",
    "            if i != j:\n",
    "                valid_mask = df[f'{dim}_{m1}'].notna() & df[f'{dim}_{m2}'].notna()\n",
    "                if valid_mask.sum() > 0:\n",
    "                    agreement = (df.loc[valid_mask, f'{dim}_{m1}'] == \n",
    "                               df.loc[valid_mask, f'{dim}_{m2}']).mean() * 100\n",
    "                    agreement_matrix[i, j] = agreement\n",
    "            else:\n",
    "                agreement_matrix[i, j] = 100\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(agreement_matrix, \n",
    "                annot=True, \n",
    "                fmt='.1f',\n",
    "                cmap='RdYlGn',\n",
    "                vmin=0, vmax=100,\n",
    "                xticklabels=[m.capitalize() for m in models],\n",
    "                yticklabels=[m.capitalize() for m in models],\n",
    "                ax=ax,\n",
    "                cbar_kws={'label': '% Agreement'})\n",
    "    \n",
    "    ax.set_title(f'{dim_names[dim]} Agreement %')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "LAB_DIR = ROOT / \"outputs\" / \"labeling\"\n",
    "INFILE = LAB_DIR / \"validation_results.csv\"\n",
    "\n",
    "df = pd.read_csv(INFILE)\n",
    "\n",
    "# Parse scores if not already done\n",
    "def parse_scores(score_str):\n",
    "    if pd.isna(score_str) or score_str == \"ERROR\":\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        parts = score_str.strip().split(',')\n",
    "        if len(parts) == 4:\n",
    "            return int(parts[0]), int(parts[1]), int(parts[2]), int(parts[3])\n",
    "    except:\n",
    "        pass\n",
    "    return None, None, None, None\n",
    "\n",
    "# Parse if needed\n",
    "if 'E_opus' not in df.columns or df['E_opus'].isna().all():\n",
    "    df[['E_opus', 'B_opus', 'P_opus', 'C_opus']] = df['opus'].apply(\n",
    "        lambda x: pd.Series(parse_scores(x))\n",
    "    )\n",
    "\n",
    "# Rename sonnet columns if they exist\n",
    "if 'escalation_score' in df.columns:\n",
    "    df = df.rename(columns={\n",
    "        'escalation_score': 'E_sonnet',\n",
    "        'blame_direction': 'B_sonnet', \n",
    "        'propaganda_level': 'P_sonnet',\n",
    "        'has_cta': 'C_sonnet'\n",
    "    })\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Rows with valid Opus scores: {df['E_opus'].notna().sum()}\")\n",
    "print(f\"Rows with valid Sonnet scores: {df['E_sonnet'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate consensus using only Opus and Sonnet\n",
    "df['E_consensus'] = df[['E_opus', 'E_sonnet']].mean(axis=1).round().astype('Int64')\n",
    "\n",
    "# For categorical, use Opus as primary with Sonnet as tiebreaker\n",
    "for dim in ['B', 'P', 'C']:\n",
    "    # If they agree, use that value; if not, use Opus (it had slightly better performance)\n",
    "    df[f'{dim}_consensus'] = df.apply(\n",
    "        lambda r: r[f'{dim}_opus'] if pd.notna(r[f'{dim}_opus']) else r[f'{dim}_sonnet'], \n",
    "        axis=1\n",
    "    ).astype('Int64')\n",
    "\n",
    "# Calculate agreement metrics\n",
    "df['E_diff'] = abs(df['E_opus'] - df['E_sonnet'])\n",
    "df['perfect_agreement'] = (\n",
    "    (df['E_diff'] <= 1) & \n",
    "    (df['B_opus'] == df['B_sonnet']) & \n",
    "    (df['P_opus'] == df['P_sonnet']) & \n",
    "    (df['C_opus'] == df['C_sonnet'])\n",
    ")\n",
    "\n",
    "# Flag rows that need review (using relaxed criteria)\n",
    "df['needs_review'] = (\n",
    "    (df['E_diff'] > 3) |  # Large escalation disagreement\n",
    "    ((df['B_opus'] != df['B_sonnet']) & df['B_opus'].notna() & df['B_sonnet'].notna()) |\n",
    "    ((df['P_opus'] != df['P_sonnet']) & df['P_opus'].notna() & df['P_sonnet'].notna())\n",
    "    # Note: Being lenient on C (call-to-action) since they agreed 91% of the time\n",
    ")\n",
    "\n",
    "# Remove rows with missing consensus values\n",
    "valid_mask = df[['E_consensus', 'B_consensus', 'P_consensus', 'C_consensus']].notna().all(axis=1)\n",
    "df_valid = df[valid_mask].copy()\n",
    "\n",
    "print(f\"\\nğŸ“Š Opus-Sonnet Agreement Analysis:\")\n",
    "print(f\"Perfect agreement: {df_valid['perfect_agreement'].sum()} ({df_valid['perfect_agreement'].sum()/len(df_valid)*100:.1f}%)\")\n",
    "print(f\"Escalation within 1 point: {(df_valid['E_diff'] <= 1).sum()} ({(df_valid['E_diff'] <= 1).sum()/len(df_valid)*100:.1f}%)\")\n",
    "print(f\"Needs review: {df_valid['needs_review'].sum()} ({df_valid['needs_review'].sum()/len(df_valid)*100:.1f}%)\")\n",
    "print(f\"Auto-accepted: {(~df_valid['needs_review']).sum()} ({(~df_valid['needs_review']).sum()/len(df_valid)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and setup (same as before)\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "LAB_DIR = ROOT / \"outputs\" / \"labeling\"\n",
    "output_dir = ROOT / \"outputs\" / \"fine_tuning\"\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "INFILE = LAB_DIR / \"validation_results.csv\"\n",
    "df = pd.read_csv(INFILE)\n",
    "\n",
    "# Parse scores (if needed)\n",
    "def parse_scores(score_str):\n",
    "    if pd.isna(score_str) or score_str == \"ERROR\":\n",
    "        return None, None, None, None\n",
    "    try:\n",
    "        parts = score_str.strip().split(',')\n",
    "        if len(parts) == 4:\n",
    "            return int(parts[0]), int(parts[1]), int(parts[2]), int(parts[3])\n",
    "    except:\n",
    "        pass\n",
    "    return None, None, None, None\n",
    "\n",
    "if 'E_opus' not in df.columns or df['E_opus'].isna().all():\n",
    "    df[['E_opus', 'B_opus', 'P_opus', 'C_opus']] = df['opus'].apply(\n",
    "        lambda x: pd.Series(parse_scores(x))\n",
    "    )\n",
    "\n",
    "if 'escalation_score' in df.columns:\n",
    "    df = df.rename(columns={\n",
    "        'escalation_score': 'E_sonnet',\n",
    "        'blame_direction': 'B_sonnet',\n",
    "        'propaganda_level': 'P_sonnet',\n",
    "        'has_cta': 'C_sonnet'\n",
    "    })\n",
    "\n",
    "# SIMPLE FILTER: Keep only rows where Opus and Sonnet escalation scores are within 1 point\n",
    "df['E_diff'] = abs(df['E_opus'] - df['E_sonnet'])\n",
    "high_agreement = df[df['E_diff'] <= 1].copy()\n",
    "\n",
    "# Calculate consensus (simple average for E, use Opus for others since it performed well)\n",
    "high_agreement['E_consensus'] = high_agreement[['E_opus', 'E_sonnet']].mean(axis=1).round().astype(int)\n",
    "high_agreement['B_consensus'] = high_agreement['B_opus'].astype(int)\n",
    "high_agreement['P_consensus'] = high_agreement['P_opus'].astype(int)\n",
    "high_agreement['C_consensus'] = high_agreement['C_opus'].astype(int)\n",
    "\n",
    "# Remove any with missing values\n",
    "final_df = high_agreement.dropna(subset=['message_text', 'E_consensus', 'B_consensus', 'P_consensus', 'C_consensus'])\n",
    "\n",
    "print(f\"âœ… Using {len(final_df)} high-agreement samples (from {len(df)} total)\")\n",
    "print(f\"\\nEscalation distribution:\")\n",
    "print(final_df['E_consensus'].value_counts().sort_index())\n",
    "\n",
    "# Create training examples\n",
    "SYSTEM_PROMPT = \"\"\"You are analyzing Telegram posts about the Russia-Ukraine war. Return FOUR integers (E,B,P,C) with NO other text:\n",
    "\n",
    "E = Escalation (0-10): 0=humanitarian, 5=major weapons, 10=nuclear rhetoric\n",
    "B = Blame (-1/0/1): -1=neutral, 0=blames Ukraine/NATO, 1=blames Russia  \n",
    "P = Propaganda (0-3): 0=factual, 3=extreme/false\n",
    "C = Call-to-action (0/1): 0=none, 1=urges action\n",
    "\n",
    "Format: \"E,B,P,C\" (e.g. \"7,1,2,0\")\"\"\"\n",
    "\n",
    "def create_training_example(row):\n",
    "    response = f\"{int(row['E_consensus'])},{int(row['B_consensus'])},{int(row['P_consensus'])},{int(row['C_consensus'])}\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": str(row['message_text'])},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Split 80/20\n",
    "train_df, val_df = train_test_split(\n",
    "    final_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=final_df['E_consensus'].clip(upper=7)\n",
    ")\n",
    "\n",
    "# Write files\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "train_file = output_dir / \"telegram_train.jsonl\"\n",
    "val_file = output_dir / \"telegram_val.jsonl\"\n",
    "\n",
    "with open(train_file, 'w', encoding='utf-8') as f:\n",
    "    for _, row in train_df.iterrows():\n",
    "        f.write(json.dumps(create_training_example(row), ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open(val_file, 'w', encoding='utf-8') as f:\n",
    "    for _, row in val_df.iterrows():\n",
    "        f.write(json.dumps(create_training_example(row), ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\\nğŸ¯ Files ready:\")\n",
    "print(f\"Training: {train_file} ({len(train_df)} examples)\")\n",
    "print(f\"Validation: {val_file} ({len(val_df)} examples)\")\n",
    "print(\"\\nâœ… Upload these to OpenAI and start fine-tuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNED MODEL EXPLORATION\n",
    "\"\"\"\n",
    "This notebook explores your 3 fine-tuned models to understand:\n",
    "1. What input format they expect\n",
    "2. How they respond\n",
    "3. Their consistency and accuracy\n",
    "4. Speed and cost comparisons\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize\n",
    "client = OpenAI()\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "\n",
    "# Your fine-tuned models\n",
    "FT_MODELS = {\n",
    "    \"mini\": {\n",
    "        \"id\": \"ft:gpt-4o-mini-2024-07-18:politics-ai-research:ukraine-telegram-mini:BfSq29k1\",\n",
    "        \"base\": \"gpt-4o-mini\",\n",
    "        \"train_loss\": 0.033,\n",
    "        \"valid_loss\": 0.149\n",
    "    },\n",
    "    \"nano\": {\n",
    "        \"id\": \"ft:gpt-4.1-nano-2025-04-14:politics-ai-research:ukraine-classifier-nano:BfSlvv7Q\", \n",
    "        \"base\": \"gpt-4.1-nano\",\n",
    "        \"train_loss\": 0.139,\n",
    "        \"valid_loss\": 0.190\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"id\": \"ft:gpt-4.1-2025-04-14:politics-ai-research:ukraine-classifier:BfStxtYw\",\n",
    "        \"base\": \"gpt-4.1\",\n",
    "        \"train_loss\": 1.377,  # High train loss - might be undertrained?\n",
    "        \"valid_loss\": 0.029   # But low valid loss - interesting!\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ Fine-Tuned Models Loaded:\")\n",
    "for name, info in FT_MODELS.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Model: {info['id']}\")\n",
    "    print(f\"  Base: {info['base']}\")\n",
    "    print(f\"  Train/Valid Loss: {info['train_loss']:.3f} / {info['valid_loss']:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Test 1: Basic Functionality - What format do they expect?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: INPUT FORMAT DISCOVERY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different input formats\n",
    "test_messages = [\n",
    "    \"Russian forces strike Kyiv infrastructure\",  # Simple text\n",
    "    \"[Channel: test] Russian forces strike Kyiv\",  # With channel\n",
    "    \"M1: Russian forces strike Kyiv\",  # With index\n",
    "    \"Score this: Russian forces strike Kyiv\",  # With instruction\n",
    "]\n",
    "\n",
    "test_prompts = [\n",
    "    None,  # No system prompt\n",
    "    \"Score the message\",  # Simple instruction\n",
    "    \"Return E,B,P,C scores\",  # Specific format\n",
    "    \"You are a war message classifier. Score: E(0-10),B(-1/0/1),P(0-3),C(0/1)\"  # Full prompt\n",
    "]\n",
    "\n",
    "def test_model_response(model_id, user_msg, system_msg=None):\n",
    "    \"\"\"Test how model responds to different inputs\"\"\"\n",
    "    messages = []\n",
    "    if system_msg:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=messages,\n",
    "            max_tokens=50,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test each model with different formats\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\n\\nğŸ” Testing {model_name.upper()} Model:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for msg in test_messages[:2]:  # Test first 2 message formats\n",
    "        for prompt in test_prompts[:2]:  # Test first 2 prompt types\n",
    "            result = test_model_response(model_info['id'], msg, prompt)\n",
    "            print(f\"\\nInput: '{msg}'\")\n",
    "            if prompt:\n",
    "                print(f\"System: '{prompt}'\")\n",
    "            print(f\"Output: {result}\")\n",
    "            \n",
    "            # Parse if it looks like scores\n",
    "            if ',' in result and len(result.split(',')) == 4:\n",
    "                try:\n",
    "                    scores = [x.strip() for x in result.split(',')]\n",
    "                    print(f\"Parsed: E={scores[0]}, B={scores[1]}, P={scores[2]}, C={scores[3]}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# %%\n",
    "# Test 2: Consistency Check - Same message, multiple runs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: CONSISTENCY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_message = \"NATO announces new military aid package for Ukraine worth $2 billion\"\n",
    "runs_per_model = 5\n",
    "\n",
    "consistency_results = {}\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nğŸ”„ Testing {model_name.upper()} consistency ({runs_per_model} runs):\")\n",
    "    \n",
    "    results = []\n",
    "    for i in range(runs_per_model):\n",
    "        response = test_model_response(model_info['id'], test_message)\n",
    "        results.append(response)\n",
    "        print(f\"  Run {i+1}: {response}\")\n",
    "    \n",
    "    # Check if all results are identical\n",
    "    unique_results = set(results)\n",
    "    consistency_results[model_name] = {\n",
    "        'results': results,\n",
    "        'unique': len(unique_results),\n",
    "        'consistent': len(unique_results) == 1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Consistency: {'âœ… PERFECT' if len(unique_results) == 1 else f'âš ï¸  {len(unique_results)} different outputs'}\")\n",
    "\n",
    "# %%\n",
    "# Test 3: Speed Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 3: SPEED & PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load sample messages\n",
    "df = pd.read_csv(ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\", nrows=100)\n",
    "test_samples = df[df['message_text'].notna()]['message_text'].tolist()[:10]\n",
    "\n",
    "speed_results = {}\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nâš¡ Testing {model_name.upper()} speed:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    responses = []\n",
    "    \n",
    "    for msg in test_samples:\n",
    "        response = test_model_response(model_info['id'], msg[:200])\n",
    "        responses.append(response)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    speed_results[model_name] = {\n",
    "        'total_time': elapsed,\n",
    "        'avg_time': elapsed / len(test_samples),\n",
    "        'responses': responses\n",
    "    }\n",
    "    \n",
    "    print(f\"  Total time: {elapsed:.2f}s\")\n",
    "    print(f\"  Avg per message: {elapsed/len(test_samples)*1000:.0f}ms\")\n",
    "    print(f\"  Throughput: {len(test_samples)/elapsed:.1f} messages/second\")\n",
    "\n",
    "# %%\n",
    "# Test 4: Batch Processing Capability\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 4: BATCH PROCESSING TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try sending multiple messages at once\n",
    "batch_formats = [\n",
    "    # Format 1: Newline separated\n",
    "    \"Message 1: Russia attacks Kyiv\\nMessage 2: Peace talks resume\\nMessage 3: NATO sends aid\",\n",
    "    \n",
    "    # Format 2: Numbered\n",
    "    \"1. Russia attacks Kyiv\\n2. Peace talks resume\\n3. NATO sends aid\",\n",
    "    \n",
    "    # Format 3: Indexed\n",
    "    \"M0: Russia attacks Kyiv\\nM1: Peace talks resume\\nM2: NATO sends aid\",\n",
    "]\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nğŸ“¦ Testing {model_name.upper()} batch capability:\")\n",
    "    \n",
    "    for i, batch in enumerate(batch_formats):\n",
    "        response = test_model_response(model_info['id'], batch)\n",
    "        print(f\"\\nFormat {i+1} response: {response}\")\n",
    "        \n",
    "        # Check if it returned multiple scores\n",
    "        if '|' in response or '\\n' in response or response.count(',') > 4:\n",
    "            print(\"  âœ… Appears to handle batches!\")\n",
    "        else:\n",
    "            print(\"  âŒ Single response only\")\n",
    "\n",
    "# %%\n",
    "# Test 5: Edge Cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 5: EDGE CASES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "edge_cases = [\n",
    "    \"\",  # Empty\n",
    "    \"Ğ¿Ñ€Ğ¸Ğ²ĞµÑ‚\",  # Non-English\n",
    "    \"ğŸš€ğŸ’¥\",  # Emojis only\n",
    "    \"a\" * 500,  # Very long\n",
    "    \"NATO NATO NATO NATO\",  # Repetitive\n",
    "    \"2+2=4\",  # Non-war content\n",
    "]\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nğŸ”§ Testing {model_name.upper()} edge cases:\")\n",
    "    \n",
    "    for case in edge_cases[:3]:  # Test first 3\n",
    "        response = test_model_response(model_info['id'], case)\n",
    "        print(f\"  '{case[:20]}...' â†’ {response}\")\n",
    "\n",
    "# %%\n",
    "# Visualization of Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Model Loss Comparison\n",
    "ax = axes[0, 0]\n",
    "models = list(FT_MODELS.keys())\n",
    "train_losses = [FT_MODELS[m]['train_loss'] for m in models]\n",
    "valid_losses = [FT_MODELS[m]['valid_loss'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, train_losses, width, label='Train Loss', alpha=0.8)\n",
    "ax.bar(x + width/2, valid_losses, width, label='Valid Loss', alpha=0.8)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training vs Validation Loss')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "# 2. Speed Comparison\n",
    "if speed_results:\n",
    "    ax = axes[0, 1]\n",
    "    speeds = [speed_results[m]['avg_time'] * 1000 for m in models]\n",
    "    ax.bar(models, speeds)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Avg Response Time (ms)')\n",
    "    ax.set_title('Response Speed Comparison')\n",
    "\n",
    "# 3. Consistency Results\n",
    "if consistency_results:\n",
    "    ax = axes[1, 0]\n",
    "    consistency = [consistency_results[m]['unique'] for m in models]\n",
    "    colors = ['green' if c == 1 else 'orange' for c in consistency]\n",
    "    ax.bar(models, consistency, color=colors)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Number of Unique Outputs')\n",
    "    ax.set_title('Consistency Test (5 runs, same input)')\n",
    "    ax.axhline(y=1, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Summary Table\n",
    "ax = axes[1, 1]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "summary_data = []\n",
    "for model in models:\n",
    "    summary_data.append([\n",
    "        model.upper(),\n",
    "        f\"{FT_MODELS[model]['base']}\",\n",
    "        f\"{FT_MODELS[model]['train_loss']:.3f}\",\n",
    "        f\"{FT_MODELS[model]['valid_loss']:.3f}\",\n",
    "        f\"{speed_results.get(model, {}).get('avg_time', 0)*1000:.0f}ms\" if speed_results else \"N/A\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=summary_data,\n",
    "    colLabels=['Model', 'Base', 'Train Loss', 'Valid Loss', 'Avg Speed'],\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "plt.suptitle('Fine-Tuned Model Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Final Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. INPUT FORMAT:\")\n",
    "print(\"   Your models likely expect just the raw message text\")\n",
    "print(\"   No system prompt needed (it's baked into the fine-tuning)\")\n",
    "\n",
    "print(\"\\n2. OUTPUT FORMAT:\")\n",
    "print(\"   Models should return: E,B,P,C (4 comma-separated integers)\")\n",
    "\n",
    "print(\"\\n3. BEST MODEL:\")\n",
    "# Analyze which performed best\n",
    "if consistency_results:\n",
    "    consistent_models = [m for m in models if consistency_results[m]['consistent']]\n",
    "    print(f\"   Most consistent: {', '.join(consistent_models) if consistent_models else 'None perfectly consistent'}\")\n",
    "\n",
    "if speed_results:\n",
    "    fastest = min(models, key=lambda m: speed_results[m]['avg_time'])\n",
    "    print(f\"   Fastest: {fastest.upper()} ({speed_results[fastest]['avg_time']*1000:.0f}ms/msg)\")\n",
    "\n",
    "print(f\"\\n4. CONCERNING OBSERVATIONS:\")\n",
    "if FT_MODELS['full']['train_loss'] > 1.0:\n",
    "    print(\"   - 'full' model has high train loss (1.377) - might need more training\")\n",
    "print(\"   - Test batch processing capability before full-scale run\")\n",
    "print(\"   - Consider the base model costs (gpt-4.1 is most expensive)\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Run full scoring with best performing model\")\n",
    "print(\"   - Use batch processing if supported\")\n",
    "print(\"   - Monitor for consistency issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
