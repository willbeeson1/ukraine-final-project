{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Strategy: Run 4 notebooks in parallel, each processing ALL 174k messages\n",
    "# with a different model. Uses batching to maximize throughput.\n",
    "# \"\"\"\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from openai import OpenAI\n",
    "# import asyncio\n",
    "# import aiohttp\n",
    "# from pathlib import Path\n",
    "# import json\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# from tqdm.notebook import tqdm\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# # Configuration\n",
    "# ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "# TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "# OUT_DIR = ROOT / \"outputs\" / \"telegram_scoring_parallel\"\n",
    "# OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# # SELECT YOUR MODEL FOR THIS NOTEBOOK\n",
    "# # Change this for each notebook you run in parallel\n",
    "# MODEL_NAME = \"chatgpt-4o-latest\"  # Options: \"gpt-4o-mini\", \"o3-mini\", \"gpt-4.1-mini\", \"gpt-4.1-nano\"\n",
    "\n",
    "# # Model configurations with batching\n",
    "# MODEL_CONFIGS = {\n",
    "#     \"gpt-4o-mini\": {\n",
    "#         \"rpm\": 10000,\n",
    "#         \"tpm\": 200000,\n",
    "#         \"batch_size\": 5,  # Messages per request\n",
    "#         \"concurrent\": 8000,  # Concurrent requests\n",
    "#         \"max_retries\": 3\n",
    "#     },\n",
    "#     \"o3-mini\": {\n",
    "#         \"rpm\": 500,\n",
    "#         \"tpm\": 200000,\n",
    "#         \"batch_size\": 25,\n",
    "#         \"concurrent\": 20,\n",
    "#         \"max_retries\": 3\n",
    "#     },\n",
    "#     \"gpt-4.1-mini\": {\n",
    "#         \"rpm\": 500,\n",
    "#         \"tpm\": 200000,\n",
    "#         \"batch_size\": 25,\n",
    "#         \"concurrent\": 20,\n",
    "#         \"max_retries\": 3\n",
    "#     },\n",
    "#     \"gpt-4.1-nano\": {\n",
    "#         \"rpm\": 500,\n",
    "#         \"tpm\": 400000,  # Higher token limit\n",
    "#         \"batch_size\": 30,  # Can handle more\n",
    "#         \"concurrent\": 20,\n",
    "#         \"max_retries\": 3\n",
    "#     }, \n",
    "#     \"chatgpt-4o-latest\": {\n",
    "#         \"rpm\": 200,\n",
    "#         \"tpm\": 500000,  # 2.5x more tokens!\n",
    "#         \"batch_size\": 10,  # Can batch more\n",
    "#         \"concurrent\": 150,  # Near RPM limit\n",
    "#         \"max_retries\": 3\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Get config for selected model\n",
    "# CONFIG = MODEL_CONFIGS[MODEL_NAME]\n",
    "\n",
    "# print(f\"üöÄ Model Comparison Setup: {MODEL_NAME}\")\n",
    "# print(f\"üìä Will process ALL messages with this model\")\n",
    "# print(f\"‚ö° Batch size: {CONFIG['batch_size']} messages per request\")\n",
    "# print(f\"üîÑ Concurrent requests: {CONFIG['concurrent']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 2: Load Data\n",
    "# print(\"\\nüìä Loading Telegram data...\")\n",
    "# df = pd.read_csv(TELEGRAM_CSV)\n",
    "# df = df[df['message_text'].notna()].copy()\n",
    "# total_messages = len(df)\n",
    "# print(f\"‚úÖ Loaded {total_messages:,} messages\")\n",
    "\n",
    "# # Calculate batches\n",
    "# total_batches = (total_messages + CONFIG['batch_size'] - 1) // CONFIG['batch_size']\n",
    "# print(f\"üì¶ Total batches: {total_batches:,}\")\n",
    "# print(f\"‚è±Ô∏è  Estimated time: {total_batches / CONFIG['rpm']:.1f} minutes (if rate limited)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_PROMPT = \"\"\"War message scorer. Output only: M#:E,B,P,C|...\n",
    "# E(0-10): 0=humanitarian 3=combat 5=major-weapons 7=nationwide-strikes 9=nuclear-threats\n",
    "# B(-1/0/1): -1=neutral 0=blames-Ukraine/NATO 1=blames-Russia\n",
    "# P(0-3): 0=factual 1=spin 2=propaganda 3=extreme-disinfo\n",
    "# C(0/1): 0=no-action 1=calls-to-action\n",
    "# Messages:\"\"\"\n",
    "\n",
    "# def create_message_batches(df, batch_size):\n",
    "#     \"\"\"Create batches of messages for processing\"\"\"\n",
    "#     batches = []\n",
    "    \n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         batch_df = df.iloc[i:i+batch_size]\n",
    "#         batch_messages = []\n",
    "        \n",
    "#         for idx, row in batch_df.iterrows():\n",
    "#             text = str(row['message_text'])[:200].replace('\\n', ' ')\n",
    "#             msg = f\"M{idx}:[{row['channel_username']}] {text}\"\n",
    "#             batch_messages.append((idx, msg))\n",
    "        \n",
    "#         batches.append(batch_messages)\n",
    "    \n",
    "#     return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, rpm):\n",
    "        # NO LIMIT - let OpenAI handle rate limiting\n",
    "        pass\n",
    "    \n",
    "    async def acquire(self):\n",
    "        # Do nothing - just return immediately\n",
    "        pass\n",
    "\n",
    "async def process_batch_async(session, batch_messages, model_name, rate_limiter, retry_count=0):\n",
    "    \"\"\"Process a batch of messages\"\"\"\n",
    "    await rate_limiter.acquire()\n",
    "    \n",
    "    # Create batch prompt\n",
    "    messages_text = \"\\n\".join([msg for _, msg in batch_messages])\n",
    "    \n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": BATCH_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": messages_text}\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": len(batch_messages) * 10  # ~10 tokens per message\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        async with session.post(url, json=payload, headers=headers, timeout=30) as response:\n",
    "            if response.status == 200:\n",
    "                result = await response.json()\n",
    "                content = result['choices'][0]['message']['content']\n",
    "                \n",
    "                # Parse batch results\n",
    "                results = {}\n",
    "                parts = content.split('|')\n",
    "                \n",
    "                for part in parts:\n",
    "                    if ':' in part:\n",
    "                        try:\n",
    "                            msg_id, scores = part.split(':')\n",
    "                            msg_idx = int(msg_id.replace('M', '').strip())\n",
    "                            score_values = [int(x.strip()) for x in scores.split(',')]\n",
    "                            \n",
    "                            if len(score_values) == 4:\n",
    "                                results[msg_idx] = {\n",
    "                                    'escalation_score': score_values[0],\n",
    "                                    'blame_direction': score_values[1],\n",
    "                                    'propaganda_level': score_values[2],\n",
    "                                    'has_cta': score_values[3]\n",
    "                                }\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                return results\n",
    "            \n",
    "            elif response.status == 429:  # Rate limit\n",
    "                if retry_count < CONFIG['max_retries']:\n",
    "                    await asyncio.sleep(2 ** retry_count)\n",
    "                    return await process_batch_async(session, batch_messages, model_name, rate_limiter, retry_count + 1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        if retry_count < CONFIG['max_retries']:\n",
    "            await asyncio.sleep(2 ** retry_count)\n",
    "            return await process_batch_async(session, batch_messages, model_name, rate_limiter, retry_count + 1)\n",
    "    \n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_all_messages(df, model_name):\n",
    "    \"\"\"Process all messages with real-time progress updates\"\"\"\n",
    "    print(f\"\\nüöÄ Starting processing with {model_name}\")\n",
    "    \n",
    "    # Create batches\n",
    "    batches = create_message_batches(df, CONFIG['batch_size'])\n",
    "    print(f\"üì¶ Created {len(batches):,} batches\")\n",
    "    print(f\"‚ö° Max concurrent: {CONFIG['concurrent']} (could use up to 8000!)\")\n",
    "    \n",
    "    # Initialize\n",
    "    rate_limiter = RateLimiter(CONFIG['rpm'])\n",
    "    all_results = {}\n",
    "    failed_batches = 0\n",
    "    completed_batches = 0\n",
    "    messages_processed = 0\n",
    "    \n",
    "    # Process with massive concurrency\n",
    "    connector = aiohttp.TCPConnector(limit=0, force_close=True)\n",
    "    timeout = aiohttp.ClientTimeout(total=30)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "        # Create ALL tasks at once\n",
    "        tasks = []\n",
    "        batch_to_size = {}  # Track batch sizes\n",
    "        \n",
    "        for i, batch_messages in enumerate(batches):\n",
    "            task = asyncio.create_task(\n",
    "                process_batch_async(session, batch_messages, model_name, rate_limiter)\n",
    "            )\n",
    "            tasks.append(task)\n",
    "            batch_to_size[i] = len(batch_messages)\n",
    "        \n",
    "        # Process with REAL-TIME updates\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with tqdm(total=len(batches), desc=f\"Batches\", position=0) as batch_pbar:\n",
    "            with tqdm(total=total_messages, desc=f\"Messages\", position=1) as msg_pbar:\n",
    "                # Process as they complete (not in order)\n",
    "                for i, future in enumerate(asyncio.as_completed(tasks)):\n",
    "                    try:\n",
    "                        result = await future\n",
    "                        if isinstance(result, dict) and result:\n",
    "                            all_results.update(result)\n",
    "                            messages_processed += len(result)\n",
    "                            msg_pbar.update(len(result))\n",
    "                        else:\n",
    "                            failed_batches += 1\n",
    "                    except Exception as e:\n",
    "                        failed_batches += 1\n",
    "                    \n",
    "                    completed_batches += 1\n",
    "                    batch_pbar.update(1)\n",
    "                    \n",
    "                    # Update stats every 100 batches\n",
    "                    if completed_batches % 100 == 0:\n",
    "                        elapsed = time.time() - start_time\n",
    "                        rate = completed_batches / elapsed\n",
    "                        msg_rate = messages_processed / elapsed\n",
    "                        \n",
    "                        batch_pbar.set_postfix({\n",
    "                            'rate': f'{rate:.1f} batch/s',\n",
    "                            'failed': failed_batches\n",
    "                        })\n",
    "                        msg_pbar.set_postfix({\n",
    "                            'rate': f'{msg_rate:.0f} msg/s',\n",
    "                            'success': f'{messages_processed/((completed_batches)*CONFIG[\"batch_size\"])*100:.1f}%'\n",
    "                        })\n",
    "    \n",
    "    print(f\"\\n‚úÖ Completed! Processed {len(all_results):,} messages\")\n",
    "    print(f\"‚ùå Failed batches: {failed_batches:,} ({failed_batches/len(batches)*100:.1f}%)\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your actual rate limit\n",
    "import openai\n",
    "client = OpenAI()\n",
    "try:\n",
    "    # Make a test request\n",
    "    response = client.chat.completions.with_raw_response.create(\n",
    "        model=\"chatgpt-4o-latest\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "        max_tokens=1\n",
    "    )\n",
    "    # Check rate limit headers\n",
    "    print(\"Rate limit headers:\")\n",
    "    print(f\"RPM Limit: {response.headers.get('x-ratelimit-limit-requests')}\")\n",
    "    print(f\"TPM Limit: {response.headers.get('x-ratelimit-limit-tokens')}\")\n",
    "    print(f\"Remaining: {response.headers.get('x-ratelimit-remaining-requests')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to see the actual error\n",
    "async def debug_test():\n",
    "    url = \"https://api.openai.com/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\n",
    "    \n",
    "    async def make_request(session):\n",
    "        payload = {\n",
    "            \"model\": \"chatgpt-4o-latest\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"test\"}],\n",
    "            \"max_tokens\": 1\n",
    "        }\n",
    "        try:\n",
    "            async with session.post(url, json=payload, headers=headers) as resp:\n",
    "                if resp.status != 200:\n",
    "                    error_text = await resp.text()\n",
    "                    return f\"Status {resp.status}: {error_text}\"\n",
    "                return \"Success\"\n",
    "        except Exception as e:\n",
    "            return f\"Exception: {str(e)}\"\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        result = await make_request(session)\n",
    "        print(f\"Result: {result}\")\n",
    "\n",
    "# Run it\n",
    "await debug_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ü§ñ PROCESSING ALL MESSAGES WITH: {MODEL_NAME}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Run async processing\n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(process_all_messages(df, MODEL_NAME))\n",
    "\n",
    "# Update dataframe with results\n",
    "for idx, scores in results.items():\n",
    "    if idx in df.index:\n",
    "        for col, value in scores.items():\n",
    "            df.at[idx, col] = value\n",
    "\n",
    "# Calculate statistics\n",
    "elapsed_time = (time.time() - start_time) / 60\n",
    "n_scored = df['escalation_score'].notna().sum()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ PROCESSING COMPLETE FOR {MODEL_NAME}!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"‚è±Ô∏è  Time elapsed: {elapsed_time:.1f} minutes\")\n",
    "print(f\"üìä Messages scored: {n_scored:,} / {total_messages:,}\")\n",
    "print(f\"üéØ Success rate: {n_scored/total_messages*100:.1f}%\")\n",
    "print(f\"‚ö° Processing rate: {n_scored/elapsed_time:.0f} messages/minute\")\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = OUT_DIR / f\"telegram_scored_{MODEL_NAME}_{timestamp}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nüìÅ Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE-TUNED MODEL EXPLORATION\n",
    "\"\"\"\n",
    "This notebook explores your 3 fine-tuned models to understand:\n",
    "1. What input format they expect\n",
    "2. How they respond\n",
    "3. Their consistency and accuracy\n",
    "4. Speed and cost comparisons\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    organization=\"org-d28nmVmBQpF2eNppsJOqaB9l\",          # optional if key already tied\n",
    "    project=\"proj_SXBV23aZ3XH51x5y1qwF48jV\"                # ‚Üê critical\n",
    ")\n",
    "\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "\n",
    "# Your fine-tuned models\n",
    "FT_MODELS = {\n",
    "    \"mini\": {\n",
    "        \"id\": \"ft:gpt-4o-mini-2024-07-18:politics-ai-research:ukraine-telegram-mini:BfSq29k1\",\n",
    "        \"base\": \"gpt-4o-mini\",\n",
    "        \"train_loss\": 0.033,\n",
    "        \"valid_loss\": 0.149\n",
    "    },\n",
    "    \"nano\": {\n",
    "        \"id\": \"ft:gpt-4.1-nano-2025-04-14:politics-ai-research:ukraine-classifier-nano:BfSlvv7Q\", \n",
    "        \"base\": \"gpt-4.1-nano\",\n",
    "        \"train_loss\": 0.139,\n",
    "        \"valid_loss\": 0.190\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"id\": \"ft:gpt-4.1-2025-04-14:politics-ai-research:ukraine-classifier:BfStxtYw\",\n",
    "        \"base\": \"gpt-4.1\",\n",
    "        \"train_loss\": 1.377,  # High train loss - might be undertrained?\n",
    "        \"valid_loss\": 0.029   # But low valid loss - interesting!\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üéØ Fine-Tuned Models Loaded:\")\n",
    "for name, info in FT_MODELS.items():\n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Model: {info['id']}\")\n",
    "    print(f\"  Base: {info['base']}\")\n",
    "    print(f\"  Train/Valid Loss: {info['train_loss']:.3f} / {info['valid_loss']:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Test 1: Basic Functionality - What format do they expect?\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 1: INPUT FORMAT DISCOVERY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test different input formats\n",
    "test_messages = [\n",
    "    \"Russian forces strike Kyiv infrastructure\",  # Simple text\n",
    "    \"[Channel: test] Russian forces strike Kyiv\",  # With channel\n",
    "    \"M1: Russian forces strike Kyiv\",  # With index\n",
    "    \"Score this: Russian forces strike Kyiv\",  # With instruction\n",
    "]\n",
    "\n",
    "test_prompts = [\n",
    "    None,  # No system prompt\n",
    "    \"Score the message\",  # Simple instruction\n",
    "    \"Return E,B,P,C scores\",  # Specific format\n",
    "    \"You are a war message classifier. Score: E(0-10),B(-1/0/1),P(0-3),C(0/1)\"  # Full prompt\n",
    "]\n",
    "\n",
    "def test_model_response(model_id, user_msg, system_msg=None):\n",
    "    \"\"\"Test how model responds to different inputs\"\"\"\n",
    "    messages = []\n",
    "    if system_msg:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_msg})\n",
    "    messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=messages,\n",
    "            max_tokens=50,\n",
    "            temperature=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test each model with different formats\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\n\\nüîç Testing {model_name.upper()} Model:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for msg in test_messages[:2]:  # Test first 2 message formats\n",
    "        for prompt in test_prompts[:2]:  # Test first 2 prompt types\n",
    "            result = test_model_response(model_info['id'], msg, prompt)\n",
    "            print(f\"\\nInput: '{msg}'\")\n",
    "            if prompt:\n",
    "                print(f\"System: '{prompt}'\")\n",
    "            print(f\"Output: {result}\")\n",
    "            \n",
    "            # Parse if it looks like scores\n",
    "            if ',' in result and len(result.split(',')) == 4:\n",
    "                try:\n",
    "                    scores = [x.strip() for x in result.split(',')]\n",
    "                    print(f\"Parsed: E={scores[0]}, B={scores[1]}, P={scores[2]}, C={scores[3]}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# %%\n",
    "# Test 2: Consistency Check - Same message, multiple runs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 2: CONSISTENCY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_message = \"NATO announces new military aid package for Ukraine worth $2 billion\"\n",
    "runs_per_model = 5\n",
    "\n",
    "consistency_results = {}\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nüîÑ Testing {model_name.upper()} consistency ({runs_per_model} runs):\")\n",
    "    \n",
    "    results = []\n",
    "    for i in range(runs_per_model):\n",
    "        response = test_model_response(model_info['id'], test_message)\n",
    "        results.append(response)\n",
    "        print(f\"  Run {i+1}: {response}\")\n",
    "    \n",
    "    # Check if all results are identical\n",
    "    unique_results = set(results)\n",
    "    consistency_results[model_name] = {\n",
    "        'results': results,\n",
    "        'unique': len(unique_results),\n",
    "        'consistent': len(unique_results) == 1\n",
    "    }\n",
    "    \n",
    "    print(f\"  Consistency: {'‚úÖ PERFECT' if len(unique_results) == 1 else f'‚ö†Ô∏è  {len(unique_results)} different outputs'}\")\n",
    "\n",
    "# %%\n",
    "# Test 3: Speed Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 3: SPEED & PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load sample messages\n",
    "df = pd.read_csv(ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\", nrows=100)\n",
    "test_samples = df[df['message_text'].notna()]['message_text'].tolist()[:10]\n",
    "\n",
    "speed_results = {}\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\n‚ö° Testing {model_name.upper()} speed:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    responses = []\n",
    "    \n",
    "    for msg in test_samples:\n",
    "        response = test_model_response(model_info['id'], msg[:200])\n",
    "        responses.append(response)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    speed_results[model_name] = {\n",
    "        'total_time': elapsed,\n",
    "        'avg_time': elapsed / len(test_samples),\n",
    "        'responses': responses\n",
    "    }\n",
    "    \n",
    "    print(f\"  Total time: {elapsed:.2f}s\")\n",
    "    print(f\"  Avg per message: {elapsed/len(test_samples)*1000:.0f}ms\")\n",
    "    print(f\"  Throughput: {len(test_samples)/elapsed:.1f} messages/second\")\n",
    "\n",
    "# %%\n",
    "# Test 4: Batch Processing Capability\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 4: BATCH PROCESSING TEST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try sending multiple messages at once\n",
    "batch_formats = [\n",
    "    # Format 1: Newline separated\n",
    "    \"Message 1: Russia attacks Kyiv\\nMessage 2: Peace talks resume\\nMessage 3: NATO sends aid\",\n",
    "    \n",
    "    # Format 2: Numbered\n",
    "    \"1. Russia attacks Kyiv\\n2. Peace talks resume\\n3. NATO sends aid\",\n",
    "    \n",
    "    # Format 3: Indexed\n",
    "    \"M0: Russia attacks Kyiv\\nM1: Peace talks resume\\nM2: NATO sends aid\",\n",
    "]\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nüì¶ Testing {model_name.upper()} batch capability:\")\n",
    "    \n",
    "    for i, batch in enumerate(batch_formats):\n",
    "        response = test_model_response(model_info['id'], batch)\n",
    "        print(f\"\\nFormat {i+1} response: {response}\")\n",
    "        \n",
    "        # Check if it returned multiple scores\n",
    "        if '|' in response or '\\n' in response or response.count(',') > 4:\n",
    "            print(\"  ‚úÖ Appears to handle batches!\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Single response only\")\n",
    "\n",
    "# %%\n",
    "# Test 5: Edge Cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST 5: EDGE CASES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "edge_cases = [\n",
    "    \"\",  # Empty\n",
    "    \"–ø—Ä–∏–≤–µ—Ç\",  # Non-English\n",
    "    \"üöÄüí•\",  # Emojis only\n",
    "    \"a\" * 500,  # Very long\n",
    "    \"NATO NATO NATO NATO\",  # Repetitive\n",
    "    \"2+2=4\",  # Non-war content\n",
    "]\n",
    "\n",
    "for model_name, model_info in FT_MODELS.items():\n",
    "    print(f\"\\nüîß Testing {model_name.upper()} edge cases:\")\n",
    "    \n",
    "    for case in edge_cases[:3]:  # Test first 3\n",
    "        response = test_model_response(model_info['id'], case)\n",
    "        print(f\"  '{case[:20]}...' ‚Üí {response}\")\n",
    "\n",
    "# %%\n",
    "# Visualization of Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Model Loss Comparison\n",
    "ax = axes[0, 0]\n",
    "models = list(FT_MODELS.keys())\n",
    "train_losses = [FT_MODELS[m]['train_loss'] for m in models]\n",
    "valid_losses = [FT_MODELS[m]['valid_loss'] for m in models]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, train_losses, width, label='Train Loss', alpha=0.8)\n",
    "ax.bar(x + width/2, valid_losses, width, label='Valid Loss', alpha=0.8)\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training vs Validation Loss')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "\n",
    "# 2. Speed Comparison\n",
    "if speed_results:\n",
    "    ax = axes[0, 1]\n",
    "    speeds = [speed_results[m]['avg_time'] * 1000 for m in models]\n",
    "    ax.bar(models, speeds)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Avg Response Time (ms)')\n",
    "    ax.set_title('Response Speed Comparison')\n",
    "\n",
    "# 3. Consistency Results\n",
    "if consistency_results:\n",
    "    ax = axes[1, 0]\n",
    "    consistency = [consistency_results[m]['unique'] for m in models]\n",
    "    colors = ['green' if c == 1 else 'orange' for c in consistency]\n",
    "    ax.bar(models, consistency, color=colors)\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Number of Unique Outputs')\n",
    "    ax.set_title('Consistency Test (5 runs, same input)')\n",
    "    ax.axhline(y=1, color='green', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Summary Table\n",
    "ax = axes[1, 1]\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "summary_data = []\n",
    "for model in models:\n",
    "    summary_data.append([\n",
    "        model.upper(),\n",
    "        f\"{FT_MODELS[model]['base']}\",\n",
    "        f\"{FT_MODELS[model]['train_loss']:.3f}\",\n",
    "        f\"{FT_MODELS[model]['valid_loss']:.3f}\",\n",
    "        f\"{speed_results.get(model, {}).get('avg_time', 0)*1000:.0f}ms\" if speed_results else \"N/A\"\n",
    "    ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=summary_data,\n",
    "    colLabels=['Model', 'Base', 'Train Loss', 'Valid Loss', 'Avg Speed'],\n",
    "    cellLoc='center',\n",
    "    loc='center'\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2)\n",
    "\n",
    "plt.suptitle('Fine-Tuned Model Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Final Recommendations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä ANALYSIS & RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. INPUT FORMAT:\")\n",
    "print(\"   Your models likely expect just the raw message text\")\n",
    "print(\"   No system prompt needed (it's baked into the fine-tuning)\")\n",
    "\n",
    "print(\"\\n2. OUTPUT FORMAT:\")\n",
    "print(\"   Models should return: E,B,P,C (4 comma-separated integers)\")\n",
    "\n",
    "print(\"\\n3. BEST MODEL:\")\n",
    "# Analyze which performed best\n",
    "if consistency_results:\n",
    "    consistent_models = [m for m in models if consistency_results[m]['consistent']]\n",
    "    print(f\"   Most consistent: {', '.join(consistent_models) if consistent_models else 'None perfectly consistent'}\")\n",
    "\n",
    "if speed_results:\n",
    "    fastest = min(models, key=lambda m: speed_results[m]['avg_time'])\n",
    "    print(f\"   Fastest: {fastest.upper()} ({speed_results[fastest]['avg_time']*1000:.0f}ms/msg)\")\n",
    "\n",
    "print(f\"\\n4. CONCERNING OBSERVATIONS:\")\n",
    "if FT_MODELS['full']['train_loss'] > 1.0:\n",
    "    print(\"   - 'full' model has high train loss (1.377) - might need more training\")\n",
    "print(\"   - Test batch processing capability before full-scale run\")\n",
    "print(\"   - Consider the base model costs (gpt-4.1 is most expensive)\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Run full scoring with best performing model\")\n",
    "print(\"   - Use batch processing if supported\")\n",
    "print(\"   - Monitor for consistency issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this to see what project your API key is using\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# List available models to see what you have access to\n",
    "try:\n",
    "    models = client.models.list()\n",
    "    ft_models = [m for m in models if m.id.startswith('ft:')]\n",
    "    print(f\"Found {len(ft_models)} fine-tuned models:\")\n",
    "    for m in ft_models:\n",
    "        print(f\"  - {m.id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing models: {e}\")\n",
    "\n",
    "# Check your default project\n",
    "print(f\"\\nCurrent API key org: {client.organization if hasattr(client, 'organization') else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it's a project issue\n",
    "from openai import OpenAI\n",
    "\n",
    "# Try without any project specification first\n",
    "client = OpenAI()\n",
    "\n",
    "# Test the simplest possible call\n",
    "test_model = \"ft:gpt-4o-mini-2024-07-18:politics-ai-research:ukraine-telegram-mini:BfSq29k1\"\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=test_model,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "        max_tokens=10,\n",
    "        temperature=0\n",
    "    )\n",
    "    print(f\"‚úÖ SUCCESS! Response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    \n",
    "    # If that fails, try specifying organization\n",
    "    try:\n",
    "        client_with_org = OpenAI(\n",
    "            organization=\"org-d28nmVmBQpF2eNppsJOqaB9l\"  # From your error message\n",
    "        )\n",
    "        response = client_with_org.chat.completions.create(\n",
    "            model=test_model,\n",
    "            messages=[{\"role\": \"user\", \"content\": \"test\"}],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(f\"‚úÖ SUCCESS with org! Response: {response.choices[0].message.content}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Still failing: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List ALL your accessible projects\n",
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\",\n",
    "}\n",
    "\n",
    "# Get organization details\n",
    "response = requests.get(\"https://api.openai.com/v1/organization\", headers=headers)\n",
    "print(\"Organization info:\", response.json())\n",
    "\n",
    "# Try to get project list (this endpoint might not be public)\n",
    "response = requests.get(\"https://api.openai.com/v1/projects\", headers=headers)\n",
    "print(\"Projects:\", response.json() if response.status_code == 200 else \"Not accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
