{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "import subprocess, sys, importlib, os, re\n",
    "from datetime import datetime\n",
    "import truthbrush as tb\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ UNIVERSAL PATCH CELL  (run once, very top of notebook) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import subprocess, sys, importlib, os, types\n",
    "from pathlib import Path\n",
    "\n",
    "# 1Ô∏è‚É£  make sure both python-dotenv and curl_cffi exist\n",
    "def ensure(pkg, src=None):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except ModuleNotFoundError:\n",
    "        subprocess.check_call(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", src or pkg]\n",
    "        )\n",
    "\n",
    "ensure(\"python-dotenv\")\n",
    "ensure(\"curl_cffi\")\n",
    "\n",
    "# 2Ô∏è‚É£  reload .env (override=True guarantees fresh values)\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(usecwd=True), override=True)\n",
    "\n",
    "# 3Ô∏è‚É£  import truthbrush and inject curl_cffi so NameError can‚Äôt happen\n",
    "import curl_cffi                     # noqa:  F401  (needed for side-effect)\n",
    "import truthbrush.api as tb_api\n",
    "tb_api.curl_cffi = curl_cffi         # hand it to truthbrush‚Äôs module scope\n",
    "\n",
    "import truthbrush as tb\n",
    "print(\"‚úî Patch cell finished ‚Äì environment refreshed, curl_cffi wired\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import truthbrush as tb\n",
    "from datetime import datetime, timezone\n",
    "import random, time\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# install SINGLE wrapper around _get   (skip if it already exists)\n",
    "# ---------------------------------------------------------------------------\n",
    "if not hasattr(tb.api.Api, \"_get_base\"):\n",
    "    tb.api.Api._get_base = tb.api.Api._get          # save original\n",
    "\n",
    "    def _polite_get(self, url, params=None):\n",
    "        \"\"\"Call Truth Social, then pause a polite, *adaptive* amount of time.\"\"\"\n",
    "        resp = self._get_base(url, params)\n",
    "\n",
    "        # ---------- adaptive back-off if we're about to hit the hard limit -----\n",
    "        if (self.ratelimit_remaining is not None\n",
    "                and self.ratelimit_remaining <= 10\n",
    "                and self.ratelimit_reset):\n",
    "            wait = max(\n",
    "                0,\n",
    "                (self.ratelimit_reset -\n",
    "                 datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "                ).total_seconds()\n",
    "            ) + random.uniform(1, 2)               # 1‚Äì2 s jitter\n",
    "            print(f\"üìâ near limit ‚Äì sleeping {wait:.1f}s\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "        # ---------- normal gentle delay (you may tune this) --------------------\n",
    "        else:\n",
    "            time.sleep(random.uniform(0.6, 1.2))    # was 1.5‚Äì3.0\n",
    "\n",
    "        return resp\n",
    "\n",
    "    tb.api.Api._get = _polite_get\n",
    "    print(\"‚úì polite-delay wrapper installed (0.6‚Äì1.2 s baseline)\")\n",
    "else:\n",
    "    print(\"‚úì wrapper already present ‚Äì no re-patch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = \"3XLbnc2wV48rUCGT-nKGnZPT2xFf3QRg0TZR1oMkNzU\"   # live bearer token (generated 2:26 AM)\n",
    "api   = tb.Api(token=TOKEN)\n",
    "api.auth_id = api.auth_id or \"\"\n",
    "print(\"client ready\")\n",
    "\n",
    "print(\"lookup test:\")\n",
    "try:\n",
    "    print(api.lookup(\"realDonaldTrump\")[\"id\"][:8], \"‚Ä¶ lookup OK\")\n",
    "except Exception as e:\n",
    "    print(\"lookup failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Truth Social scrape  ‚Üí  CSV (auto-flush every 100 hits) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import os, re, csv, json, random, time, pathlib, sys\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "SAVE_EVERY   = 100                               # flush cadence\n",
    "CSV_BASENAME = \"new_truth_scrape_matches.csv\"    # output file name\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"russia\",\"russian\",\"ukraine\",\"ukrainian\",\"ru-uk\",\"putin\",\n",
    "    \"zelensky\",\"zelenskyy\",\"kremlin\",\"kyiv\",\"crimea\",\"donbas\",\n",
    "    \"mariupol\",\"kherson\",\"luhansk\",\"dnipro\",\"odessa\",\"invasion\",\"war\",\n",
    "]\n",
    "SEED_HANDLES = [\"realDonaldTrump\",\"TeamTrump\",\"TrumpWarRoom\",\"WhiteHouse\",\"PressSec\"]\n",
    "\n",
    "# ----------------------------------------------------------------------------- helpers\n",
    "def keyword_hit(html:str)->bool: return any(k in html.lower() for k in KEYWORDS)\n",
    "\n",
    "def canonical_handle(hint:str)->str|None:\n",
    "    \"\"\"resolve acct ‚Üí canonical handle or None\"\"\"\n",
    "    try:                # fast path\n",
    "        return api.lookup(user_handle=hint.lstrip(\"@\")).get(\"acct\")\n",
    "    except Exception:   # fall back to search\n",
    "        try:\n",
    "            page = next(api.search(\"accounts\", hint, limit=1))\n",
    "            return page[\"accounts\"][0][\"acct\"] if page[\"accounts\"] else None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def prepend_first(item, iterator):\n",
    "    yield item\n",
    "    yield from iterator\n",
    "\n",
    "# ----------------------------------------------------------------------------- resolve\n",
    "handles = [h for h in (canonical_handle(h) for h in SEED_HANDLES) if h]\n",
    "if not handles:\n",
    "    raise RuntimeError(\"Could not resolve any seed handles ‚Äì check token/login.\")\n",
    "print(\"Scanning:\", handles)\n",
    "\n",
    "# ----------------------------------------------------------------------------- choose output path\n",
    "ROOT = pathlib.Path.cwd()\n",
    "while ROOT.parent != ROOT and not (ROOT/\".git\").exists() and not (ROOT/\".env\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "out_path = ROOT / \"outputs\" / CSV_BASENAME\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# write header once\n",
    "with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    csv.DictWriter(f, fieldnames=[\"created_at\",\"account\",\"id\",\"text\"]).writeheader()\n",
    "\n",
    "def flush(buf:list[dict]):\n",
    "    \"\"\"append buffered rows to CSV and clear list\"\"\"\n",
    "    if not buf: return\n",
    "    with out_path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"created_at\",\"account\",\"id\",\"text\"])\n",
    "        w.writerows(buf)\n",
    "    buf.clear()\n",
    "\n",
    "# ----------------------------------------------------------------------------- scrape\n",
    "total_hits = 0\n",
    "for h in handles:\n",
    "    print(f\"\\n‚Ü≥ pulling @{h}\")\n",
    "    try:\n",
    "        gen         = api.pull_statuses(username=h, replies=False, verbose=False)\n",
    "        first       = next(gen)                      # heartbeat\n",
    "        print(\"  ‚úì first post received\")\n",
    "        gen         = prepend_first(first, gen)\n",
    "\n",
    "        pbar        = tqdm(gen, unit=\"post\", desc=f\"{h}\", leave=True)\n",
    "        matched     = 0\n",
    "        buf         = []\n",
    "\n",
    "        for post in pbar:\n",
    "            if post and post.get(\"content\") and keyword_hit(post[\"content\"]):\n",
    "                matched += 1; total_hits += 1\n",
    "                buf.append({\n",
    "                    \"created_at\": post[\"created_at\"],\n",
    "                    \"account\"   : h,\n",
    "                    \"id\"        : post[\"id\"],\n",
    "                    \"text\"      : re.sub(r\"<[^>]+>\",\"\",post[\"content\"]).strip(),\n",
    "                })\n",
    "                if len(buf) >= SAVE_EVERY:\n",
    "                    flush(buf)\n",
    "            if matched and matched % 25 == 0:\n",
    "                pbar.set_description(f\"{h}  hits:{matched}\")\n",
    "\n",
    "        pbar.close(); flush(buf)\n",
    "        print(f\"‚úì @{h}: {matched} matches saved\")\n",
    "\n",
    "    except Exception as e:\n",
    "        flush(buf)\n",
    "        if \"401\" in str(e) or \"unauthorized\" in str(e).lower():\n",
    "            print(f\"‚ö†Ô∏è  token expired while scraping @{h}.  \"\n",
    "                  f\"Matches so far are safely saved to CSV.\\n\"\n",
    "                  \"‚û°Ô∏è  Refresh bearer token and rerun to resume.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  error on @{h}: {e} ‚Äì continuing ‚Ä¶\")\n",
    "\n",
    "print(f\"\\n‚úÖ scrape complete ¬∑ total rows on disk: {total_hits}\")\n",
    "print(\"üìÑ CSV location:\", out_path.relative_to(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- 1.  resume Trump with tighter filter --------------------\n",
    "import re, csv, pathlib, time, random, json\n",
    "from datetime import datetime, timezone\n",
    "from dateutil import parser as dt_parse\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚îÄ‚îÄ parameters\n",
    "SAVE_EVERY   = 100\n",
    "OUTFILE      = \"new_truth_scrape_matches_v2.csv\"\n",
    "TRUMP_HANDLE = \"realDonaldTrump\"\n",
    "\n",
    "KEYWORDS = [\n",
    "    \"russia\",\"russian\",\"ukraine\",\"ukrainian\",\"ru-uk\",\"putin\",\n",
    "    \"zelensky\",\"zelenskyy\",\"kremlin\",\"kyiv\",\"crimea\",\"donbas\",\n",
    "    \"mariupol\",\"kherson\",\"luhansk\",\"dnipro\",\"odessa\",\"invasion\",\"war\",\n",
    "]\n",
    "WORD_RE = re.compile(r'\\b(' + \"|\".join(KEYWORDS) + r')\\b', re.I)\n",
    "\n",
    "def keyword_hit(text:str)->bool:\n",
    "    return bool(WORD_RE.search(text or \"\"))\n",
    "\n",
    "# ‚îÄ‚îÄ find last Trump ID already scraped (if any)\n",
    "ROOT = pathlib.Path.cwd()\n",
    "while ROOT.parent!=ROOT and not (ROOT/\".git\").exists() and not (ROOT/\".env\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "already = ROOT / \"outputs\" / \"new_truth_scrape_matches.csv\"\n",
    "since_id = None\n",
    "if already.exists():\n",
    "    df_existing = pd.read_csv(already)\n",
    "    trump_rows  = df_existing[df_existing[\"account\"]==TRUMP_HANDLE]\n",
    "    if not trump_rows.empty:\n",
    "        since_id = str(trump_rows[\"id\"].max())     # resume *after* this id\n",
    "        print(\"‚è© resuming after id\", since_id)\n",
    "\n",
    "# ‚îÄ‚îÄ output file\n",
    "out_path = ROOT / \"outputs\" / OUTFILE\n",
    "out_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "if not out_path.exists():\n",
    "    with out_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        csv.DictWriter(f, fieldnames=[\"created_at\",\"account\",\"id\",\"text\"]).writeheader()\n",
    "\n",
    "def flush(buf):\n",
    "    if not buf: return\n",
    "    with out_path.open(\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"created_at\",\"account\",\"id\",\"text\"])\n",
    "        w.writerows(buf)\n",
    "    ts = datetime.now().strftime('%H:%M:%S')\n",
    "    print(f\"üíæ  [{ts}] saved {len(buf)} rows\")\n",
    "\n",
    "# ‚îÄ‚îÄ scrape loop (Trump only) ------------------------------------------------\n",
    "buf=[]; matched=0\n",
    "print(f\"‚Ü≥ pulling @{TRUMP_HANDLE} ‚Ä¶\")\n",
    "try:\n",
    "    gen = api.pull_statuses(username=TRUMP_HANDLE,\n",
    "                            replies=False,\n",
    "                            verbose=False,\n",
    "                            since_id=since_id)\n",
    "    for post in tqdm(gen, unit=\"post\"):\n",
    "        if keyword_hit(post.get(\"content\",\"\")):\n",
    "            matched += 1\n",
    "            buf.append({\n",
    "                \"created_at\": post[\"created_at\"],\n",
    "                \"account\"   : TRUMP_HANDLE,\n",
    "                \"id\"        : post[\"id\"],\n",
    "                \"text\"      : re.sub(r\"<[^>]+>\",\"\",post[\"content\"]).strip()\n",
    "            })\n",
    "            if len(buf) >= SAVE_EVERY:\n",
    "                flush(buf); buf.clear()\n",
    "    flush(buf)\n",
    "except KeyboardInterrupt:\n",
    "    flush(buf)\n",
    "    print(\"‚èπ interrupted ‚Äì everything so far is saved\")\n",
    "\n",
    "print(f\"‚úì finished Trump resume ‚Äì {matched} new matches\")\n",
    "\n",
    "# ----------------- 2.  quick sanity-check / viz ----------------------------\n",
    "df = pd.read_csv(out_path)          # v2 file (just Trump resume)\n",
    "print(\"rows in v2:\", len(df))\n",
    "\n",
    "# -- posts per month\n",
    "df[\"month\"] = pd.to_datetime(df[\"created_at\"]).dt.to_period(\"M\")\n",
    "counts = df.groupby(\"month\").size()\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "counts.plot(kind=\"bar\")\n",
    "plt.title(\"RU/UA-related Trump Truths per month\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -- top keyword hits\n",
    "def which_keyword(text):\n",
    "    m = WORD_RE.search(text)\n",
    "    return m.group(0).lower() if m else None\n",
    "df[\"kw\"] = df[\"text\"].apply(which_keyword)\n",
    "topk = df[\"kw\"].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "topk.plot(kind=\"bar\")\n",
    "plt.title(\"Top keyword hits\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
