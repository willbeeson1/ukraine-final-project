{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, requests, pandas as pd, numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- .env loader that works in notebooks ------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def find_repo_root(start: Path, marker=\".git\") -> Path:\n",
    "    \"\"\"Walk up until we see a folder containing the given marker ('.git' or '.env').\"\"\"\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(f\"Repository root with {marker} not found from {start}\")\n",
    "\n",
    "# 1) locate repo root (folder that has .env **or** .git)\n",
    "repo_root = find_repo_root(Path.cwd(), \".env\")\n",
    "\n",
    "# 2) load environment variables\n",
    "load_dotenv(repo_root / \".env\")\n",
    "\n",
    "# 3) add src/ to Python path (optional, if youâ€™ll import from src/)\n",
    "src_path = repo_root / \"src\"\n",
    "if src_path.exists():\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# 4) fetch secrets (raise fast if any missing)\n",
    "REQUIRED = [\"OPENAI_API_KEY\", \"NEWSAPI_KEY\"]\n",
    "CREDS = {k: os.getenv(k) for k in REQUIRED}\n",
    "missing = [k for k, v in CREDS.items() if not v]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing secrets in .env: {', '.join(missing)}\")\n",
    "\n",
    "# handy variables\n",
    "OPENAI_KEY       = CREDS[\"OPENAI_API_KEY\"]\n",
    "NEWSAPI_KEY      = CREDS[\"NEWSAPI_KEY\"]\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_SECRET    = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_AGENT     = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "print(f\"âœ…  .env loaded from {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Fetch & cache headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 1.  Fetch & cache *all* Ukraineâ€‘related headlines (24Â FebÂ 2022 â†’ AprÂ 20Â 2025)\n",
    "#      â€“ pulls from EVERY English source in NewsAPI, paged dayâ€‘byâ€‘day\n",
    "# ---------------------------------------------------------------\n",
    "import sys, time, urllib.parse, requests, pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")        # be sure it is set\n",
    "assert NEWSAPI_KEY, \"â¡ï¸  set NEWSAPI_KEY envâ€‘var first!\"\n",
    "\n",
    "# date range: full war period\n",
    "start_date = datetime(2025, 5, 27)            # day after last available\n",
    "end_date   = datetime(2025, 6, 6)            # â€œnowâ€\n",
    "\n",
    "# broad query that catches >Â 99Â % of RUâ€“UA war pieces\n",
    "base_query = (\n",
    "    'ukraine OR kyiv OR kiev OR lviv OR odessa OR donbas OR donbass '\n",
    "    'OR \"volodymyr zelensky\" OR zelenskyy OR putin OR russia OR drone OR \"trojan horse\" OR russian OR ukranian OR war'\n",
    ")\n",
    "ENC_QUERY = urllib.parse.quote_plus(base_query)\n",
    "\n",
    "DAY_URL = (\n",
    "    'https://newsapi.org/v2/everything?'\n",
    "    'q={q}&from={f}&to={t}&language=en&sortBy=publishedAt&pageSize=100&page={pg}'\n",
    ")\n",
    "\n",
    "def daterange(start, end):\n",
    "    for n in range((end - start).days + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "records = []\n",
    "total   = 0\n",
    "for day in daterange(start_date, end_date):\n",
    "    d_str = day.strftime('%Y-%m-%d')\n",
    "    next_str = (day + timedelta(1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    pg = 1\n",
    "    while True:\n",
    "        url  = DAY_URL.format(q=ENC_QUERY, f=d_str, t=next_str, pg=pg)\n",
    "        resp = requests.get(url, headers={'X-Api-Key': NEWSAPI_KEY})\n",
    "        if resp.status_code != 200:\n",
    "            sys.stderr.write(f\"\\nâš ï¸  {d_str} page {pg} â†’ {resp.status_code}: {resp.json().get('message','')}\\n\")\n",
    "            break\n",
    "\n",
    "        arts = resp.json().get('articles', [])\n",
    "        if not arts:\n",
    "            break\n",
    "\n",
    "        for a in arts:\n",
    "            records.append({\n",
    "                \"date\"  : d_str,\n",
    "                \"source\": a[\"source\"][\"id\"] or \"unknown\",\n",
    "                \"title\" : a[\"title\"]\n",
    "            })\n",
    "        total += len(arts)\n",
    "        if len(arts) < 100:                      # last page\n",
    "            break\n",
    "        pg += 1\n",
    "        time.sleep(0.3)                          # stay well under 30Â req/min\n",
    "\n",
    "    sys.stdout.write(f\"\\r{d_str} âœ“ {total:,} headlines so far\"); sys.stdout.flush()\n",
    "\n",
    "print(f\"\\nDone! Collected {total:,} headlines.\")\n",
    "df = pd.DataFrame(records).drop_duplicates()\n",
    "df.to_csv(\"raw_headlines_5_27_to_6_6.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Fast relevance filter (RUâ€“UA headlines âœ outputs/headlines_ru_ua.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "#  FAST FILTER â€” split into â‰¤50 000-request chunks, show progress,\n",
    "#                auto-download & save war-only CSV\n",
    "# ------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import json, time, datetime as dt\n",
    "import pandas as pd, openai\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MAX_REQ   = 40_000\n",
    "RAW_CSV   = ROOT / \"outputs\" / \"raw_headlines_5_27_to_6_6.csv\"\n",
    "CHUNK_DIR = ROOT / \"outputs\" / \"filter_chunks\"\n",
    "OUT_CSV   = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "CHUNK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- load full set ------------------------------------\n",
    "df = pd.read_csv(RAW_CSV, parse_dates=[\"date\"])\n",
    "print(\"ğŸ“„ loaded\", len(df), \"headlines\")\n",
    "\n",
    "# ---------- build chunked JSONL files ------------------------\n",
    "chunk_paths, part_no, line_in_chunk = [], 1, 0\n",
    "pbar = tqdm(total=len(df), desc=\"Writing JSONL chunks\", unit=\"lines\")\n",
    "\n",
    "def new_writer(n: int):\n",
    "    p = CHUNK_DIR / f\"filter_tasks_part{n:02d}.jsonl\"\n",
    "    return p.open(\"w\"), p\n",
    "\n",
    "writer, cur_path = new_writer(part_no)\n",
    "\n",
    "for idx, title in df[\"title\"].items():\n",
    "    writer.write(json.dumps({\n",
    "        \"custom_id\": str(idx),\n",
    "        \"method\":   \"POST\",\n",
    "        \"url\":      \"/v1/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-mini\",\n",
    "            \"temperature\": 0,\n",
    "            \"max_tokens\": 1,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\",\n",
    "                 \"content\": \"Return YES if the headline is about/related to the Russia-Ukraine war, otherwise NO.\"},\n",
    "                {\"role\": \"user\",\n",
    "                 \"content\": (\"\" if pd.isna(title) else str(title))[:500]}\n",
    "            ]\n",
    "        }\n",
    "    }) + \"\\n\")\n",
    "    line_in_chunk += 1\n",
    "    pbar.update(1)\n",
    "\n",
    "    if line_in_chunk >= MAX_REQ:\n",
    "        writer.close()\n",
    "        chunk_paths.append(cur_path)\n",
    "        part_no += 1\n",
    "        writer, cur_path = new_writer(part_no)\n",
    "        line_in_chunk = 0\n",
    "\n",
    "writer.close()\n",
    "chunk_paths.append(cur_path)\n",
    "pbar.close()\n",
    "print(f\"âœ… built {len(chunk_paths)} chunk(s) â€“ each â‰¤{MAX_REQ:,} requests\")\n",
    "\n",
    "# ---------- launch the batches --------------------------------\n",
    "client = openai.OpenAI()\n",
    "batch_ids = []\n",
    "\n",
    "for p in chunk_paths:\n",
    "    fid   = client.files.create(file=open(p, \"rb\"), purpose=\"batch\").id\n",
    "    batch = client.batches.create(\n",
    "              input_file_id=fid,\n",
    "              endpoint=\"/v1/chat/completions\",\n",
    "              completion_window=\"24h\")\n",
    "    batch_ids.append(batch.id)\n",
    "\n",
    "    # live progress for this batch\n",
    "    bar = tqdm(total=MAX_REQ if p != chunk_paths[-1] else None,\n",
    "               desc=f\"{p.name}  â†’ {batch.id[:8]}\",\n",
    "               unit=\"req\")\n",
    "\n",
    "    while True:\n",
    "        b  = client.batches.retrieve(batch.id)\n",
    "        rc = b.request_counts\n",
    "        bar.total = rc.total or bar.total         # fill in when known\n",
    "        bar.n     = rc.completed + rc.failed\n",
    "        bar.refresh()\n",
    "\n",
    "        if b.status == \"completed\":\n",
    "            bar.close()\n",
    "            print(\"âœ…\", b.id, \"completed\")\n",
    "            break\n",
    "        if b.status == \"failed\":\n",
    "            bar.close()\n",
    "            raise RuntimeError(f\"Batch {b.id} failed â€“ check dashboard\")\n",
    "        time.sleep(15)\n",
    "\n",
    "print(\"\\nğŸ‰ all batches finished:\", \", \".join(batch_ids))\n",
    "\n",
    "# ---------- download, filter & save ---------------------------\n",
    "yes_ids = set()\n",
    "for bid in batch_ids:\n",
    "    out_id  = client.batches.retrieve(bid).output_file_id\n",
    "    content = client.files.content(out_id).content\n",
    "    for line in content.splitlines():\n",
    "        rec = json.loads(line)\n",
    "        if rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip().upper().startswith(\"Y\"):\n",
    "            yes_ids.add(int(rec[\"custom_id\"]))\n",
    "\n",
    "df_yes = df.loc[yes_ids].reset_index(drop=True)\n",
    "df_yes.to_csv(OUT_CSV, index=False)\n",
    "print(f\"ğŸ¯ kept {len(df_yes):,}/{len(df)} headlines â€“ saved â†’\", OUT_CSV.relative_to(ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "#### Compile Filtered Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Merge 3 filter-batches  â†’  outputs/headlines_ru_ua.csv      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, openai, datetime as dt\n",
    "\n",
    "ROOT      = Path.cwd().resolve().parents[0]          # adjust if needed\n",
    "RAW_CSV   = ROOT / \"outputs\" / \"raw_headlines.csv\"\n",
    "OUT_CSV   = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "\n",
    "BATCH_IDS = [\n",
    "    \"batch_6836686d268081909e722fc9116cda78\",  # part-01\n",
    "    \"batch_68368e492f708190b81b77cad2380411\",  # part-02\n",
    "    \"batch_6836ae30c38c8190833297ae74f6ac27\",  # part-03\n",
    "]\n",
    "\n",
    "client   = openai.OpenAI()\n",
    "yes_ids  = set()\n",
    "\n",
    "print(\"â¬‡ï¸  downloading outputs & collecting YES ids â€¦\")\n",
    "for bid in BATCH_IDS:\n",
    "    b = client.batches.retrieve(bid)\n",
    "    assert b.status == \"completed\", f\"{bid} not completed (status={b.status})\"\n",
    "    out_txt = client.files.content(b.output_file_id).content\n",
    "    for ln in out_txt.splitlines():\n",
    "        rec = json.loads(ln)\n",
    "        if rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"] \\\n",
    "               .strip().upper().startswith(\"Y\"):\n",
    "            yes_ids.add(int(rec[\"custom_id\"]))\n",
    "\n",
    "# ---------- filter master CSV -----------------------------------\n",
    "df_all = pd.read_csv(RAW_CSV, parse_dates=[\"date\"])\n",
    "df_yes = df_all.loc[sorted(yes_ids)].reset_index(drop=True)   # â† list, not set\n",
    "df_yes.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "# ---------- summary ---------------------------------------------\n",
    "n_in  = len(df_all)\n",
    "n_out = len(df_yes)\n",
    "print(f\"\\nğŸ“Š Filter summary  ({dt.datetime.now():%Y-%m-%d %H:%M})\")\n",
    "print(f\" â€¢ total headlines analysed : {n_in:,}\")\n",
    "print(f\" â€¢ war-related kept         : {n_out:,}  ({n_out/n_in*100:0.1f} %)\")\n",
    "print(f\" â€¢ irrelevant discarded     : {n_in - n_out:,}\")\n",
    "print(f\"âœ…  Saved â†’ {OUT_CSV.relative_to(ROOT)}\")\n",
    "\n",
    "# ---------- quick peek ------------------------------------------\n",
    "display(df_yes.sample(5, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Pull 750 Headlines to Create Gold Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Draft gold-set sampler â€“ evenly-spaced 750 headlines        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT       = Path.cwd().resolve().parents[0]\n",
    "SRC_CSV    = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "GOLD_CSV   = ROOT / \"outputs\" / \"gold_candidates_750_v2.csv\"\n",
    "TARGET_N   = 750                     # ~one headline every 1Â½ days\n",
    "\n",
    "# ---------- load & prep -----------------------------------------\n",
    "df = pd.read_csv(SRC_CSV, parse_dates=[\"date\"]).sort_values(\"date\")\n",
    "span_days = (df[\"date\"].iloc[-1] - df[\"date\"].iloc[0]).days\n",
    "step      = span_days / TARGET_N\n",
    "\n",
    "# ---------- even-time sampling ----------------------------------\n",
    "want_dates = pd.date_range(df[\"date\"].min(),\n",
    "                           df[\"date\"].max(),\n",
    "                           periods=TARGET_N).date\n",
    "\n",
    "# pick the headline closest to each desired date (within Â±3 days)\n",
    "picked_idx = []\n",
    "for d in want_dates:\n",
    "    window = df.loc[(df[\"date\"].dt.date - d).abs().sort_values().index]\n",
    "    # take the first *unused* headline in that window\n",
    "    for idx in window.index:\n",
    "        if idx not in picked_idx:\n",
    "            picked_idx.append(idx)\n",
    "            break\n",
    "\n",
    "gold_df = (df.loc[picked_idx]\n",
    "             .sort_values(\"date\")\n",
    "             .reset_index(drop=True))\n",
    "\n",
    "gold_df.to_csv(GOLD_CSV, index=False)\n",
    "print(f\"âœ… wrote {len(gold_df)} rows â†’ {GOLD_CSV.relative_to(ROOT)}\")\n",
    "display(gold_df.head(10))            # quick peek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Quick headline-density plot (daily count + global average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Daily headline count  â€¢ dashed global average               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path.cwd().resolve().parents[0]\n",
    "SRC_CSV  = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "\n",
    "df = pd.read_csv(SRC_CSV, parse_dates=[\"date\"])\n",
    "daily = (df.groupby(df[\"date\"].dt.date)\n",
    "           .size()\n",
    "           .rename(\"n\"))\n",
    "\n",
    "avg_per_day = daily.mean()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(daily.index, daily, lw=1, alpha=.7)\n",
    "plt.axhline(avg_per_day, ls=\"--\", c=\"tab:red\",\n",
    "            label=f\"global avg = {avg_per_day:0.1f} / day\")\n",
    "plt.title(\"Number of war-related headlines per day\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"â±ï¸  date span : {daily.index.min()} â†’ {daily.index.max()}\")\n",
    "print(f\"ğŸ“Š  mean      : {avg_per_day:0.2f}  headlines / day\")\n",
    "print(f\"ğŸ”¢  total     : {daily.sum():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
