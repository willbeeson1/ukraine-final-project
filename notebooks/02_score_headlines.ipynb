{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Reload Cached Scores (up to 4/20/2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# FAST-PATH â–¸ reuse cached daily-index if it exists\n",
    "# ---------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "CSV_CACHE = ROOT / \"outputs\" / \"ukraine_escalation_daily.csv\"   # always RELATIVE to repo root\n",
    "\n",
    "if CSV_CACHE.exists():\n",
    "    print(f\"ğŸ“„  Loading cached scores from {CSV_CACHE.relative_to(ROOT)}\")\n",
    "\n",
    "    # read: first column is the index we wrote earlier\n",
    "    df = (pd.read_csv(CSV_CACHE, index_col=0)        # â† index_col=0 fixes the error\n",
    "            .rename(columns=str.lower)               # normalise headers\n",
    "            .rename(columns={\"score\": \"escalation\"}) # unify column name if needed\n",
    "    )\n",
    "\n",
    "    # make sure index is datetime (it was stored as ISO strings)\n",
    "    df.index = pd.to_datetime(df.index, errors=\"raise\")\n",
    "    df.index.name = \"date\"\n",
    "\n",
    "    # add rolling 7-day mean (if not already present / you want to recompute)\n",
    "    if \"roll7\" not in df.columns:\n",
    "        df[\"roll7\"] = df[\"escalation\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "    # quick peek\n",
    "    display(df.head())\n",
    "    df.plot(y=[\"escalation\",\"roll7\"], figsize=(10,4),\n",
    "            label=[\"Daily\",\"7-day mean\"], title=\"Cached escalation series\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    USE_CACHE_ONLY = True      # later cells can skip the batch-download logic\n",
    "else:\n",
    "    print(\"âš ï¸  No cached CSV found â€“ full batch logic will run.\")\n",
    "    USE_CACHE_ONLY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Score Recent Headlines (up to 5/27/2025)\n",
    "### Load cache & figure out what remains to score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# repo paths  (ROOT was defined in the earlier bootstrap cell)\n",
    "# ---------------------------------------------------------------\n",
    "import pandas as pd, numpy as np, json, time, datetime as dt, openai, pathlib\n",
    "OPENAI_CLIENT = openai.OpenAI()\n",
    "\n",
    "CSV_CACHE   = ROOT / \"outputs\" / \"ukraine_escalation_daily.csv\"\n",
    "NEW_RAW_CSV = ROOT / \"outputs\" / \"raw_headlines_4_21_to_5_27.csv\"\n",
    "\n",
    "# --- load existing daily index ----------------------------------\n",
    "df_existing = (pd.read_csv(CSV_CACHE, index_col=0)      # first col is the index\n",
    "                 .rename(columns=str.lower)             # 'escalation', 'roll7'\n",
    "                 .rename(columns={\"score\": \"escalation\"}))\n",
    "df_existing.index = pd.to_datetime(df_existing.index)\n",
    "df_existing.index.name = \"date\"\n",
    "\n",
    "print(\"âœ… loaded cached index â†’\", df_existing.index.min().date(),\n",
    "      \"â€¦\", df_existing.index.max().date())\n",
    "\n",
    "# --- load the NEW headlines ------------------------------------\n",
    "raw_new = pd.read_csv(NEW_RAW_CSV, parse_dates=[\"date\"])\n",
    "raw_new[\"title\"] = raw_new[\"title\"].astype(str)\n",
    "\n",
    "# keep only dates that are NOT in the cache\n",
    "todo = (raw_new[~raw_new[\"date\"].isin(df_existing.index)]\n",
    "          .groupby(\"date\")[\"title\"].apply(list)\n",
    "          .sort_index())\n",
    "\n",
    "print(f\"ğŸ†• need to score {len(todo)} new day(s) â€¢ {raw_new.shape[0]:,} headlines total\")\n",
    "\n",
    "if not len(todo):\n",
    "    print(\"ğŸ‰ cache already up-to-date â€“ nothing to do\"); raise SystemExit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Build one JSONL & launch a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# build JSONL  (â‰¤120 headlines / day, no look-back headlines)\n",
    "# ---------------------------------------------------------------\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an analyst quantifying RUSSIAâ€“UKRAINE military-escalation RISK.\n",
    "\n",
    "Scale (integer only):\n",
    "0  = No fighting / purely diplomatic\n",
    "2  = Low-level skirmishes\n",
    "4  = Noticeable escalation (large drone / missile strike)\n",
    "6  = Major battlefield offensive or big weapons shipment\n",
    "8  = Strategic escalation (Crimea bridge hit, WMD threat)\n",
    "10 = Nuclear rhetoric, nuclear alert or actual WMD use\n",
    "\n",
    "Return ONLY the integer 0-10.\n",
    "\"\"\"\n",
    "\n",
    "MAX_HEADLINES = 120\n",
    "jsonl_path = ROOT / \"outputs\" / \"ua_tasks_2025Q2.jsonl\"\n",
    "\n",
    "with jsonl_path.open(\"w\") as fh:\n",
    "    for day, titles in todo.items():\n",
    "        task = {\n",
    "            \"custom_id\": str(day.date()),\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 3,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",\n",
    "                     \"content\": \"### Headlines TODAY\\n\" +\n",
    "                                \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        fh.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "print(\"âœ… wrote JSONL â†’\", jsonl_path.name)\n",
    "\n",
    "# ------------- launch batch & poll -----------------------------\n",
    "file_id = OPENAI_CLIENT.files.create(file=jsonl_path.open(\"rb\"),\n",
    "                                     purpose=\"batch\").id\n",
    "batch   = OPENAI_CLIENT.batches.create(\n",
    "            input_file_id=file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\")\n",
    "\n",
    "print(\"ğŸš€ batch\", batch.id, \"submitted â€“ pollingâ€¦\")\n",
    "while True:\n",
    "    b = OPENAI_CLIENT.batches.retrieve(batch.id)\n",
    "    rc = b.request_counts\n",
    "    print(f\"\\r{dt.datetime.now():%H:%M:%S}  {b.status:<10}\"\n",
    "          f\"{rc.completed}/{rc.total}\", end=\"\")\n",
    "    if b.status in {\"completed\", \"failed\", \"expired\"}:\n",
    "        print()\n",
    "        if b.status != \"completed\":\n",
    "            raise RuntimeError(\"Batch finished with status\", b.status)\n",
    "        break\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# DIRECT CALLS FOR SMALL BACK-FILL WINDOWS  (â‰¤ ~100 days)\n",
    "# ------------------------------------------------------------------\n",
    "import time, json, pandas as pd, numpy as np, datetime as dt, openai, tqdm\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an analyst quantifying RUSSIAâ€“UKRAINE military-escalation RISK.\n",
    "\n",
    "Scale (integer only):\n",
    "0  = No active fighting; purely diplomatic headlines\n",
    "2  = Low-level skirmishes, no major offensives\n",
    "4  = Noticeable escalation (large drone / missile strikes)\n",
    "6  = Major battlefield offensive OR significant weapons shipment\n",
    "8  = Strategic escalation (Crimea bridge hit, use of banned weapons)\n",
    "10 = Nuclear rhetoric, nuclear forces on alert, or actual WMD use\n",
    "\n",
    "Return ONLY the integer (0-10).\"\"\"\n",
    "\n",
    "MAX_HEADLINES = 120            # same cap as before\n",
    "RATE_DELAY    = 0.8            # â‰¤75 requests / min; stay well under limits\n",
    "RETRIES       = 3\n",
    "TIMEOUT       = 40\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "new_scores, lat = {}, []        # day â†’ int   & latency tracker\n",
    "pbar = tqdm.tqdm(todo.items(), total=len(todo), ncols=90, desc=\"Scoring\")\n",
    "\n",
    "for day, titles in pbar:\n",
    "    user_block = \"### Headlines TODAY\\n\" + \\\n",
    "                 \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])\n",
    "\n",
    "    for attempt in range(1, RETRIES + 1):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            rsp = openai_client.chat.completions.create(\n",
    "                    model       =\"gpt-4o-mini\",\n",
    "                    temperature =0,\n",
    "                    max_tokens  =3,\n",
    "                    timeout     =TIMEOUT,\n",
    "                    messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                              {\"role\":\"user\",  \"content\":user_block}],\n",
    "            )\n",
    "            lat.append(time.time() - t0)\n",
    "            new_scores[pd.to_datetime(day)] = int(rsp.choices[0].message.content.strip())\n",
    "            break                                   # success â†’ exit retry-loop\n",
    "        except Exception as e:\n",
    "            if attempt == RETRIES:\n",
    "                pbar.write(f\"âŒ {day.date()} failed ({e.__class__.__name__}); NaN stored\")\n",
    "                new_scores[pd.to_datetime(day)] = np.nan\n",
    "            else:\n",
    "                backoff = 3 * attempt\n",
    "                pbar.write(f\"âš ï¸ {day.date()} attempt {attempt} error: {e}; retrying in {backoff}sâ€¦\")\n",
    "                time.sleep(backoff)\n",
    "\n",
    "    time.sleep(RATE_DELAY)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"âœ… finished   mean latency = {np.nanmean(lat):.2f}s   failures = {pd.isna(list(new_scores.values())).sum()}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MERGE with existing index and save\n",
    "# ------------------------------------------------------------------\n",
    "ser_new   = pd.Series(new_scores, name=\"escalation\").sort_index()\n",
    "df_merged = (pd.concat([df_existing[\"escalation\"], ser_new])\n",
    "               .sort_index()\n",
    "               .to_frame())\n",
    "df_merged[\"roll7\"] = df_merged[\"escalation\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "cache_path = ROOT / \"outputs\" / \"ukraine_escalation_daily.csv\"\n",
    "df_merged.to_csv(cache_path, date_format=\"%Y-%m-%d\")\n",
    "print(\"ğŸ“„ updated cache â†’\", cache_path.relative_to(ROOT))\n",
    "\n",
    "# quick sanity plot\n",
    "df_merged[\"escalation\"].plot(figsize=(12,3), alpha=.5, lw=.8)\n",
    "df_merged[\"roll7\"].plot(color=\"tab:red\", lw=2)\n",
    "plt.title(\"Updated escalation index (incl. new back-fill)\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Embed ~40 000 headlines  â†’  text-embedding-3-small      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, tqdm, numpy as np, openai\n",
    "\n",
    "ROOT        = Path.cwd().resolve().parents[0]\n",
    "JSONL_PATH  = ROOT / \"outputs\" / \"filter_tasks_part01.jsonl\"\n",
    "EMB_MODEL   = \"text-embedding-3-small\"\n",
    "CHUNK_N     = 1_000                                 # up to 2 048 allowed\n",
    "OUT_NPY     = ROOT / \"outputs\" / \"emb_part01.npy\"\n",
    "OUT_IDX     = ROOT / \"outputs\" / \"idx_part01.npy\"\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# ---------- load titles -------------------------------------------\n",
    "idxs, titles, skipped = [], [], 0\n",
    "with JSONL_PATH.open() as fh:\n",
    "    for ln in fh:\n",
    "        obj   = json.loads(ln)\n",
    "        title = obj[\"body\"][\"messages\"][1][\"content\"]\n",
    "        if not isinstance(title, str) or not title.strip():\n",
    "            skipped += 1\n",
    "            continue\n",
    "        idxs.append(int(obj[\"custom_id\"]))\n",
    "        titles.append(title.strip())\n",
    "\n",
    "n = len(titles)\n",
    "print(f\"ğŸ“ kept {n:,} titles   |  skipped {skipped}\")\n",
    "\n",
    "# ---------- embed --------------------------------------------------\n",
    "if OUT_NPY.exists() and OUT_IDX.exists():\n",
    "    print(\"âœ… embeddings already on disk â€“ skipping\")\n",
    "else:\n",
    "    print(f\"ğŸ”„ embedding {n:,} titles with {EMB_MODEL} â€¦\")\n",
    "    embeds = np.empty((n, 1536), dtype=np.float32)   # 1536-dim!\n",
    "\n",
    "    for i in tqdm.tqdm(range(0, n, CHUNK_N), ncols=90):\n",
    "        chunk = titles[i:i+CHUNK_N]\n",
    "        resp  = client.embeddings.create(model=EMB_MODEL, input=chunk)\n",
    "        vecs  = [r.embedding for r in resp.data]      # list-of-lists\n",
    "        embeds[i : i+len(chunk)] = vecs              # write exact slice\n",
    "\n",
    "    np.save(OUT_NPY, embeds)\n",
    "    np.save(OUT_IDX, np.asarray(idxs, dtype=np.int32))\n",
    "    print(f\"ğŸ’¾ saved {embeds.shape[0]:,} Ã— {embeds.shape[1]} matrix â†’ {OUT_NPY.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Quick pilot: score a small, time-stratified sample          â•‘\n",
    "# â•‘            (change SAMPLE_N as you wish)                    â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, time, json, openai, tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- parameters -------------------------------------------------------\n",
    "SAMPLE_N     = 200                         # 200 â†’ ~1 headline / 6 days\n",
    "SEED         = 42                          # reproducible random\n",
    "OUT_DIR      = ROOT / \"outputs\"\n",
    "OUT_CSV      = OUT_DIR / \"sample_headline_scores.csv\"\n",
    "MAX_TOKENS_R = 3                           # model only returns â€œ0â€ â€¦ â€œ10â€\n",
    "DELAY_SEC    = 0.7                         # â‰¤ 80 req/min on 4-o-mini\n",
    "\n",
    "# ---------- load filtered headline universe ----------------------------------\n",
    "DF_ALL = pd.read_csv(\"outputs/headlines_ru_ua.csv\", parse_dates=[\"date\"])\n",
    "DF_ALL = DF_ALL.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# ---------- even-time sampling -----------------------------------------------\n",
    "lin_idx  = np.linspace(0, len(DF_ALL)-1, SAMPLE_N, dtype=int)\n",
    "sample   = DF_ALL.loc[lin_idx].sample(frac=1, random_state=SEED)  # shuffle once\n",
    "\n",
    "print(f\"ğŸ¯ sampled {len(sample)} headlines  ({sample['date'].min().date()} â†’ {sample['date'].max().date()})\")\n",
    "\n",
    "# ---------- build gold-set exemplars for the system prompt -------------------\n",
    "GOLD_EX = [\n",
    "    (0,  \"Vessel to collect first humanitarian wheat shipment under Ukraine grain deal\"),\n",
    "    (1,  \"UN offers Putin SWIFT access in exchange for Black Sea grain extension\"),\n",
    "    (2,  \"Russia: Person injured in Ukrainian drone attack on Belgorod region\"),\n",
    "    (3,  \"Russian shelling kills Kherson residents, Zelenskyy denounces â€˜terrorist attacksâ€™\"),\n",
    "    (4,  \"Russia blames Ukraine drone attack for major Crimea fuel-depot fire\"),\n",
    "    (5,  \"U.S. Eyes Giving Ukraine Patriot Missile Defense Systems\"),\n",
    "    (6,  \"Ukraine launches â€˜main thrustâ€™ of counter-offensive, punches through Russian defences\"),\n",
    "    (7,  \"Russia unleashes country-wide missile barrage on Ukraine energy grid\"),\n",
    "    (8,  \"Putin blames Ukraine for Crimea Bridge blast, calls it a â€˜terrorist actâ€™\"),\n",
    "    (9,  \"Russia planning â€˜provocationsâ€™ at Zaporizhzhia nuclear plant, Zelensky warns\"),\n",
    "    (10, \"Russia threatens US nuclear war by Christmas, analysts say\"),\n",
    "]\n",
    "\n",
    "sys_lines = [\"You are an analyst scoring how *escalatory* a single headline is on a **0-10 integer scale**.\",\n",
    "             \"Examples:\"]\n",
    "for lvl, hl in GOLD_EX:\n",
    "    sys_lines.append(f\"{lvl} â†’ {hl}\")\n",
    "sys_lines.append(\"Return **only** the integer (0-10).\")\n",
    "SYSTEM_PROMPT = \"\\n\".join(sys_lines)\n",
    "\n",
    "# ---------- score sample ------------------------------------------------------\n",
    "client   = openai.OpenAI()\n",
    "scores   = []\n",
    "latency  = []\n",
    "\n",
    "pbar = tqdm.tqdm(sample.itertuples(index=False), total=len(sample), ncols=90, desc=\"Scoring\")\n",
    "for row in pbar:\n",
    "    user_msg = f\"[{row.date.date()}] {row.title}\"\n",
    "    t0       = time.time()\n",
    "    rsp      = client.chat.completions.create(\n",
    "                  model=\"gpt-o4-mini\",\n",
    "                  temperature=0,\n",
    "                  max_tokens=MAX_TOKENS_R,\n",
    "                  messages=[{\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "                            {\"role\":\"user\",  \"content\":user_msg}],\n",
    "              )\n",
    "    latency.append(time.time()-t0)\n",
    "    try:\n",
    "        s = int(rsp.choices[0].message.content.strip())\n",
    "    except ValueError:\n",
    "        s = np.nan                      # in the unlikely case the model slips\n",
    "    scores.append(s)\n",
    "    time.sleep(DELAY_SEC)\n",
    "\n",
    "sample = sample.assign(score=scores)\n",
    "\n",
    "print(f\"âœ… finished   mean latency = {np.mean(latency):.2f}s   NaNs = {sample['score'].isna().sum()}\")\n",
    "\n",
    "sample.to_csv(OUT_CSV, index=False)\n",
    "print(\"ğŸ’¾ wrote\", OUT_CSV.relative_to(Path.cwd()))\n",
    "\n",
    "# ---------- quick visual ------------------------------------------------------\n",
    "daily_small = (sample.groupby(sample[\"date\"].dt.date)[\"score\"]\n",
    "                        .mean()\n",
    "                        .rename(\"sample_mean\"))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(daily_small.index, daily_small, marker='o', lw=.8, alpha=.7)\n",
    "plt.title(f\"Pilot {SAMPLE_N}-headline escalation scores\")\n",
    "plt.ylabel(\"0 = low â€¦ 10 = high\")\n",
    "plt.grid(alpha=.3); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv(OUT_CSV, index=False)\n",
    "print(f\"ğŸ’¾ wrote {OUT_CSV}\")          # simpler & always works\n",
    "\n",
    "# ---------- quick visual ------------------------------------------------------\n",
    "daily_small = (sample.groupby(sample[\"date\"].dt.date)[\"score\"]\n",
    "                        .mean()\n",
    "                        .rename(\"sample_mean\"))\n",
    "\n",
    "TIMELINE   = Path(\"../src/ukraine-war-timeline.json\")          # JSONL\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(daily_small.index, daily_small, marker='o', lw=.8, alpha=.7)\n",
    "plt.title(f\"Pilot {SAMPLE_N}-headline escalation scores\")\n",
    "plt.ylabel(\"0 = low â€¦ 10 = high\")\n",
    "plt.grid(alpha=.3); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Save sample-scores CSV (already computed above)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "sample.to_csv(OUT_CSV, index=False)\n",
    "print(f\"ğŸ’¾ wrote {OUT_CSV}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Build per-day mean from the 200-headline pilot\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "daily_small = (sample\n",
    "               .groupby(sample[\"date\"].dt.date)[\"score\"]\n",
    "               .mean()\n",
    "               .rename(\"sample_mean\"))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Load major-event timeline (JSON-lines)  âœ list of dicts\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TIMELINE = Path(\"../src/ukraine-war-timeline.json\")      # adjust if needed\n",
    "events   = []\n",
    "with TIMELINE.open() as fh:\n",
    "    for ln in fh:\n",
    "        events.append(json.loads(ln))\n",
    "\n",
    "tl = (pd.DataFrame(events)\n",
    "        .query(\"major == True\")                          # keep only major\n",
    "        .assign(date = lambda d: pd.to_datetime(d[\"date\"])))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Plot: scatter + red verticals\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(daily_small.index, daily_small, marker='o', lw=.8, alpha=.8)\n",
    "\n",
    "for _, r in tl.iterrows():\n",
    "    plt.axvline(r[\"date\"], color=\"crimson\", lw=1.4, alpha=.8)\n",
    "    plt.text(r[\"date\"], 10.3, r.get(\"label\", \"\"), rotation=90,\n",
    "             ha=\"right\", va=\"bottom\", color=\"crimson\", fontsize=7)\n",
    "\n",
    "plt.title(f\"Pilot {SAMPLE_N}-headline escalation scores  +  major events\")\n",
    "plt.ylabel(\"0 = low â€¦ 10 = high\")\n",
    "plt.ylim(-0.5, 10.5)\n",
    "plt.grid(alpha=.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  7-day rolling mean  +  simple pre/post deltas               â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1) daily series & 7-day rolling mean\n",
    "# ---------------------------------------------------------------\n",
    "daily = (sample\n",
    "         .groupby(sample[\"date\"].dt.date)[\"score\"]\n",
    "         .mean()\n",
    "         .rename(\"daily_mean\")\n",
    "         .sort_index())\n",
    "\n",
    "roll7 = daily.rolling(7, min_periods=1).mean().rename(\"roll7\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "ax.plot(daily.index, roll7, lw=2, color=\"tab:red\", label=\"7-day mean\")\n",
    "ax.scatter(daily.index, daily, s=18, alpha=.6, label=\"daily mean\")\n",
    "for _, r in tl.iterrows():                         # red event lines already in tl\n",
    "    ax.axvline(r[\"date\"], color=\"crimson\", lw=.7, alpha=.6)\n",
    "ax.set_title(\"Daily & 7-day rolling escalation index   +  major events\")\n",
    "ax.set_ylabel(\"Escalation  (0 = low â€¦ 10 = high)\")\n",
    "ax.grid(alpha=.3); ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2) naive Â±7-day pre/post comparison around each *major* event\n",
    "# ---------------------------------------------------------------\n",
    "rows = []\n",
    "for _, ev in tl.iterrows():\n",
    "    d0   = ev[\"date\"].date()\n",
    "    pre  = daily.loc[(daily.index >= d0 - pd.Timedelta(days=7)) &\n",
    "                     (daily.index <  d0)]\n",
    "    post = daily.loc[(daily.index >  d0) &\n",
    "                     (daily.index <= d0 + pd.Timedelta(days=7))]\n",
    "    if len(pre) >= 3 and len(post) >= 3:           # need enough points\n",
    "        rows.append({\n",
    "            \"date\"     : d0,\n",
    "            \"label\"    : ev.get(\"label\",\"\"),\n",
    "            \"pre_mean\" : pre.mean(),\n",
    "            \"post_mean\": post.mean(),\n",
    "            \"delta\"    : post.mean() - pre.mean()\n",
    "        })\n",
    "\n",
    "if rows:                                           # -------- only if we have data\n",
    "    event_effects = (pd.DataFrame(rows)\n",
    "                       .sort_values(\"date\")\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "    display(event_effects.style.format({\"pre_mean\":\"{:.2f}\",\n",
    "                                        \"post_mean\":\"{:.2f}\",\n",
    "                                        \"delta\":\"{:+.2f}\"}))\n",
    "\n",
    "    n_pos = (event_effects[\"delta\"] >  0).sum()\n",
    "    n_neg = (event_effects[\"delta\"] <  0).sum()\n",
    "    print(f\"\\nğŸ“Š  Pre/post window (Â±7 d)   â†‘ delta: {n_pos}   â†“ delta: {n_neg}   \"\n",
    "          f\"zero/insufficient: {len(event_effects) - n_pos - n_neg}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No events have â‰¥3 pre- and post-day observations in this 200-headline pilot\\n\"\n",
    "          \"    â€” re-run after full scoring for meaningful statistics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SCORE all ~40 000 RUâ€“UA headlines with GPT-4.1-nano  (uses GOLD_EX) â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path, PurePath\n",
    "import json, time, datetime as dt, pandas as pd, openai, tqdm, math\n",
    "\n",
    "# â”€â”€ repo paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT       = Path.cwd().resolve().parents[0]\n",
    "HEAD_CSV   = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "CHUNK_DIR  = ROOT / \"outputs\" / \"score_chunks\"\n",
    "OUT_CSV    = ROOT / \"outputs\" / \"headline_scores_full.csv\"\n",
    "STATE_JSON = CHUNK_DIR / \"_progress.json\"\n",
    "\n",
    "# â”€â”€ model / batch params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL      = \"gpt-4o-mini\"      # \n",
    "MAX_REQ    = 8_000              # platform limit\n",
    "COMPL_WIN  = \"24h\"\n",
    "\n",
    "# â”€â”€ your exemplar rubric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GOLD_EX = \"\"\"\n",
    "You are a defence analyst scoring how *escalatory* ONE Ukraine-war-related headline is.\n",
    "\n",
    "Two example sets to establish scale (integer only):\n",
    "---\n",
    "```\n",
    "0  Vessel to collect first humanitarian wheat shipment under Ukraine grain deal\n",
    "1  UN Offers Putin SWIFT access in exchange for Black Sea grain extension, sources say\n",
    "2  Russia: Person injured in Ukrainian drone attack on Belgorod region\n",
    "3  Russian shelling kills Kherson residents, Zelenskyy denounces â€˜terrorist attacksâ€™\n",
    "4  Russia blames Ukraine drone attack for major Crimea fuel-depot fire â€“ Reuters\n",
    "5  U.S. Eyes Giving Ukraine Patriot Missile Defense Systems\n",
    "6  Ukraine launches â€˜main thrustâ€™ of counter-offensive, punches through Russian defenses\n",
    "7  Russia unleashes country-wide missile barrage on Ukraine as Putin addresses security conference\n",
    "8  Putin blames Ukraine for Crimea Bridge blast, calls it a â€˜terrorist actâ€™\n",
    "9  Russia planning â€˜provocationsâ€™ at nuclear plant, Zelensky tells Macron\n",
    "10 Russia threatens US nuclear war by Christmas? Hereâ€™s what we know of threats and possible responses\n",
    "```\n",
    "---\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"level\": 0,\n",
    "    \"headline\": \"How did Russia and the West agree to a prisoner exchange?\",\n",
    "    \"reason\": \"Purely diplomatic focus on a de-escalatory prisoner exchange agreement, with no active fighting reported. [cite: 82]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 1,\n",
    "    \"headline\": \"Russia bans access to over 80 Western media outlets\",\n",
    "    \"reason\": \"An act of information warfare (banning media outlets) directly related to the conflict, escalating tensions beyond diplomacy but without kinetic military action. [cite: 78]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 2,\n",
    "    \"headline\": \"Russia: Person injured in Ukrainian drone attack on Belgorod region\",\n",
    "    \"reason\": \"Reports an isolated drone attack causing a single injury, clearly fitting the 'low-level skirmish, few casualties' definition.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 3,\n",
    "    \"headline\": \"Russian strikes kill at least three in Kharkiv and injure dozens\",\n",
    "    \"reason\": \"Describes Russian strikes resulting in multiple deaths and dozens of injuries, significantly escalating beyond a low-level skirmish (L2) but not specified as a targeted logistics strike (L4).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 4,\n",
    "    \"headline\": \"Russia blames Ukraine drone attack for major Crimea fuel depot fire - Reuters\",\n",
    "    \"reason\": \"A clear example of a single strike on a key logistics target (fuel depot), matching the rubric's definition for this level.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 5,\n",
    "    \"headline\": \"Ukraine brings war to Russia in â€˜massiveâ€™ drone strike on military, government targets - Fox News\",\n",
    "    \"reason\": \"A 'massive' drone strike on multiple military and government targets inside Russia implies a coordinated attack of greater scale and impact than a single logistics strike (L4), but not a major theater-wide offensive (L6).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 6,\n",
    "    \"headline\": \"U.S. To Send â€œMassive Surgeâ€ Of Weapons To Ukraine\",\n",
    "    \"reason\": \"The term \\\"Massive Surge\\\" directly indicates a major shipment of weapons, fulfilling one of the criteria for this level (\\\"major weapons shipment\\\"). [cite: 94]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 7,\n",
    "    \"headline\": \"Russia unleashes country-wide missile barrage on Ukraine as Putin addresses security conference\",\n",
    "    \"reason\": \"A \\\"country-wide missile barrage\\\" signifies a broad and intense series of attacks aimed at overwhelming defenses or causing widespread disruption across Ukraine, an escalation beyond a standard major offensive (L6) and impacting national infrastructure/morale. [cite: 49]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 8,\n",
    "    \"headline\": \"Putin Blames Ukraine For Crimea Bridge Blast, Calls It A \\\"Terrorist Act\\\"\",\n",
    "    \"reason\": \"Directly refers to an attack on the Crimea Bridge, a key strategic infrastructure target explicitly mentioned as an example for this level of escalation in the rubric.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 9,\n",
    "    \"headline\": \"West criticizes Russia at U.N. for Ukraine war after missile crash in Poland\",\n",
    "    \"reason\": \"A missile incident in a NATO member state (Poland), causing fatalities and attributed to the conflict, represents a severe strategic escalation due to the high risk of wider conflict (e.g., NATO involvement), just short of explicit nuclear threats (L10).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 10,\n",
    "    \"headline\": \"Russia threatens US nuclear war by Christmas? Here's what we know of threats and possible responses\",\n",
    "    \"reason\": \"Explicitly reports on Russia making threats of nuclear war, fitting the 'nuclear rhetoric' criterion for the highest escalation level. [cite: 92]\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Output only the matching integer 0-10.\n",
    "\"\"\".strip()\n",
    "\n",
    "# â”€â”€ create temp dir if first run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CHUNK_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1) build / discover JSONL chunks (idempotent)                      #\n",
    "# ------------------------------------------------------------------ #\n",
    "df = pd.read_csv(HEAD_CSV, parse_dates=[\"date\"])\n",
    "if not STATE_JSON.exists():                        # first ever run\n",
    "    chunks, part, n = [], 1, 0\n",
    "    fh = (CHUNK_DIR / f\"new_score_part{part:02d}.jsonl\").open(\"w\")\n",
    "    for idx, row in df.iterrows():\n",
    "        task = {\n",
    "            \"custom_id\": str(idx),\n",
    "            \"method\":   \"POST\",\n",
    "            \"url\":      \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 3,\n",
    "                \"messages\": [\n",
    "                    {\"role\":\"system\", \"content\": GOLD_EX},\n",
    "                    {\"role\":\"user\",   \"content\": str(row[\"title\"])[:500]}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        fh.write(json.dumps(task) + \"\\n\")\n",
    "        n += 1\n",
    "        if n >= MAX_REQ:\n",
    "            fh.close(); chunks.append(fh.name)\n",
    "            part += 1; n = 0\n",
    "            fh = (CHUNK_DIR / f\"score_part{part:02d}.jsonl\").open(\"w\")\n",
    "    fh.close(); chunks.append(fh.name)\n",
    "\n",
    "    json.dump({\"chunks\": chunks, \"launched\": {}, \"done\": {}},\n",
    "              STATE_JSON.open(\"w\"), indent=2)\n",
    "    print(f\"ğŸ“ prepared {len(chunks)} JSONL chunks for scoring\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2) resume-safe batch launcher                                      #\n",
    "# ------------------------------------------------------------------ #\n",
    "state  = json.load(STATE_JSON.open())\n",
    "client = openai.OpenAI()\n",
    "\n",
    "for path in state[\"chunks\"]:\n",
    "    fname = PurePath(path).name\n",
    "    if fname in state[\"done\"]:                       # already finished\n",
    "        continue\n",
    "\n",
    "    if fname in state[\"launched\"]:                  # running / finalising\n",
    "        bid = state[\"launched\"][fname]\n",
    "    else:                                           # need to launch\n",
    "        fid  = client.files.create(file=open(path,\"rb\"), purpose=\"batch\").id\n",
    "        bid  = client.batches.create(\n",
    "                  input_file_id=fid,\n",
    "                  endpoint=\"/v1/chat/completions\",\n",
    "                  completion_window=COMPL_WIN).id\n",
    "        state[\"launched\"][fname] = bid\n",
    "        json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "        print(\"ğŸš€ launched\", bid, \"for\", fname)\n",
    "\n",
    "    bar = tqdm.tqdm(total=MAX_REQ, desc=f\"{fname} â†’ {bid[:8]}\", unit=\"req\")\n",
    "    while True:\n",
    "        b   = client.batches.retrieve(bid)\n",
    "        rc  = b.request_counts\n",
    "        bar.total = rc.total or bar.total\n",
    "        bar.n     = rc.completed + rc.failed\n",
    "        bar.refresh()\n",
    "        if b.status == \"completed\":\n",
    "            bar.close(); print(\"âœ…\", fname, \"done\")\n",
    "            state[\"done\"][fname] = bid\n",
    "            json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "            break\n",
    "        if b.status == \"failed\":\n",
    "            bar.close(); raise RuntimeError(f\"Batch {bid} failed\")\n",
    "        time.sleep(15)\n",
    "\n",
    "print(\"\\nğŸ‰ every batch completed â€“ assembling final CSV\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3) download & merge                                                #\n",
    "# ------------------------------------------------------------------ #\n",
    "scores = {}\n",
    "for fname, bid in state[\"done\"].items():\n",
    "    out_txt = client.files.content(\n",
    "                 client.batches.retrieve(bid).output_file_id).content\n",
    "    for ln in out_txt.splitlines():\n",
    "        rec = json.loads(ln)\n",
    "        try:\n",
    "            scores[int(rec[\"custom_id\"])] = int(\n",
    "                rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip())\n",
    "        except ValueError:\n",
    "            scores[int(rec[\"custom_id\"])] = math.nan\n",
    "\n",
    "df_out = df.assign(score = pd.Series(scores).astype(\"Int64\"))\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"ğŸ’¾ wrote {OUT_CSV.relative_to(ROOT.parent)}   \"\n",
    "      f\"(NaNs: {df_out['score'].isna().sum()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SCORE all ~40,000 RUâ€“UA headlines with Claude Haiku 3.5 (uses GOLD_EX) â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, time, datetime as dt, pandas as pd, tqdm, math\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# â”€â”€ repo paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT       = Path.cwd().resolve().parents[0]\n",
    "HEAD_CSV   = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "CHUNK_DIR  = ROOT / \"outputs\" / \"score_chunks_anthropic\"\n",
    "OUT_CSV    = ROOT / \"outputs\" / \"headline_scores_anthropic.csv\"\n",
    "STATE_JSON = CHUNK_DIR / \"_progress.json\"\n",
    "\n",
    "# â”€â”€ model / batch params â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MODEL      = \"claude-3-5-haiku-20241022\"  # Cost-effective for simple tasks\n",
    "MAX_REQ    = 10_000              # Conservative limit per batch (Anthropic allows up to 100k)\n",
    "\n",
    "# â”€â”€ your exemplar rubric â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GOLD_EX = \"\"\"\n",
    "You are a defence analyst scoring how *escalatory* ONE Ukraine-war-related headline is.\n",
    "\n",
    "Two example sets to establish scale (integer only):\n",
    "---\n",
    "```\n",
    "0  Vessel to collect first humanitarian wheat shipment under Ukraine grain deal\n",
    "1  UN Offers Putin SWIFT access in exchange for Black Sea grain extension, sources say\n",
    "2  Russia: Person injured in Ukrainian drone attack on Belgorod region\n",
    "3  Russian shelling kills Kherson residents, Zelenskyy denounces 'terrorist attacks'\n",
    "4  Russia blames Ukraine drone attack for major Crimea fuel-depot fire â€“ Reuters\n",
    "5  U.S. Eyes Giving Ukraine Patriot Missile Defense Systems\n",
    "6  Ukraine launches 'main thrust' of counter-offensive, punches through Russian defenses\n",
    "7  Russia unleashes country-wide missile barrage on Ukraine as Putin addresses security conference\n",
    "8  Putin blames Ukraine for Crimea Bridge blast, calls it a 'terrorist act'\n",
    "9  Russia planning 'provocations' at nuclear plant, Zelensky tells Macron\n",
    "10 Russia threatens US nuclear war by Christmas? Here's what we know of threats and possible responses\n",
    "```\n",
    "---\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"level\": 0,\n",
    "    \"headline\": \"How did Russia and the West agree to a prisoner exchange?\",\n",
    "    \"reason\": \"Purely diplomatic focus on a de-escalatory prisoner exchange agreement, with no active fighting reported. [cite: 82]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 1,\n",
    "    \"headline\": \"Russia bans access to over 80 Western media outlets\",\n",
    "    \"reason\": \"An act of information warfare (banning media outlets) directly related to the conflict, escalating tensions beyond diplomacy but without kinetic military action. [cite: 78]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 2,\n",
    "    \"headline\": \"Russia: Person injured in Ukrainian drone attack on Belgorod region\",\n",
    "    \"reason\": \"Reports an isolated drone attack causing a single injury, clearly fitting the 'low-level skirmish, few casualties' definition.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 3,\n",
    "    \"headline\": \"Russian strikes kill at least three in Kharkiv and injure dozens\",\n",
    "    \"reason\": \"Describes Russian strikes resulting in multiple deaths and dozens of injuries, significantly escalating beyond a low-level skirmish (L2) but not specified as a targeted logistics strike (L4).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 4,\n",
    "    \"headline\": \"Russia blames Ukraine drone attack for major Crimea fuel depot fire - Reuters\",\n",
    "    \"reason\": \"A clear example of a single strike on a key logistics target (fuel depot), matching the rubric's definition for this level.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 5,\n",
    "    \"headline\": \"Ukraine brings war to Russia in 'massive' drone strike on military, government targets - Fox News\",\n",
    "    \"reason\": \"A 'massive' drone strike on multiple military and government targets inside Russia implies a coordinated attack of greater scale and impact than a single logistics strike (L4), but not a major theater-wide offensive (L6).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 6,\n",
    "    \"headline\": \"U.S. To Send \"Massive Surge\" Of Weapons To Ukraine\",\n",
    "    \"reason\": \"The term \\\"Massive Surge\\\" directly indicates a major shipment of weapons, fulfilling one of the criteria for this level (\\\"major weapons shipment\\\"). [cite: 94]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 7,\n",
    "    \"headline\": \"Russia unleashes country-wide missile barrage on Ukraine as Putin addresses security conference\",\n",
    "    \"reason\": \"A \\\"country-wide missile barrage\\\" signifies a broad and intense series of attacks aimed at overwhelming defenses or causing widespread disruption across Ukraine, an escalation beyond a standard major offensive (L6) and impacting national infrastructure/morale. [cite: 49]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 8,\n",
    "    \"headline\": \"Putin Blames Ukraine For Crimea Bridge Blast, Calls It A \\\"Terrorist Act\\\"\",\n",
    "    \"reason\": \"Directly refers to an attack on the Crimea Bridge, a key strategic infrastructure target explicitly mentioned as an example for this level of escalation in the rubric.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 9,\n",
    "    \"headline\": \"West criticizes Russia at U.N. for Ukraine war after missile crash in Poland\",\n",
    "    \"reason\": \"A missile incident in a NATO member state (Poland), causing fatalities and attributed to the conflict, represents a severe strategic escalation due to the high risk of wider conflict (e.g., NATO involvement), just short of explicit nuclear threats (L10).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 10,\n",
    "    \"headline\": \"Russia threatens US nuclear war by Christmas? Here's what we know of threats and possible responses\",\n",
    "    \"reason\": \"Explicitly reports on Russia making threats of nuclear war, fitting the 'nuclear rhetoric' criterion for the highest escalation level. [cite: 92]\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Output only the matching integer 0-10.\n",
    "\"\"\".strip()\n",
    "\n",
    "# â”€â”€ create temp dir if first run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CHUNK_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1) build / discover batches (idempotent)                           #\n",
    "# ------------------------------------------------------------------ #\n",
    "df = pd.read_csv(HEAD_CSV, parse_dates=[\"date\"])\n",
    "if not STATE_JSON.exists():                        # first ever run\n",
    "    batches, batch_num, requests = [], 1, []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        request = {\n",
    "            \"custom_id\": str(idx),\n",
    "            \"params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"max_tokens\": 10,  # Just need a single digit\n",
    "                \"temperature\": 0,\n",
    "                \"system\": GOLD_EX,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": str(row[\"title\"])[:500]}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        requests.append(request)\n",
    "        \n",
    "        if len(requests) >= MAX_REQ:\n",
    "            batch_file = CHUNK_DIR / f\"batch_{batch_num:03d}.json\"\n",
    "            json.dump({\"requests\": requests}, batch_file.open(\"w\"))\n",
    "            batches.append(str(batch_file))\n",
    "            batch_num += 1\n",
    "            requests = []\n",
    "    \n",
    "    # Save remaining requests\n",
    "    if requests:\n",
    "        batch_file = CHUNK_DIR / f\"batch_{batch_num:03d}.json\"\n",
    "        json.dump({\"requests\": requests}, batch_file.open(\"w\"))\n",
    "        batches.append(str(batch_file))\n",
    "    \n",
    "    json.dump({\"batches\": batches, \"launched\": {}, \"done\": {}},\n",
    "              STATE_JSON.open(\"w\"), indent=2)\n",
    "    print(f\"ğŸ“ prepared {len(batches)} batch files for scoring\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2) resume-safe batch launcher                                      #\n",
    "# ------------------------------------------------------------------ #\n",
    "state = json.load(STATE_JSON.open())\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "for batch_path in state[\"batches\"]:\n",
    "    batch_name = Path(batch_path).name\n",
    "    if batch_name in state[\"done\"]:                 # already finished\n",
    "        continue\n",
    "    \n",
    "    if batch_name in state[\"launched\"]:            # running / finalizing\n",
    "        batch_id = state[\"launched\"][batch_name]\n",
    "    else:                                           # need to launch\n",
    "        # Load the batch data\n",
    "        with open(batch_path, 'r') as f:\n",
    "            batch_data = json.load(f)\n",
    "        \n",
    "        # Create the batch\n",
    "        batch = client.messages.batches.create(\n",
    "            requests=batch_data[\"requests\"]\n",
    "        )\n",
    "        batch_id = batch.id\n",
    "        state[\"launched\"][batch_name] = batch_id\n",
    "        json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "        print(f\"ğŸš€ launched {batch_id} for {batch_name}\")\n",
    "    \n",
    "    # Poll for completion\n",
    "    # First get the number of requests in this batch\n",
    "    with open(batch_path, 'r') as f:\n",
    "        batch_data = json.load(f)\n",
    "        num_requests = len(batch_data[\"requests\"])\n",
    "    \n",
    "    bar = tqdm.tqdm(total=num_requests, desc=f\"{batch_name} â†’ {batch_id[:8]}\", unit=\"req\")\n",
    "    while True:\n",
    "        batch = client.messages.batches.retrieve(batch_id)\n",
    "        \n",
    "        # Update progress\n",
    "        completed = (batch.request_counts.succeeded + batch.request_counts.errored + \n",
    "                    batch.request_counts.canceled + batch.request_counts.expired)\n",
    "        bar.n = completed\n",
    "        bar.refresh()\n",
    "        \n",
    "        if batch.processing_status == \"ended\":\n",
    "            bar.close()\n",
    "            print(f\"âœ… {batch_name} done\")\n",
    "            state[\"done\"][batch_name] = batch_id\n",
    "            json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "            break\n",
    "        \n",
    "        time.sleep(10)  # Check every 10 seconds\n",
    "\n",
    "print(\"\\nğŸ‰ every batch completed â€“ assembling final CSV\")\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 3) download & merge results                                        #\n",
    "# ------------------------------------------------------------------ #\n",
    "scores = {}\n",
    "for batch_name, batch_id in state[\"done\"].items():\n",
    "    batch = client.messages.batches.retrieve(batch_id)\n",
    "    \n",
    "    # Get results\n",
    "    if batch.results_url:\n",
    "        # Download results (this returns a string of JSONL data)\n",
    "        results_response = client._client.get(batch.results_url)\n",
    "        results_data = results_response.text\n",
    "        \n",
    "        # Process each line\n",
    "        for line in results_data.strip().split('\\n'):\n",
    "            if line:\n",
    "                result = json.loads(line)\n",
    "                custom_id = int(result[\"custom_id\"])\n",
    "                \n",
    "                if result[\"result\"][\"type\"] == \"succeeded\":\n",
    "                    try:\n",
    "                        # Extract the score from the message content\n",
    "                        content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "                        # Try to extract just the number\n",
    "                        score = int(''.join(c for c in content if c.isdigit()))\n",
    "                        if 0 <= score <= 10:\n",
    "                            scores[custom_id] = score\n",
    "                        else:\n",
    "                            scores[custom_id] = math.nan\n",
    "                    except (ValueError, KeyError, IndexError):\n",
    "                        scores[custom_id] = math.nan\n",
    "                else:\n",
    "                    scores[custom_id] = math.nan\n",
    "\n",
    "# Create output DataFrame\n",
    "df_out = df.assign(score=pd.Series(scores).astype(\"Int64\"))\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(f\"ğŸ’¾ wrote {OUT_CSV.relative_to(ROOT.parent)}   \"\n",
    "      f\"(NaNs: {df_out['score'].isna().sum()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  RESUME BATCH PROCESSING - Continue from where you left off           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, math\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# â”€â”€ repo paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT       = Path.cwd().resolve().parents[0]\n",
    "HEAD_CSV   = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "CHUNK_DIR  = ROOT / \"outputs\" / \"score_chunks_anthropic\"\n",
    "OUT_CSV    = ROOT / \"outputs\" / \"headline_scores_full.csv\"\n",
    "STATE_JSON = CHUNK_DIR / \"_progress.json\"\n",
    "\n",
    "# Model\n",
    "MODEL = \"claude-3-5-haiku-20241022\"\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Load state\n",
    "if not STATE_JSON.exists():\n",
    "    print(\"âŒ No state file found. Run the initial batch processing script first.\")\n",
    "    exit(1)\n",
    "\n",
    "state = json.load(STATE_JSON.open())\n",
    "\n",
    "# Check what's left to process\n",
    "remaining_batches = []\n",
    "for batch_path in state[\"batches\"]:\n",
    "    batch_name = Path(batch_path).name\n",
    "    if batch_name not in state[\"done\"]:\n",
    "        remaining_batches.append((batch_path, batch_name))\n",
    "\n",
    "print(f\"ğŸ“Š Processing Status:\")\n",
    "print(f\"   Total batches: {len(state['batches'])}\")\n",
    "print(f\"   Completed: {len(state['done'])}\")\n",
    "print(f\"   Remaining: {len(remaining_batches)}\")\n",
    "\n",
    "if not remaining_batches:\n",
    "    print(\"\\nâœ… All batches have been completed!\")\n",
    "    exit(0)\n",
    "\n",
    "print(f\"\\nğŸš€ Resuming processing of {len(remaining_batches)} remaining batches...\")\n",
    "\n",
    "# Process remaining batches\n",
    "for batch_path, batch_name in remaining_batches:\n",
    "    print(f\"\\nğŸ“¦ Processing {batch_name}...\")\n",
    "    \n",
    "    if batch_name in state[\"launched\"]:\n",
    "        # Check if this batch is still running or completed\n",
    "        batch_id = state[\"launched\"][batch_name]\n",
    "        try:\n",
    "            batch = client.messages.batches.retrieve(batch_id)\n",
    "            \n",
    "            if batch.processing_status == \"ended\":\n",
    "                # Batch completed while we were away\n",
    "                print(f\"   âœ… Batch already completed!\")\n",
    "                state[\"done\"][batch_name] = batch_id\n",
    "                json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "                continue\n",
    "            elif batch.processing_status == \"in_progress\":\n",
    "                print(f\"   â³ Batch still processing, waiting for completion...\")\n",
    "            else:\n",
    "                print(f\"   âŒ Batch in unexpected state: {batch.processing_status}\")\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error checking batch status: {str(e)}\")\n",
    "            print(f\"   ğŸ”„ Will try to relaunch this batch...\")\n",
    "            # Remove from launched so we can retry\n",
    "            del state[\"launched\"][batch_name]\n",
    "            json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "    \n",
    "    # Launch or wait for batch\n",
    "    if batch_name not in state[\"launched\"]:\n",
    "        # Need to launch this batch\n",
    "        try:\n",
    "            with open(batch_path, 'r') as f:\n",
    "                batch_data = json.load(f)\n",
    "            \n",
    "            batch = client.messages.batches.create(\n",
    "                requests=batch_data[\"requests\"]\n",
    "            )\n",
    "            batch_id = batch.id\n",
    "            state[\"launched\"][batch_name] = batch_id\n",
    "            json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "            print(f\"   ğŸš€ Launched new batch: {batch_id}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Error launching batch: {str(e)}\")\n",
    "            if \"rate_limit\" in str(e).lower():\n",
    "                print(\"   ğŸ’¡ Hit rate limit. You may need to wait or upgrade your plan.\")\n",
    "            elif \"insufficient_credit\" in str(e).lower() or \"payment\" in str(e).lower():\n",
    "                print(\"   ğŸ’³ Insufficient credits. Please add more credits to your account.\")\n",
    "                print(f\"   ğŸ“Š You've completed {len(state['done'])}/{len(state['batches'])} batches so far.\")\n",
    "                break\n",
    "            continue\n",
    "    else:\n",
    "        batch_id = state[\"launched\"][batch_name]\n",
    "    \n",
    "    # Wait for completion\n",
    "    with open(batch_path, 'r') as f:\n",
    "        batch_data = json.load(f)\n",
    "        num_requests = len(batch_data[\"requests\"])\n",
    "    \n",
    "    bar = tqdm.tqdm(total=num_requests, desc=f\"{batch_name} â†’ {batch_id[:8]}\", unit=\"req\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            batch = client.messages.batches.retrieve(batch_id)\n",
    "            \n",
    "            # Update progress\n",
    "            completed = (batch.request_counts.succeeded + batch.request_counts.errored + \n",
    "                        batch.request_counts.canceled + batch.request_counts.expired)\n",
    "            bar.n = completed\n",
    "            bar.refresh()\n",
    "            \n",
    "            if batch.processing_status == \"ended\":\n",
    "                bar.close()\n",
    "                print(f\"   âœ… Batch completed!\")\n",
    "                state[\"done\"][batch_name] = batch_id\n",
    "                json.dump(state, STATE_JSON.open(\"w\"), indent=2)\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            bar.close()\n",
    "            print(f\"   âŒ Error checking batch status: {str(e)}\")\n",
    "            break\n",
    "        \n",
    "        time.sleep(10)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nğŸ“Š Final Status:\")\n",
    "print(f\"   Completed: {len(state['done'])}/{len(state['batches'])} batches\")\n",
    "\n",
    "if len(state['done']) == len(state['batches']):\n",
    "    print(\"\\nğŸ‰ All batches completed! Running final assembly...\")\n",
    "    \n",
    "    # Import and run the retrieval script to get all results\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, \"retrieve_partial_results.py\"])\n",
    "else:\n",
    "    print(f\"\\nâ¸ï¸  Processing paused. {len(state['batches']) - len(state['done'])} batches remaining.\")\n",
    "    print(\"   Run this script again when you have more credits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  DEBUG RETRIEVE RESULTS - See what's in the batch responses           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, math\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# â”€â”€ repo paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT       = Path.cwd().resolve().parents[0]\n",
    "HEAD_CSV   = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "CHUNK_DIR  = ROOT / \"outputs\" / \"score_chunks_anthropic\"\n",
    "STATE_JSON = CHUNK_DIR / \"_progress.json\"\n",
    "PARTIAL_CSV = ROOT / \"outputs\" / \"headline_scores_partial.csv\"\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Load state\n",
    "state = json.load(STATE_JSON.open())\n",
    "print(f\"ğŸ“Š Found {len(state['done'])} completed batches\")\n",
    "\n",
    "# Let's debug the first batch\n",
    "if state[\"done\"]:\n",
    "    batch_name, batch_id = list(state[\"done\"].items())[0]\n",
    "    print(f\"\\nğŸ” Debugging batch: {batch_name} (ID: {batch_id})\")\n",
    "    \n",
    "    try:\n",
    "        # Retrieve batch\n",
    "        batch = client.messages.batches.retrieve(batch_id)\n",
    "        print(f\"âœ… Batch retrieved successfully\")\n",
    "        print(f\"   Status: {batch.processing_status}\")\n",
    "        print(f\"   Results URL: {batch.results_url}\")\n",
    "        \n",
    "        if batch.results_url:\n",
    "            # Method 1: Using requests with API key\n",
    "            print(\"\\nğŸ“¥ Attempting to download results...\")\n",
    "            headers = {\n",
    "                \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                \"anthropic-version\": \"2023-06-01\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(batch.results_url, headers=headers)\n",
    "            print(f\"   Response status: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Save raw response for inspection\n",
    "                debug_file = CHUNK_DIR / f\"debug_{batch_name}_results.txt\"\n",
    "                with open(debug_file, 'w') as f:\n",
    "                    f.write(response.text)\n",
    "                print(f\"   Saved raw response to: {debug_file}\")\n",
    "                \n",
    "                # Try to parse first few lines\n",
    "                lines = response.text.strip().split('\\n')\n",
    "                print(f\"\\nğŸ“„ Found {len(lines)} result lines\")\n",
    "                print(\"\\nğŸ” First 3 results (raw):\")\n",
    "                for i, line in enumerate(lines[:3]):\n",
    "                    print(f\"\\nLine {i}:\")\n",
    "                    print(line[:200] + \"...\" if len(line) > 200 else line)\n",
    "                    \n",
    "                    try:\n",
    "                        result = json.loads(line)\n",
    "                        print(f\"âœ… Parsed successfully\")\n",
    "                        print(f\"   Keys: {list(result.keys())}\")\n",
    "                        if 'result' in result:\n",
    "                            print(f\"   Result keys: {list(result['result'].keys())}\")\n",
    "                            print(f\"   Result type: {result['result'].get('type', 'N/A')}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"âŒ Parse error: {e}\")\n",
    "            else:\n",
    "                print(f\"âŒ Failed to download: {response.status_code}\")\n",
    "                print(f\"   Response: {response.text[:500]}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {type(e).__name__}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Now let's create a corrected retrieval function\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š ATTEMPTING FULL RETRIEVAL WITH CORRECTED PARSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores = {}\n",
    "total_processed = 0\n",
    "total_errors = 0\n",
    "\n",
    "for batch_name, batch_id in state[\"done\"].items():\n",
    "    print(f\"\\nğŸ“¦ Processing {batch_name}...\")\n",
    "    \n",
    "    try:\n",
    "        batch = client.messages.batches.retrieve(batch_id)\n",
    "        \n",
    "        if batch.results_url:\n",
    "            # Download results\n",
    "            headers = {\n",
    "                \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                \"anthropic-version\": \"2023-06-01\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(batch.results_url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                lines = response.text.strip().split('\\n')\n",
    "                batch_scores = 0\n",
    "                batch_errors = 0\n",
    "                \n",
    "                for line in lines:\n",
    "                    if line:\n",
    "                        try:\n",
    "                            data = json.loads(line)\n",
    "                            custom_id = int(data[\"custom_id\"])\n",
    "                            \n",
    "                            if data[\"result\"][\"type\"] == \"succeeded\":\n",
    "                                # Extract content from the message\n",
    "                                message = data[\"result\"][\"message\"]\n",
    "                                content = message[\"content\"][0][\"text\"].strip()\n",
    "                                \n",
    "                                # Extract just the number\n",
    "                                score_text = ''.join(c for c in content if c.isdigit())\n",
    "                                if score_text:\n",
    "                                    score = int(score_text)\n",
    "                                    if 0 <= score <= 10:\n",
    "                                        scores[custom_id] = score\n",
    "                                        batch_scores += 1\n",
    "                                    else:\n",
    "                                        scores[custom_id] = math.nan\n",
    "                                        batch_errors += 1\n",
    "                                else:\n",
    "                                    scores[custom_id] = math.nan\n",
    "                                    batch_errors += 1\n",
    "                            else:\n",
    "                                scores[custom_id] = math.nan\n",
    "                                batch_errors += 1\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            batch_errors += 1\n",
    "                            if \"custom_id\" in line:  # Only print if it looks like valid data\n",
    "                                print(f\"   âš ï¸ Parse error: {str(e)[:100]}\")\n",
    "                \n",
    "                print(f\"   âœ… Processed: {batch_scores} scores, {batch_errors} errors\")\n",
    "                total_processed += batch_scores\n",
    "                total_errors += batch_errors\n",
    "            else:\n",
    "                print(f\"   âŒ HTTP {response.status_code}: {response.text[:200]}\")\n",
    "        else:\n",
    "            print(f\"   âŒ No results URL available\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {type(e).__name__}: {str(e)}\")\n",
    "\n",
    "# Save results\n",
    "if scores:\n",
    "    print(f\"\\nğŸ“Š Total scores retrieved: {len(scores)}\")\n",
    "    print(f\"   Successful: {total_processed}\")\n",
    "    print(f\"   Errors: {total_errors}\")\n",
    "    \n",
    "    df = pd.read_csv(HEAD_CSV, parse_dates=[\"date\"])\n",
    "    df_out = df.assign(score=pd.Series(scores).astype(\"Int64\"))\n",
    "    df_out.to_csv(PARTIAL_CSV, index=False)\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Saved to: {PARTIAL_CSV.relative_to(ROOT.parent)}\")\n",
    "    print(f\"   Headlines scored: {df_out['score'].notna().sum()}/{len(df_out)}\")\n",
    "    \n",
    "    # Show distribution\n",
    "    print(\"\\nğŸ“Š Score distribution:\")\n",
    "    score_counts = df_out['score'].value_counts().sort_index()\n",
    "    for score, count in score_counts.items():\n",
    "        print(f\"   Level {score}: {count:,} headlines\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ No scores retrieved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## New Interchangeable Headline Scoring for Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  SCORE Ukraine War Headlines - Batch API Version                      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration - can change model\n",
    "ROOT = Path.cwd().resolve().parents[0]\n",
    "HEADLINES_CSV = ROOT / \"outputs\" / \"headlines_ru_ua.csv\"\n",
    "OUT_CSV = ROOT / \"outputs\" / \"headline_scores_anthropic_claude-sonnet-4-20250514.csv\"\n",
    "MODEL = \"claude-sonnet-4-20250514\"  # Can change to sonnet-4 or opus-4\n",
    "\n",
    "# Gold set exemplar rubric (exact same as original)\n",
    "GOLD_EX = \"\"\"You are a defence analyst scoring how *escalatory* ONE Ukraine-war-related headline is.\n",
    "\n",
    "Two example sets to establish scale (integer only):\n",
    "---\n",
    "```\n",
    "0  Vessel to collect first humanitarian wheat shipment under Ukraine grain deal\n",
    "1  UN Offers Putin SWIFT access in exchange for Black Sea grain extension, sources say\n",
    "2  Russia: Person injured in Ukrainian drone attack on Belgorod region\n",
    "3  Russian shelling kills Kherson residents, Zelenskyy denounces 'terrorist attacks'\n",
    "4  Russia blames Ukraine drone attack for major Crimea fuel-depot fire â€“ Reuters\n",
    "5  U.S. Eyes Giving Ukraine Patriot Missile Defense Systems\n",
    "6  Ukraine launches 'main thrust' of counter-offensive, punches through Russian defenses\n",
    "7  Russia unleashes country-wide missile barrage on Ukraine as Putin addresses security conference\n",
    "8  Putin blames Ukraine for Crimea Bridge blast, calls it a 'terrorist act'\n",
    "9  Russia planning 'provocations' at nuclear plant, Zelensky tells Macron\n",
    "10 Russia threatens US nuclear war by Christmas? Here's what we know of threats and possible responses\n",
    "```\n",
    "---\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"level\": 0,\n",
    "    \"headline\": \"How did Russia and the West agree to a prisoner exchange?\",\n",
    "    \"reason\": \"Purely diplomatic focus on a de-escalatory prisoner exchange agreement, with no active fighting reported. [cite: 82]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 1,\n",
    "    \"headline\": \"Russia bans access to over 80 Western media outlets\",\n",
    "    \"reason\": \"An act of information warfare (banning media outlets) directly related to the conflict, escalating tensions beyond diplomacy but without kinetic military action. [cite: 78]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 2,\n",
    "    \"headline\": \"Russia: Person injured in Ukrainian drone attack on Belgorod region\",\n",
    "    \"reason\": \"Reports an isolated drone attack causing a single injury, clearly fitting the 'low-level skirmish, few casualties' definition.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 3,\n",
    "    \"headline\": \"Russian strikes kill at least three in Kharkiv and injure dozens\",\n",
    "    \"reason\": \"Describes Russian strikes resulting in multiple deaths and dozens of injuries, significantly escalating beyond a low-level skirmish (L2) but not specified as a targeted logistics strike (L4).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 4,\n",
    "    \"headline\": \"Russia blames Ukraine drone attack for major Crimea fuel depot fire - Reuters\",\n",
    "    \"reason\": \"A clear example of a single strike on a key logistics target (fuel depot), matching the rubric's definition for this level.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 5,\n",
    "    \"headline\": \"Ukraine brings war to Russia in 'massive' drone strike on military, government targets - Fox News\",\n",
    "    \"reason\": \"A 'massive' drone strike on multiple military and government targets inside Russia implies a coordinated attack of greater scale and impact than a single logistics strike (L4), but not a major theater-wide offensive (L6).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 6,\n",
    "    \"headline\": \"U.S. To Send \"Massive Surge\" Of Weapons To Ukraine\",\n",
    "    \"reason\": \"The term \\\"Massive Surge\\\" directly indicates a major shipment of weapons, fulfilling one of the criteria for this level (\\\"major weapons shipment\\\"). [cite: 94]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 7,\n",
    "    \"headline\": \"Russia unleashes country-wide missile barrage on Ukraine as Putin addresses security conference\",\n",
    "    \"reason\": \"A \\\"country-wide missile barrage\\\" signifies a broad and intense series of attacks aimed at overwhelming defenses or causing widespread disruption across Ukraine, an escalation beyond a standard major offensive (L6) and impacting national infrastructure/morale. [cite: 49]\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 8,\n",
    "    \"headline\": \"Putin Blames Ukraine For Crimea Bridge Blast, Calls It A \\\"Terrorist Act\\\"\",\n",
    "    \"reason\": \"Directly refers to an attack on the Crimea Bridge, a key strategic infrastructure target explicitly mentioned as an example for this level of escalation in the rubric.\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 9,\n",
    "    \"headline\": \"West criticizes Russia at U.N. for Ukraine war after missile crash in Poland\",\n",
    "    \"reason\": \"A missile incident in a NATO member state (Poland), causing fatalities and attributed to the conflict, represents a severe strategic escalation due to the high risk of wider conflict (e.g., NATO involvement), just short of explicit nuclear threats (L10).\"\n",
    "  },\n",
    "  {\n",
    "    \"level\": 10,\n",
    "    \"headline\": \"Russia threatens US nuclear war by Christmas? Here's what we know of threats and possible responses\",\n",
    "    \"reason\": \"Explicitly reports on Russia making threats of nuclear war, fitting the 'nuclear rhetoric' criterion for the highest escalation level. [cite: 92]\"\n",
    "  }\n",
    "]\n",
    "```\n",
    "\n",
    "Output only the matching integer 0-10.\"\"\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(HEADLINES_CSV, parse_dates=[\"date\"])\n",
    "print(f\"ğŸ“Š Processing {len(df)} headlines\")\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Quiet the httpx + anthropic client\n",
    "for name in (\"httpx\", \"anthropic\"):\n",
    "    logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "# Add index for tracking\n",
    "df[\"batch_idx\"] = range(len(df))\n",
    "\n",
    "# Prepare batch requests\n",
    "requests_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row.get(\"title\")) or str(row[\"title\"]).strip() == \"\":\n",
    "        continue\n",
    "        \n",
    "    request = {\n",
    "        \"custom_id\": str(row[\"batch_idx\"]),\n",
    "        \"params\": {\n",
    "            \"model\": MODEL,\n",
    "            \"max_tokens\": 10,  # Just need a single digit 0-10\n",
    "            \"temperature\": 0,\n",
    "            \"system\": GOLD_EX,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": str(row[\"title\"])[:500]}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    requests_list.append(request)\n",
    "\n",
    "print(f\"ğŸ“ Prepared {len(requests_list)} requests\")\n",
    "\n",
    "# Create batch\n",
    "batch = client.messages.batches.create(requests=requests_list)\n",
    "print(f\"ğŸš€ Launched batch {batch.id}\")\n",
    "\n",
    "# Monitor progress\n",
    "bar = tqdm.tqdm(total=len(requests_list), desc=\"Processing\", unit=\"headline\")\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    batch_status = client.messages.batches.retrieve(batch.id)\n",
    "    completed = (batch_status.request_counts.succeeded + \n",
    "                batch_status.request_counts.errored + \n",
    "                batch_status.request_counts.canceled + \n",
    "                batch_status.request_counts.expired)\n",
    "    bar.n = completed\n",
    "    bar.refresh()\n",
    "    \n",
    "    if batch_status.processing_status == \"ended\":\n",
    "        bar.close()\n",
    "        break\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"âœ… Batch processing complete in {elapsed_time/60:.1f} minutes\")\n",
    "\n",
    "# Parse results\n",
    "scores = {}\n",
    "parse_errors = []\n",
    "\n",
    "# Retrieve the final batch status\n",
    "batch_final = client.messages.batches.retrieve(batch.id)\n",
    "\n",
    "if batch_final.results_url:\n",
    "    print(f\"ğŸ“¥ Fetching results from batch {batch.id}\")\n",
    "    \n",
    "    headers = {\n",
    "        \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "        \"anthropic-version\": \"2023-06-01\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(batch_final.results_url, headers=headers, stream=True)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Process JSONL results line by line\n",
    "        for line in response.iter_lines():\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                result = json.loads(line)\n",
    "                custom_id = result.get(\"custom_id\")\n",
    "                \n",
    "                if custom_id is None:\n",
    "                    continue\n",
    "                \n",
    "                idx = int(custom_id)\n",
    "                \n",
    "                # Check if request succeeded\n",
    "                if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                    parse_errors.append(f\"Request {custom_id} failed: {result.get('result', {}).get('type')}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract the response text\n",
    "                message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "                \n",
    "                # Parse the score - should be a single digit 0-10\n",
    "                # Try direct int conversion first\n",
    "                try:\n",
    "                    score = int(message_content)\n",
    "                    if 0 <= score <= 10:\n",
    "                        scores[idx] = score\n",
    "                    else:\n",
    "                        parse_errors.append(f\"Invalid score {score} for request {custom_id}\")\n",
    "                except ValueError:\n",
    "                    # If that fails, try extracting digits\n",
    "                    digits = re.findall(r'\\d+', message_content)\n",
    "                    if digits and 0 <= int(digits[0]) <= 10:\n",
    "                        scores[idx] = int(digits[0])\n",
    "                    else:\n",
    "                        parse_errors.append(f\"Could not parse score from: {message_content}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                parse_errors.append(f\"Error parsing result: {e}\")\n",
    "                continue\n",
    "                \n",
    "        print(f\"âœ… Successfully parsed {len(scores)} results\")\n",
    "    else:\n",
    "        print(f\"âŒ Error fetching results: HTTP {response.status_code}\")\n",
    "else:\n",
    "    print(\"âŒ No results URL available\")\n",
    "\n",
    "# Map scores back to dataframe\n",
    "df[\"score\"] = df[\"batch_idx\"].map(scores).astype(\"Int64\")\n",
    "\n",
    "# Save results\n",
    "df_out = df.drop(columns=[\"batch_idx\"])\n",
    "df_out.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nâœ… Scoring complete\")\n",
    "print(f\"   Total headlines: {len(df)}\")\n",
    "print(f\"   Successfully scored: {len(scores)}\")\n",
    "print(f\"   Failed: {len(df) - len(scores)}\")\n",
    "print(f\"   Success rate: {len(scores)/len(df)*100:.1f}%\")\n",
    "\n",
    "if parse_errors:\n",
    "    print(f\"\\nâš ï¸  Parse errors encountered ({len(parse_errors)} total):\")\n",
    "    for error in parse_errors[:5]:\n",
    "        print(f\"   - {error}\")\n",
    "    if len(parse_errors) > 5:\n",
    "        print(f\"   ... and {len(parse_errors) - 5} more errors\")\n",
    "\n",
    "# Show distribution of scores\n",
    "if len(scores) > 0:\n",
    "    print(\"\\nğŸ“Š Score distribution:\")\n",
    "    score_series = pd.Series(scores.values())\n",
    "    print(f\"   Mean: {score_series.mean():.2f}\")\n",
    "    print(f\"   Std: {score_series.std():.2f}\")\n",
    "    print(f\"   Range: {score_series.min()}-{score_series.max()}\")\n",
    "    \n",
    "    # Distribution by score\n",
    "    print(\"\\n   Score counts:\")\n",
    "    for score in range(11):\n",
    "        count = (score_series == score).sum()\n",
    "        pct = count / len(score_series) * 100\n",
    "        print(f\"   {score:2d}: {count:6d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“ Results saved to: {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
