{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tracking Ukraine-War Escalation in US Headlines (Feb 2022 → Apr 2025)\n",
    "**Goal** : build a **daily “escalation-risk” index** from mainstream English headlines about the Russia–Ukraine war, scoring each day on a 0-10 scale (0 = diplomacy, 10 = nuclear threat) using GPT-4o-mini.  \n",
    "We will later compare this index to sentiment on Reddit war-discussion subreddits to test whether grassroots discourse **anticipates, mirrors, or lags** mainstream coverage.\n",
    "\n",
    "Data pipeline (current PoC):\n",
    "1. Download Ukraine-related headlines via **NewsAPI**.  \n",
    "2. Cache `date | source | title` to CSV.  \n",
    "3. Prompt GPT-4o-mini with headline batches; receive a single 0-10 score per day.  \n",
    "4. Plot the raw series and 7-day rolling mean; save as `ukraine_escalation_daily.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, requests, pandas as pd, numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- .env loader that works in notebooks ------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def find_repo_root(start: Path, marker=\".git\") -> Path:\n",
    "    \"\"\"Walk up until we see a folder containing the given marker ('.git' or '.env').\"\"\"\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / marker).exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(f\"Repository root with {marker} not found from {start}\")\n",
    "\n",
    "# 1) locate repo root (folder that has .env **or** .git)\n",
    "repo_root = find_repo_root(Path.cwd(), \".env\")\n",
    "\n",
    "# 2) load environment variables\n",
    "load_dotenv(repo_root / \".env\")\n",
    "\n",
    "# 3) add src/ to Python path (optional, if you’ll import from src/)\n",
    "src_path = repo_root / \"src\"\n",
    "if src_path.exists():\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "# 4) fetch secrets (raise fast if any missing)\n",
    "REQUIRED = [\"OPENAI_API_KEY\", \"NEWSAPI_KEY\"]\n",
    "CREDS = {k: os.getenv(k) for k in REQUIRED}\n",
    "missing = [k for k, v in CREDS.items() if not v]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing secrets in .env: {', '.join(missing)}\")\n",
    "\n",
    "# handy variables\n",
    "OPENAI_KEY       = CREDS[\"OPENAI_API_KEY\"]\n",
    "NEWSAPI_KEY      = CREDS[\"NEWSAPI_KEY\"]\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_SECRET    = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_AGENT     = os.getenv(\"REDDIT_USER_AGENT\")\n",
    "\n",
    "print(f\"✅  .env loaded from {repo_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- configuration -------------------------------------------------\n",
    "start_date = datetime(2022, 2, 24)          # <∎  invasion day\n",
    "end_date   = datetime(2025, 4, 20)          # <∎  “now”\n",
    "\n",
    "# Pull EVERYTHING (no source/domain filter) that mentions Ukraine\n",
    "# We add a few spelling / city variants so 99 % of stories match\n",
    "base_query = (\n",
    "    'ukraine OR kyiv OR kiev OR lviv OR odessa OR donbas OR donbass '\n",
    "    'OR \"volodymyr zelensky\" OR zelenskyy OR putin OR russia OR invasion'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 1. Fetch & cache headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# 1.  Fetch & cache *all* Ukraine‑related headlines (24 Feb 2022 → Apr 20 2025)\n",
    "#      – pulls from EVERY English source in NewsAPI, paged day‑by‑day\n",
    "# ---------------------------------------------------------------\n",
    "import sys, time, urllib.parse, requests, pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "NEWSAPI_KEY = os.getenv(\"NEWSAPI_KEY\")        # be sure it is set\n",
    "assert NEWSAPI_KEY, \"➡️  set NEWSAPI_KEY env‑var first!\"\n",
    "\n",
    "# date range: full war period\n",
    "start_date = datetime(2022, 2, 24)            # invasion day\n",
    "end_date   = datetime(2025, 4, 20)            # “now”\n",
    "\n",
    "# broad query that catches > 99 % of RU–UA war pieces\n",
    "base_query = (\n",
    "    'ukraine OR kyiv OR kiev OR lviv OR odessa OR donbas OR donbass '\n",
    "    'OR \"volodymyr zelensky\" OR zelenskyy'\n",
    ")\n",
    "ENC_QUERY = urllib.parse.quote_plus(base_query)\n",
    "\n",
    "DAY_URL = (\n",
    "    'https://newsapi.org/v2/everything?'\n",
    "    'q={q}&from={f}&to={t}&language=en&sortBy=publishedAt&pageSize=100&page={pg}'\n",
    ")\n",
    "\n",
    "def daterange(start, end):\n",
    "    for n in range((end - start).days + 1):\n",
    "        yield start + timedelta(n)\n",
    "\n",
    "records = []\n",
    "total   = 0\n",
    "for day in daterange(start_date, end_date):\n",
    "    d_str = day.strftime('%Y-%m-%d')\n",
    "    next_str = (day + timedelta(1)).strftime('%Y-%m-%d')\n",
    "\n",
    "    pg = 1\n",
    "    while True:\n",
    "        url  = DAY_URL.format(q=ENC_QUERY, f=d_str, t=next_str, pg=pg)\n",
    "        resp = requests.get(url, headers={'X-Api-Key': NEWSAPI_KEY})\n",
    "        if resp.status_code != 200:\n",
    "            sys.stderr.write(f\"\\n⚠️  {d_str} page {pg} → {resp.status_code}: {resp.json().get('message','')}\\n\")\n",
    "            break\n",
    "\n",
    "        arts = resp.json().get('articles', [])\n",
    "        if not arts:\n",
    "            break\n",
    "\n",
    "        for a in arts:\n",
    "            records.append({\n",
    "                \"date\"  : d_str,\n",
    "                \"source\": a[\"source\"][\"id\"] or \"unknown\",\n",
    "                \"title\" : a[\"title\"]\n",
    "            })\n",
    "        total += len(arts)\n",
    "        if len(arts) < 100:                      # last page\n",
    "            break\n",
    "        pg += 1\n",
    "        time.sleep(0.3)                          # stay well under 30 req/min\n",
    "\n",
    "    sys.stdout.write(f\"\\r{d_str} ✓ {total:,} headlines so far\"); sys.stdout.flush()\n",
    "\n",
    "print(f\"\\nDone! Collected {total:,} headlines.\")\n",
    "df = pd.DataFrame(records).drop_duplicates()\n",
    "df.to_csv(\"raw_headlines.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Helper — write ONE jsonl file that contains 1 task per calendar day\n",
    "# ================================================================\n",
    "import json, pathlib, datetime as dt, pandas as pd, numpy as np, openai\n",
    "\n",
    "def build_tasks_jsonl(date_start:str,\n",
    "                      date_end  :str,\n",
    "                      out_path  :str,\n",
    "                      csv_path  =\"raw_headlines.csv\",\n",
    "                      max_today =120,      # headline cap per day\n",
    "                      max_look  =30):      # how many prev headlines to feed\n",
    "    \"\"\"\n",
    "    Build the JSONL needed for the 'per‑day escalation index' prompt.\n",
    "    One task = one calendar day between date_start and date_end (inclusive).\n",
    "    - date strings: 'YYYY‑MM‑DD'\n",
    "    - out_path: filename to create\n",
    "    \"\"\"\n",
    "    # ---- Load headlines & construct dict {date: [list of titles]} -----------\n",
    "    df = pd.read_csv(csv_path, parse_dates=[\"date\"])\n",
    "    day_mask = (df[\"date\"]>=date_start) & (df[\"date\"]<=date_end)\n",
    "    daily_titles = (df.loc[day_mask]\n",
    "                      .groupby(\"date\")[\"title\"]\n",
    "                      .apply(list)\n",
    "                      .sort_index())\n",
    "\n",
    "    all_days = daily_titles.index.to_list()\n",
    "\n",
    "    # ---- system prompt once -----------------------------------------------\n",
    "    sys_prompt = (\"\"\"\n",
    "    You are an analyst quantifying RUSSIA–UKRAINE military-escalation RISK.\n",
    "    \n",
    "    Scale (integer only):\n",
    "    0  = No active fighting; purely diplomatic headlines\n",
    "    2  = Low-level skirmishes, no major offensives\n",
    "    4  = Noticeable escalation (large drone / missile strikes)\n",
    "    6  = Major battlefield offensive OR significant weapons shipment\n",
    "    8  = Strategic escalation (Crimea bridge hit, use of banned weapons)\n",
    "    10 = Nuclear rhetoric, nuclear forces on alert, or actual WMD use\n",
    "    \n",
    "    Output ONE integer from 0-10.\n",
    "    \n",
    "    Examples\n",
    "    DATE: 2022-02-24\n",
    "    HEADLINES: Russia invades Ukraine, tanks cross border … → 10\n",
    "    \n",
    "    DATE: 2022-07-22\n",
    "    HEADLINES: Grain-export deal signed in Istanbul → 2\n",
    "    \"\"\")\n",
    "\n",
    "    out = pathlib.Path(out_path).open(\"w\")\n",
    "\n",
    "    # ---- iterate day → task -----------------------------------------------\n",
    "    for day in all_days:\n",
    "        today_list = daily_titles.loc[day][:max_today]\n",
    "\n",
    "        prev_slice = [d for d in all_days if (day-dt.timedelta(days=3) <= d < day)]\n",
    "        prev_list  = sum((daily_titles.loc[d] for d in prev_slice), [])[:max_look]\n",
    "\n",
    "        user_block = (\n",
    "            f\"### Headlines TODAY ({day.date()})\\n\"\n",
    "            + \"\\n\".join(f\"- {h}\" for h in today_list) + \"\\n\\n\"\n",
    "            f\"### Headlines PREVIOUS 3 days\\n\"\n",
    "            + \"\\n\".join(f\"- {h}\" for h in prev_list)\n",
    "        )\n",
    "\n",
    "        task = {\n",
    "            \"custom_id\": str(day.date()),          # easy key later\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 3,\n",
    "                \"messages\": [\n",
    "                    {\"role\":\"system\", \"content\": sys_prompt},\n",
    "                    {\"role\":\"user\",   \"content\": user_block}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        out.write(json.dumps(task) + \"\\n\")\n",
    "\n",
    "    out.close()\n",
    "    print(f\"✅ Wrote {len(all_days)} day‑tasks → {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Launch ≤2 M‑token batches sequentially\n",
    "# ================================================================\n",
    "import time, datetime as dt, openai, pathlib, json\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "date_slices = [                        # adjust if you like\n",
    "    (\"2022-02-24\", \"2022-12-31\"),\n",
    "    (\"2023-01-01\", \"2023-12-31\"),\n",
    "    (\"2024-01-01\", \"2024-12-31\"),\n",
    "    (\"2025-01-01\", \"2025-04-20\"),\n",
    "]\n",
    "\n",
    "launched_ids = []\n",
    "\n",
    "for idx, (d0, d1) in enumerate(date_slices, 1):\n",
    "    fname = f\"daily_escalation_tasks_{idx}.jsonl\"\n",
    "    build_tasks_jsonl(d0, d1, fname)                # <- now defined!\n",
    "\n",
    "    file_id = client.files.create(file=open(fname, \"rb\"), purpose=\"batch\").id\n",
    "    batch   = client.batches.create(\n",
    "                input_file_id=file_id,\n",
    "                endpoint=\"/v1/chat/completions\",\n",
    "                completion_window=\"24h\")\n",
    "    launched_ids.append(batch.id)\n",
    "    print(f\"\\n📤  [{idx}/{len(date_slices)}] Batch {batch.id} submitted — validating\")\n",
    "\n",
    "    # ----- poll every 20 s until finished -----------------------\n",
    "    while True:\n",
    "        b = client.batches.retrieve(batch.id)\n",
    "        c = b.request_counts\n",
    "        ts = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\r   {ts}  {b.status:<10}  {c.completed:>5}/{c.total}  (failed {c.failed})\",\n",
    "              end=\"\", flush=True)\n",
    "        if b.status in (\"completed\", \"failed\", \"expired\"):\n",
    "            print()     # newline\n",
    "            if b.status != \"completed\":\n",
    "                raise RuntimeError(f\"Batch {batch.id} ended with {b.status}\")\n",
    "            break\n",
    "        time.sleep(20)\n",
    "\n",
    "print(\"\\n🎉  All slices done.  Batch IDs →\", launched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  A. Launch (or re-launch) the 4th/last batch only            ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import datetime as dt, time, json, openai, pathlib\n",
    "\n",
    "client = openai.OpenAI()\n",
    "d0, d1   = \"2025-01-01\", \"2025-04-20\"\n",
    "fname    = \"daily_escalation_tasks_4.jsonl\"\n",
    "\n",
    "# — build / overwrite the JSONL for this slice —\n",
    "build_tasks_jsonl(d0, d1, fname)\n",
    "\n",
    "# — upload + create batch —\n",
    "file_id = client.files.create(file=open(fname, \"rb\"), purpose=\"batch\").id\n",
    "batch4  = client.batches.create(\n",
    "            input_file_id     = file_id,\n",
    "            endpoint          = \"/v1/chat/completions\",\n",
    "            completion_window = \"24h\")\n",
    "print(\"🆕  Batch-4 ID:\", batch4.id, \"— validating\")\n",
    "\n",
    "# — poll until done; auto-retry on transient connection hiccups —\n",
    "while True:\n",
    "    try:\n",
    "        b = client.batches.retrieve(batch4.id)\n",
    "        c = b.request_counts\n",
    "        ts = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\r{ts}  {b.status:<10}  {c.completed}/{c.total}  (failed {c.failed})\",\n",
    "              end=\"\", flush=True)\n",
    "        if b.status in (\"completed\", \"failed\", \"expired\"):\n",
    "            print()                       # newline\n",
    "            if b.status != \"completed\":\n",
    "                raise RuntimeError(f\"Batch-4 ended with status {b.status}\")\n",
    "            break\n",
    "        time.sleep(25)\n",
    "    except Exception as e:                # e.g. ReadTimeout / ConnectError\n",
    "        print(\"\\n⚠️  transient error:\", type(e).__name__, \"-- retrying in 30 s\")\n",
    "        time.sleep(30)\n",
    "\n",
    "print(\"✅  Batch-4 finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Download results & assemble daily index\n",
    "# ------------------------------------------------------------------\n",
    "content = client.files.content(b.output_file_id).content\n",
    "scores  = {}\n",
    "for line in content.splitlines():\n",
    "    rec  = json.loads(line)\n",
    "    day  = rec[\"custom_id\"][2:]               # strip \"d-\"\n",
    "    s    = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    try:\n",
    "        scores[pd.to_datetime(day)] = int(s)\n",
    "    except ValueError:\n",
    "        scores[pd.to_datetime(day)] = np.nan\n",
    "\n",
    "daily_idx = (pd.Series(scores)\n",
    "               .sort_index()\n",
    "               .to_frame(\"escalation\"))\n",
    "\n",
    "# 7‑day rolling mean\n",
    "daily_idx[\"roll7\"] = daily_idx[\"escalation\"].rolling(7, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 5.  Plot\n",
    "# ------------------------------------------------------------------\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(daily_idx.index, daily_idx[\"escalation\"],\n",
    "         marker='.', lw=0.8, alpha=0.6, label=\"Daily index\")\n",
    "plt.plot(daily_idx.index, daily_idx[\"roll7\"],\n",
    "         color=\"tab:red\", lw=2.5, label=\"7‑day rolling mean\")\n",
    "\n",
    "plt.ylabel(\"Escalation index  (0 = lower … 10 = higher)\")\n",
    "plt.title(\"Per‑day ‘Escalation‑Risk’ Signal in Ukraine‑War Headlines\")\n",
    "plt.grid(True, alpha=.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Merge ALL 4 batches   →   full escalation index 2022-02-24 … 2025-04-20\n",
    "# ------------------------------------------------------------------\n",
    "import json, pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime as dt, openai, collections\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# <<<—— put your four finished batch IDs here ———————————————————\n",
    "BATCH_IDS = [\n",
    "    \"batch_6807f1772ca88190b2ad0fa77855d67a\",   # 2022 slice\n",
    "    \"batch_6807f1ca7a508190a4292c2a721ab9c2\",   # 2023 slice\n",
    "    \"batch_6807f4e2abc881908e73abd7270a0c22\",   # 2024 slice\n",
    "    \"batch_680a7d198fd08190b8493d3abf137548\",   # 2025-Q1 slice  << new\n",
    "]\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "all_scores = {}          # {datetime : int}\n",
    "for bid in BATCH_IDS:\n",
    "    b = client.batches.retrieve(bid)\n",
    "    assert b.status == \"completed\", f\"{bid} not completed (status={b.status})\"\n",
    "    print(f\"⬇️  downloading {bid} …\")\n",
    "    data = client.files.content(b.output_file_id).content\n",
    "    for line in data.splitlines():\n",
    "        rec = json.loads(line)\n",
    "        day = pd.to_datetime(rec[\"custom_id\"])          # YYYY-MM-DD\n",
    "        s   = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        try:\n",
    "            all_scores[day] = int(s)\n",
    "        except ValueError:\n",
    "            all_scores[day] = np.nan\n",
    "\n",
    "print(f\"✅ merged {len(all_scores):,} daily scores\")\n",
    "\n",
    "# — build tidy DataFrame —\n",
    "ser = (pd.Series(all_scores, name=\"escalation\")\n",
    "         .sort_index())\n",
    "df  = ser.to_frame()\n",
    "df[\"roll7\"] = df[\"escalation\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "df.to_csv(\"ukraine_escalation_daily.csv\")\n",
    "print(\"📝 wrote ukraine_escalation_daily.csv\")\n",
    "\n",
    "# — quick sanity-check of the new distribution —\n",
    "print(\"\\nScore distribution (value_counts):\")\n",
    "print(df[\"escalation\"].value_counts().sort_index())\n",
    "\n",
    "# — plot —\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(df.index, df[\"escalation\"], lw=.8, alpha=.6,\n",
    "         marker='.', markersize=3, label=\"Daily index\")\n",
    "plt.plot(df.index, df[\"roll7\"], color=\"tab:red\", lw=2.5,\n",
    "         label=\"7-day rolling mean\")\n",
    "plt.ylabel(\"Escalation risk   (0 = low … 10 = much higher)\")\n",
    "plt.title(\"LLM-derived Ukraine Escalation Index  (full war period)\")\n",
    "plt.grid(alpha=.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Inspect the 2025-only slice that used the new prompt\n",
    "# ---------------------------------------------------------------\n",
    "import json, pandas as pd, numpy as np, matplotlib.pyplot as plt, openai, datetime as dt\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = openai.OpenAI()\n",
    "\n",
    "BATCH4_ID = \"batch_680a7d198fd08190b8493d3abf137548\"   #  ◀︎ paste the real ID\n",
    "\n",
    "# ── download & parse ────────────────────────────────────────────\n",
    "b = client.batches.retrieve(BATCH4_ID)\n",
    "assert b.status == \"completed\", f\"batch-4 not completed (status={b.status})\"\n",
    "\n",
    "data = client.files.content(b.output_file_id).content\n",
    "scores = {}\n",
    "for ln in data.splitlines():\n",
    "    rec  = json.loads(ln)\n",
    "    day  = pd.to_datetime(rec[\"custom_id\"])\n",
    "    s    = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    try:\n",
    "        scores[day] = int(s)\n",
    "    except ValueError:\n",
    "        scores[day] = np.nan\n",
    "\n",
    "ser = (pd.Series(scores, name=\"escalation\")\n",
    "         .sort_index())\n",
    "df  = ser.to_frame()\n",
    "df[\"roll7\"] = df[\"escalation\"].rolling(7, min_periods=1).mean()\n",
    "\n",
    "print(\"Value-counts for 2025 slice:\")\n",
    "print(df[\"escalation\"].value_counts().sort_index())\n",
    "\n",
    "# ── plot ────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(df.index, df[\"escalation\"], marker='.', lw=.8, alpha=.6,\n",
    "         label=\"Daily index\")\n",
    "plt.plot(df.index, df[\"roll7\"], color=\"tab:red\", lw=2.2,\n",
    "         label=\"7-day rolling mean\")\n",
    "plt.ylabel(\"Escalation risk   (0 = low … 10 = much higher)\")\n",
    "plt.title(\"Ukraine-War Escalation Index — 2025-Jan-01 → Apr-20  (new prompt)\")\n",
    "plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, os, pprint\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "for b in client.batches.list().data:          # no status filter available\n",
    "    if b.status in {\"in_progress\", \"validating\"}:\n",
    "        print(\"cancelling\", b.id, b.status)\n",
    "        client.batches.cancel(b.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Build day-level JSONL(s)  •  launch batches  •  analyse     ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import os, json, math, time, pathlib, datetime as dt\n",
    "import pandas as pd, numpy as np, openai\n",
    "\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 1.  Load cached headlines  → list-of-titles per day\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "df = (pd.read_csv(\"raw_headlines.csv\", parse_dates=[\"date\"])\n",
    "        .drop_duplicates(subset=[\"date\", \"title\"]))\n",
    "\n",
    "daily_titles = (df.groupby(\"date\")[\"title\"]\n",
    "                  .apply(list)               # list of headlines\n",
    "                  .sort_index())             # 1 152 rows\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 2.  Rich 0-10 rubric (system prompt) + 10 exemplars\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "\n",
    "You are a veteran conflict-desk editor and media analyst rating how much today's HEADLINES\n",
    "(together, on average) suggest an escalation in Russia-Ukraine military risk. \n",
    "\n",
    "Rate TODAY’s overall escalation-risk signal in the Russia-Ukraine war on a 0-10 integer scale, \n",
    "using the following examples as your reference guide:\n",
    "\n",
    "**How to score**  \n",
    "0  — Status-quo calm / clear signs of de-escalation  \n",
    "  *Ex.*  UN aid convoy reaches Kherson unimpeded for second day, agencies say fighting “silent”  \n",
    "1  — Small de-escalatory move (prisoner swap, minor corridor reopened)  \n",
    "  *Ex.*  200 Russian & Ukrainian soldiers freed in surprise prisoner-exchange brokered by UAE  \n",
    "2  — Cease-fire or peace-talk gesture, but fragile or localised  \n",
    "  *Ex.*  Turkey offers to host new Russia-Ukraine peace talks as Erdoğan meets Zelensky  \n",
    "3  — Routine frontline shelling & rhetoric; no notable change vs. recent days  \n",
    "  *Ex.*  UN warns Black-Sea grain deal at risk after Russia pauses participation for inspection row  \n",
    "4  — Limited strike / drone attack away from front lines; casualties low  \n",
    "  *Ex.*  Russian drones hit Kyiv power grid overnight, no deaths reported, officials say  \n",
    "5  — Large, coordinated missile/drone barrage OR major battlefield push  \n",
    "  *Ex.*  Kremlin launches 40-missile wave across 11 Ukrainian cities after rail-depot blast  \n",
    "6  — Strategic shift (partial mobilisation, new weapons promised, major aid cut-off)  \n",
    "  *Ex.*  Putin announces “partial mobilisation” of 300 000 reservists for Ukraine war effort  \n",
    "7  — Widest attack since war’s start OR deliberate strike on NATO-bordering region  \n",
    "  *Ex.*  Ukraine says Russia fires 90 missiles in biggest attack on Kyiv & Lviv since February ’22  \n",
    "8  — Open nuclear or WW-3 rhetoric / forward deployment of tactical nukes  \n",
    "  *Ex.*  Putin says Russia will station tactical nuclear weapons in Belarus this summer  \n",
    "9  — Nuclear warheads physically moved / command-and-control escalations; NATO emergency meet  \n",
    "  *Ex.*  Kremlin transfers nuclear warheads to Belarus; NATO calls urgent summit in Brussels  \n",
    "10 — Actual nuclear use order or confirmed detonation / direct NATO-Russia clash announced  \n",
    "  *Ex.*  Moscow conducts “live tactical nuclear strike drill”; White House vows “severe response”\n",
    "\n",
    "Return **only** the integer (0-10).  No extra words, no units.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 3.  Rough token sizing  → split into two chunks (< 2 M tokens each)\n",
    "#    • max 80 headlines fed per day              (trimmed below)\n",
    "#    • assume 13 tokens / headline after packing  (safe side)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "MAX_HEADLINES   = 120            # truncate per-day list\n",
    "TOKENS_PER_HEAD = 15             # safe upper-bound after packing\n",
    "TOKENS_SYS      = 80             # prompt, wrappers, misc\n",
    "TOKENS_DAY      = TOKENS_SYS + MAX_HEADLINES * TOKENS_PER_HEAD   # ≈ 1 880\n",
    "TOKENS_LIMIT    = 1_000_000      # keep a cushion under 2 M\n",
    "\n",
    "DAYS_PER_FILE   = TOKENS_LIMIT // TOKENS_DAY      # ≈ 850\n",
    "\n",
    "# -- slice the 1 152-row Series into blocks of ~850 days -------------\n",
    "day_blocks = [\n",
    "    daily_titles.iloc[i : i + DAYS_PER_FILE]\n",
    "    for i in range(0, len(daily_titles), DAYS_PER_FILE)\n",
    "]                                   # → 2 files of 850 & 302 days\n",
    "\n",
    "jsonl_paths = []\n",
    "\n",
    "for i, block in enumerate(day_blocks, 1):\n",
    "    fname = f\"ua_daily_tasks_{i}.jsonl\"\n",
    "    with open(fname, \"w\") as fh:\n",
    "        for day, titles in block.items():\n",
    "            user_block = \"### Headlines TODAY\\n\" + \\\n",
    "                         \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])\n",
    "\n",
    "            task = {\n",
    "                \"custom_id\": str(day.date()),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"temperature\": 0,\n",
    "                    \"max_tokens\": 3,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\",   \"content\": user_block}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            fh.write(json.dumps(task) + \"\\n\")\n",
    "    jsonl_paths.append(fname)\n",
    "    print(f\"✅ wrote {len(block):,} day-tasks → {fname}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 4.  Launch one batch per JSONL, poll until each finishes\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "batch_ids = []\n",
    "\n",
    "for p in jsonl_paths:\n",
    "    fid   = client.files.create(file=open(p, \"rb\"), purpose=\"batch\").id\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=fid,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "    )\n",
    "    batch_ids.append(batch.id)\n",
    "    print(f\"\\n📤 Batch {batch.id} → validating\")\n",
    "\n",
    "    # simple poll loop\n",
    "    while True:\n",
    "        b  = client.batches.retrieve(batch.id)\n",
    "        rc = b.request_counts\n",
    "        done  = rc.completed + rc.failed\n",
    "        total = rc.total\n",
    "        ts = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"\\r {ts} {b.status:<10} {done}/{total}\", end=\"\", flush=True)\n",
    "        if b.status in {\"completed\", \"failed\", \"expired\"}:\n",
    "            print()\n",
    "            if b.status != \"completed\":\n",
    "                raise RuntimeError(\"batch ended:\", b.status)\n",
    "            break\n",
    "        time.sleep(10)\n",
    "\n",
    "print(\"\\n🎉  All daily batches finished →\", batch_ids)\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 5.  Download scores  →  daily DataFrame\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "scores = {}\n",
    "for bid in batch_ids:\n",
    "    out = client.batches.retrieve(bid).output_file_id\n",
    "    for ln in client.files.content(out).content.splitlines():\n",
    "        rec = json.loads(ln)\n",
    "        day = pd.to_datetime(rec[\"custom_id\"])\n",
    "        try:\n",
    "            scores[day] = int(rec[\"response\"][\"body\"][\"choices\"][0]\n",
    "                                [\"message\"][\"content\"].strip())\n",
    "        except ValueError:\n",
    "            scores[day] = np.nan\n",
    "\n",
    "daily = (pd.Series(scores, name=\"score\")\n",
    "           .sort_index()\n",
    "           .to_frame())\n",
    "\n",
    "daily[\"roll7\"] = daily[\"score\"].rolling(7, min_periods=1).mean()\n",
    "daily.to_csv(\"ukraine_escalation_daily_v2.csv\")\n",
    "print(\"📝 wrote ukraine_escalation_daily_v2.csv\")\n",
    "\n",
    "# quick distribution check\n",
    "print(\"\\nValue counts:\")\n",
    "print(daily[\"score\"].value_counts().sort_index())\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# 6.  Plot\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(daily.index, daily[\"score\"], lw=.8, alpha=.6,\n",
    "         marker='.', markersize=3, label=\"Daily score\")\n",
    "plt.plot(daily[\"roll7\"], color=\"tab:red\", lw=2.5, label=\"7-day rolling mean\")\n",
    "plt.ylabel(\"Escalation risk  (0 = low … 10 = higher)\")\n",
    "plt.title(\"LLM-derived Ukraine Escalation Index  (Feb 2022 → Apr 2025)\")\n",
    "plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# helper: create a batch & poll; auto-retry on token-limit error\n",
    "# ---------------------------------------------------------------\n",
    "def launch_batch(file_id):\n",
    "    backoff = 90\n",
    "    while True:\n",
    "        # ---- create ---------------------------------------------------------\n",
    "        try:\n",
    "            batch = client.batches.create(\n",
    "                input_file_id=file_id,\n",
    "                endpoint=\"/v1/chat/completions\",\n",
    "                completion_window=\"24h\",\n",
    "            )\n",
    "        except openai.BadRequestError as e:\n",
    "            if \"enqueued token limit reached\" in str(e):\n",
    "                print(f\"🔄 token envelope busy — sleeping {backoff}s …\")\n",
    "                time.sleep(backoff)\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "        # ---- poll -----------------------------------------------------------\n",
    "        print(f\"\\n📤 Batch {batch.id} → validating\")\n",
    "        while True:\n",
    "            b  = client.batches.retrieve(batch.id)\n",
    "            rc = b.request_counts\n",
    "            ts = dt.datetime.now().strftime(\"%H:%M:%S\")\n",
    "            print(f\"\\r {ts} {b.status:<10} {rc.completed}/{rc.total or '?'}\",\n",
    "                  end=\"\", flush=True)\n",
    "            if b.status == \"completed\":\n",
    "                print()\n",
    "                return b                        # success\n",
    "            if b.status == \"failed\":\n",
    "                # ───── automatic token-limit retry ───────────────\n",
    "                if \"enqueued token limit\" in (b.message or \"\").lower():\n",
    "                    print(f\"\\n⚠️  token envelope still hot — sleeping {backoff}s \"\n",
    "                          \"and retrying same file …\")\n",
    "                    time.sleep(backoff)\n",
    "                    break                       # exit poll loop ⇒ outer while retries\n",
    "                raise RuntimeError(f\"batch {b.id} ended: failed\")\n",
    "            time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  A.  Load + plot *completed* batch_1                         ║\n",
    "# ║  B.  Queue leftover days in safer ≤350-day batches           ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import os, json, time, datetime as dt, pathlib\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "import openai\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# A.  Download results from the *finished* batch and plot\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "BATCH1_ID = \"batch_680a9515e434819086bd06e8b10354ac\"            # ⬅︎ your success ID\n",
    "\n",
    "out_id  = client.batches.retrieve(BATCH1_ID).output_file_id\n",
    "lines   = client.files.content(out_id).content.splitlines()\n",
    "\n",
    "daily1 = {}\n",
    "for ln in lines:\n",
    "    rec = json.loads(ln)\n",
    "    day = pd.to_datetime(rec[\"custom_id\"])\n",
    "    val = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    try:\n",
    "        daily1[day] = int(val)\n",
    "    except ValueError:\n",
    "        daily1[day] = np.nan\n",
    "\n",
    "ser1 = (pd.Series(daily1).sort_index()\n",
    "         .rename(\"score\"))\n",
    "ser1_7 = ser1.rolling(7, min_periods=1).mean()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ser1.index, ser1, marker='.', lw=.8, alpha=.6,\n",
    "         label=\"Daily score\")\n",
    "plt.plot(ser1.index, ser1_7, color=\"tab:red\", lw=2.2,\n",
    "         label=\"7-day rolling mean\")\n",
    "plt.title(\"Ukraine Escalation Index — first 531 days (batch 1)\")\n",
    "plt.ylabel(\"Escalation risk  (0 … 10)\")\n",
    "plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"✅ plotted\", len(ser1), \"days\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# B.  Re-batch the *remaining* days in ≤350-day chunks\n",
    "#     (≈ 350 × 1 600 ≈ 560 k tokens per batch — comfortably < 2 M)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "all_titles = (pd.read_csv(\"raw_headlines.csv\", parse_dates=[\"date\"])\n",
    "                .drop_duplicates(subset=[\"date\", \"title\"])\n",
    "                .groupby(\"date\")[\"title\"]\n",
    "                .apply(list)\n",
    "                .sort_index())                 # 1 152 total days\n",
    "\n",
    "done_days   = set(ser1.index.date)             # from batch 1\n",
    "\n",
    "# --- build mask with NumPy ------------------------------------\n",
    "mask = ~np.isin(all_titles.index.date, list(done_days))\n",
    "todo_series = all_titles[mask]\n",
    "print(\"⏩  remaining days to score:\", len(todo_series))\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "# B.  Re-batch leftover days in ≤200-day slices  (~375 k tokens) \n",
    "# ────────────────────────────────────────────────────────────────\n",
    "CHUNK_DAYS   = 200         # ↓ 350 → 200\n",
    "COOLDOWN_SEC = 75          # wait after each batch finishes\n",
    "\n",
    "\n",
    "# seed done_days with *all* batches that have already finished\n",
    "ALREADY_COMPLETE = [\n",
    "    \"batch_680a9515e434819086bd06e8b10354ac\",   # 531-day master\n",
    "    \"batch_680a97f72b5c819098c99c763d6da397\",   # first 350-day slice\n",
    "    \"batch_680a9fb4fb088190b2525a8b97b027ac\",   # recent 200-day slice ✅\n",
    "]\n",
    "\n",
    "done_days = set(ser1.index.date)                # first 531 days\n",
    "\n",
    "for bid in ALREADY_COMPLETE:\n",
    "    out_id = client.batches.retrieve(bid).output_file_id\n",
    "    for ln in client.files.content(out_id).content.splitlines():\n",
    "        d = pd.to_datetime(json.loads(ln)[\"custom_id\"]).date()\n",
    "        done_days.add(d)\n",
    "\n",
    "def remaining_days(scored_dates):\n",
    "    # recompute every time so reruns work incrementally\n",
    "    mask = ~np.isin(all_titles.index.date, list(scored_dates))\n",
    "    return all_titles[mask]\n",
    "\n",
    "todo_series = remaining_days(done_days)\n",
    "print(\"⏩  remaining days to score:\", len(todo_series))\n",
    "\n",
    "left_batch_ids = []\n",
    "\n",
    "while len(todo_series):\n",
    "    block        = todo_series.iloc[:CHUNK_DAYS]\n",
    "    block_no     = len(left_batch_ids) + 1\n",
    "    fname        = f\"ua_daily_tasks_left_{block_no}.jsonl\"\n",
    "    \n",
    "    # ---------- write JSONL ---------------------------------------------------\n",
    "    with open(fname, \"w\") as fh:\n",
    "        for day, titles in block.items():\n",
    "            task = {\n",
    "                \"custom_id\": str(day.date()),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"temperature\": 0,\n",
    "                    \"max_tokens\": 3,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                        {\"role\": \"user\",   \"content\": \n",
    "                           \"### Headlines TODAY\\n\" +\n",
    "                           \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "            fh.write(json.dumps(task) + \"\\n\")\n",
    "    print(f\"✅ wrote {len(block):,} day-tasks → {fname}\")\n",
    "\n",
    "    # ---------- launch & poll -------------------------------------------------\n",
    "    # upload file once\n",
    "    fid = client.files.create(file=open(fname, \"rb\"), purpose=\"batch\").id\n",
    "\n",
    "    # launch with safety wrapper\n",
    "    completed_batch = launch_batch(fid)\n",
    "    left_batch_ids.append(completed_batch.id)\n",
    "\n",
    "    # mark these days as done & recalc remainder\n",
    "    done_days.update(block.index.date)\n",
    "    todo_series = remaining_days(done_days)\n",
    "\n",
    "print(\"\\n🎉  All remaining batches finished →\", left_batch_ids)\n",
    "\n",
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  C.  Download + merge *all* scores, save & re-plot           ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "scores_rest = {}\n",
    "for bid in left_batch_ids:\n",
    "    out = client.batches.retrieve(bid).output_file_id\n",
    "    for ln in client.files.content(out).content.splitlines():\n",
    "        rec = json.loads(ln)\n",
    "        day = pd.to_datetime(rec[\"custom_id\"])\n",
    "        try:\n",
    "            scores_rest[day] = int(\n",
    "                rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            )\n",
    "        except ValueError:\n",
    "            scores_rest[day] = np.nan\n",
    "\n",
    "ser_rest = (pd.Series(scores_rest).sort_index()\n",
    "              .rename(\"score\"))\n",
    "\n",
    "# combine with the first-batch series `ser1` from section A\n",
    "full = (pd.concat([ser1, ser_rest])\n",
    "          .sort_index()\n",
    "          .rename(\"score\"))\n",
    "\n",
    "full_7 = full.rolling(7, min_periods=1).mean()\n",
    "full.to_csv(\"ukraine_escalation_daily_v2.csv\")\n",
    "print(f\"📝 wrote ukraine_escalation_daily_v2.csv  ({len(full)} days)\")\n",
    "\n",
    "# fresh plot\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(full.index, full, marker='.', lw=.8, alpha=.6, label=\"Daily score\")\n",
    "plt.plot(full.index, full_7, color=\"tab:red\", lw=2.5, label=\"7-day rolling mean\")\n",
    "plt.title(\"LLM-derived Ukraine Escalation Index  (Feb 2022 → Apr 2025)\")\n",
    "plt.ylabel(\"Escalation risk  (0 = low … 10 = high)\")\n",
    "plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  D.  Finish the last 71 days with plain chat completions     ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import time, json, pandas as pd, numpy as np, datetime as dt\n",
    "\n",
    "# -- rebuild scored series from *all* completed batches -----------\n",
    "COMPLETED_BATCHES = [\n",
    "    \"batch_680a9515e434819086bd06e8b10354ac\",   # 531-day\n",
    "    \"batch_680a97f72b5c819098c99c763d6da397\",   # 350-day\n",
    "    \"batch_680a9fb4fb088190b2525a8b97b027ac\",   # 200-day\n",
    "]\n",
    "\n",
    "scored = {}           # day → score\n",
    "for bid in COMPLETED_BATCHES:\n",
    "    out_id = client.batches.retrieve(bid).output_file_id\n",
    "    for ln in client.files.content(out_id).content.splitlines():\n",
    "        rec  = json.loads(ln)\n",
    "        day  = pd.to_datetime(rec[\"custom_id\"])\n",
    "        val  = rec[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        try:\n",
    "            scored[day] = int(val)\n",
    "        except ValueError:\n",
    "            scored[day] = np.nan\n",
    "\n",
    "# -- identify remaining days (should be 71) -----------------------\n",
    "done_days   = set(d.date() for d in scored.keys())\n",
    "mask        = ~np.isin(all_titles.index.date, list(done_days))\n",
    "todo_series = all_titles[mask]\n",
    "print(\"⚙️  days left:\", len(todo_series))\n",
    "\n",
    "# -- call chat completions one-by-one ------------------------------\n",
    "scores_tail = {}\n",
    "for i, (day, titles) in enumerate(todo_series.items(), 1):\n",
    "    user_block = \"### Headlines TODAY\\n\" + \\\n",
    "                 \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])\n",
    "\n",
    "    rsp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "        max_tokens=3,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",   \"content\": user_block},\n",
    "        ],\n",
    "    )\n",
    "    score = int(rsp.choices[0].message.content.strip())\n",
    "    scores_tail[pd.to_datetime(day)] = score\n",
    "    print(f\"{i:02}/{len(todo_series)} {day.date()} → {score}\")\n",
    "    time.sleep(0.6)            # ≤100 req/min safety cushion\n",
    "\n",
    "print(\"✅ direct calls finished\")\n",
    "\n",
    "# -- merge everything & save --------------------------------------\n",
    "ser_done  = pd.Series(scored,      name=\"score\")\n",
    "ser_tail  = pd.Series(scores_tail, name=\"score\")\n",
    "full      = pd.concat([ser_done, ser_tail]).sort_index()\n",
    "\n",
    "full_7 = full.rolling(7, min_periods=1).mean()\n",
    "full.to_csv(\"ukraine_escalation_daily_v2.csv\")\n",
    "print(\"📝 wrote ukraine_escalation_daily_v2.csv  (\", len(full), \"days)\")\n",
    "\n",
    "# -- plot ---------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(full.index, full, marker='.', lw=.8, alpha=.6, label=\"Daily score\")\n",
    "plt.plot(full.index, full_7, color=\"tab:red\", lw=2.5, label=\"7-day rolling mean\")\n",
    "plt.title(\"LLM-derived Ukraine Escalation Index  (Feb 2022 → Apr 2025)\")\n",
    "plt.ylabel(\"Escalation risk  (0 = low … 10 = high)\")\n",
    "plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- merge everything & save --------------------------------------\n",
    "ser_done  = pd.Series(scored,      name=\"score\")\n",
    "ser_tail  = pd.Series(scores_tail, name=\"score\")\n",
    "full      = pd.concat([ser_done, ser_tail]).sort_index()\n",
    "\n",
    "full_7 = full.rolling(7, min_periods=1).mean()\n",
    "full.to_csv(\"ukraine_escalation_daily_v2.csv\")\n",
    "print(\"📝 wrote ukraine_escalation_daily_v2.csv  (\", len(full), \"days)\")\n",
    "\n",
    "# -- plot ---------------------------------------------------------\n",
    "import time, json, pandas as pd, numpy as np, datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.plot(full.index, full, marker='.', lw=.8, alpha=.6, label=\"Daily score\")\n",
    "plt.plot(full.index, full_7, color=\"tab:red\", lw=2.5, label=\"7-day rolling mean\")\n",
    "plt.title(\"LLM-derived Ukraine Escalation Index  (Feb 2022 → Apr 2025)\")\n",
    "plt.ylabel(\"Escalation risk  (0 = low … 10 = high)\")\n",
    "plt.grid(alpha=.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "plt.savefig(\"fig_escalation7d.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Richer views of the 0-10 series\n",
    "\n",
    "### What the next cell shows\n",
    "1. Histogram & annual box-plots – are some years genuinely “hotter”?\n",
    "\n",
    "2. 30-day z-score (standardised anomaly) – highlights departures from the local mean; spikes often line up with mobilisation / large aid packages.\n",
    "\n",
    "3. Signal‐change heat-map – each cell = day-to-day Δscore; reds = jumps, blues = dips.\n",
    "\n",
    "4. Rolling volatility – 14-day stdev exposes periods of uncertainty even when the mean is flat.\n",
    "\n",
    "*Why it matters*\n",
    "\n",
    "- A stable headline score but rising volatility flags fog-of-war periods (e.g., Kharkiv counter-offensive rumours).\n",
    "\n",
    "- Z-scores strip out long-run drift: you can overlay them on external series (oil prices, aid deliveries) without scale headaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Visual deep-dive into Escalation Index                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. histogram + yearly boxes -----------------------------------\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,4))\n",
    "\n",
    "# -- histogram --\n",
    "full.plot.hist(bins=11, rwidth=.8, ax=ax[0])\n",
    "ax[0].set_title(\"Score histogram\"); ax[0].set_xlabel(\"Score\"); ax[0].set_ylabel(\"Days\")\n",
    "\n",
    "# -- yearly box-plots --\n",
    "full_df = full.to_frame(name=\"score\")\n",
    "full_df[\"year\"] = full_df.index.year          # helper col\n",
    "full_df.boxplot(column=\"score\", by=\"year\", ax=ax[1], showfliers=False)\n",
    "\n",
    "ax[1].set_title(\"Year-by-year distribution\")\n",
    "ax[1].set_xlabel(\"Year\"); ax[1].set_ylabel(\"Score\")\n",
    "plt.suptitle(\"\")                              # remove automatic suptitle\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 2. rolling z-score --------------------------------------------\n",
    "z = (full - full.rolling(30, min_periods=15).mean()) / \\\n",
    "    full.rolling(30, min_periods=15).std()\n",
    "\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.plot(z.index, z, lw=1)\n",
    "plt.axhline(0, c='k', lw=.7); plt.axhline(2, c='r', ls=\"--\")\n",
    "plt.title(\"30-day z-score (de-seasoned anomalies)\"); plt.ylabel(\"σ\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 3. heat-map of daily changes ----------------------------------\n",
    "delta = full.diff().fillna(0).clip(-3, 3)     # limit extreme arrows\n",
    "\n",
    "# ---- pad to multiple of 52 weeks ----\n",
    "n_days   = len(delta)\n",
    "n_cols   = 52                         # one column per week\n",
    "n_rows   = int(np.ceil(n_days / n_cols))\n",
    "pad_len  = n_rows * n_cols - n_days\n",
    "\n",
    "padded   = np.concatenate([delta.values, np.full(pad_len, np.nan)])\n",
    "mat      = padded.reshape(n_rows, n_cols)\n",
    "\n",
    "plt.figure(figsize=(14, 2.8))\n",
    "plt.imshow(mat, cmap=\"bwr\", aspect=\"auto\", vmin=-3, vmax=3)\n",
    "plt.colorbar(label=\"Δ score (day-to-day)\")\n",
    "plt.yticks(range(n_rows),\n",
    "           [(full.index[0] + pd.Timedelta(weeks=i)).strftime(\"%Y-W%V\")\n",
    "            for i in range(n_rows)])\n",
    "plt.title(\"Day-to-day jump heat-map (reds ↑, blues ↓)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# 4. rolling 14-day volatility ----------------------------------\n",
    "vol = full.rolling(14, min_periods=7).std()\n",
    "\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.plot(vol.index, vol, color=\"orange\")\n",
    "plt.title(\"14-day volatility of escalation score\"); plt.ylabel(\"σ\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Results Comparison with GPT 3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "client = OpenAI()\n",
    "MODEL       = \"gpt-3.5-turbo-0125\"\n",
    "TIMEOUT_SEC = 40\n",
    "MAX_RETRIES = 5\n",
    "BACKOFF_SEC = 10\n",
    "SPACING_SEC = 0.7  # keep < 90 req/min\n",
    "\n",
    "print(\"🚀 Starting script\", flush=True)\n",
    "\n",
    "try:\n",
    "    print(\"⚡️ Warm-up ping…\", flush=True)\n",
    "    # Warm-up ping\n",
    "    client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        temperature=0,\n",
    "        max_tokens=1,\n",
    "        messages=[{\"role\":\"user\",\"content\":\"ping\"}],\n",
    "        timeout=TIMEOUT_SEC\n",
    "    )\n",
    "    print(\"✅ Ping OK\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ping failed: {e}\", flush=True)\n",
    "    raise\n",
    "\n",
    "print(f\"🔢 Loaded all_titles: {len(all_titles)} days\", flush=True)\n",
    "\n",
    "scores35 = {}\n",
    "latencies = []\n",
    "\n",
    "total_days = len(all_titles)\n",
    "day_index = 0\n",
    "\n",
    "for day, titles in all_titles.items():\n",
    "    day_index += 1\n",
    "    print(f\"\\n[{day_index}/{total_days}] Processing {day.date()}\")\n",
    "\n",
    "    user_block = \"### Headlines TODAY\\n\" + \"\\n\".join(\n",
    "        f\"- {t}\" for t in titles[:MAX_HEADLINES]\n",
    "    )\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            rsp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                temperature=0,\n",
    "                max_tokens=3,\n",
    "                timeout=TIMEOUT_SEC,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\",   \"content\": user_block},\n",
    "                ],\n",
    "            )\n",
    "            latency = time.time() - t0\n",
    "            latencies.append(latency)\n",
    "            score = int(rsp.choices[0].message.content.strip())\n",
    "            scores35[pd.to_datetime(day)] = score\n",
    "            print(f\" → Success (latency {latency:.2f}s): score = {score}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == 1:\n",
    "                print(f\" ⚠️  {day.date()} attempt {attempt} error: {type(e).__name__}: {e}\")\n",
    "            if attempt == MAX_RETRIES:\n",
    "                print(f\" ❌  {day.date()} failed after {MAX_RETRIES} attempts; storing NaN\")\n",
    "                scores35[pd.to_datetime(day)] = np.nan\n",
    "            else:\n",
    "                backoff = BACKOFF_SEC * attempt\n",
    "                print(f\"    retrying in {backoff}s...\")\n",
    "                time.sleep(backoff)\n",
    "\n",
    "    time.sleep(SPACING_SEC)\n",
    "\n",
    "print(f\"\\n✅ Run done – mean latency {np.nanmean(latencies):.2f}s  |  failures {sum(pd.isna(list(scores35.values())))}\")\n",
    "\n",
    "# ---- compare to 4-o-mini --------------------------------------\n",
    "turbo = pd.Series(scores35).sort_index()\n",
    "df_cmp = pd.DataFrame({\"mini\": full, \"turbo\": turbo}).dropna()\n",
    "rho   = df_cmp.mini.corr(df_cmp.turbo).round(3)\n",
    "med_d = (df_cmp.mini - df_cmp.turbo).abs().median()\n",
    "\n",
    "print(f\"\\nPearson ρ = {rho}   |   median |Δ| = {med_d}\")\n",
    "\n",
    "ax = df_cmp.plot.scatter(\"mini\", \"turbo\", alpha=0.35, figsize=(4, 4))\n",
    "ax.plot([0, 10], [0, 10], \"r--\", lw=1)\n",
    "ax.set_xlabel(\"GPT-4o-mini score\")\n",
    "ax.set_ylabel(\"3.5-turbo score\")\n",
    "ax.set_title(\"Daily escalation scores: 4-o-mini vs 3.5-turbo\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Results Comparison with Claude-3-Haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Cheap benchmark — Claude-3-Haiku vs. GPT-4o-mini            ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "from tqdm.auto import tqdm\n",
    "import anthropic, time, pandas as pd, numpy as np, matplotlib.pyplot as plt, datetime as dt\n",
    "\n",
    "# ---------- configuration ----------------------------------------------------\n",
    "MODEL_ID      = \"claude-3-haiku-20240307\"    # explicit snapshot id\n",
    "TIMEOUT_SEC   = 40\n",
    "MAX_RETRIES   = 5\n",
    "BACKOFF_SEC   = 10\n",
    "SPACING_SEC   = 0.6                          # 100 req/min soft limit for Haiku\n",
    "MAX_TOKENS_OUT= 3                            # we only need the integer score\n",
    "\n",
    "client = anthropic.Anthropic()               # picks up ANTHROPIC_API_KEY from env\n",
    "\n",
    "# ---------- warm-up ping ------------------------------------------------------\n",
    "print(\"⚡️ Haiku warm-up ping…\", flush=True)\n",
    "_ = client.messages.create(\n",
    "        model      = MODEL_ID,\n",
    "        max_tokens = 1,\n",
    "        system     = \"ping-test\",\n",
    "        messages   = [{\"role\":\"user\",\"content\":\"ping\"}],\n",
    "        timeout    = TIMEOUT_SEC,\n",
    ")\n",
    "print(\"✅ ping OK\\n\", flush=True)\n",
    "\n",
    "# ---------- 5-day smoke-test --------------------------------------------------\n",
    "print(\"★ 5-day smoke-test (Claude 3 Haiku)\")\n",
    "for d, titles in list(all_titles.items())[:5]:\n",
    "    block = \"### Headlines TODAY\\n\" + \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])\n",
    "    rsp   = client.messages.create(\n",
    "              model      = MODEL_ID,\n",
    "              max_tokens = MAX_TOKENS_OUT,\n",
    "              system     = SYSTEM_PROMPT,\n",
    "              messages   = [{\"role\":\"user\",\"content\": block}],\n",
    "              timeout    = TIMEOUT_SEC,\n",
    "            )\n",
    "    score = rsp.content[0].text.strip()\n",
    "    print(f\"{d.date()} → {score}\")\n",
    "print(\"★ smoke-test OK — starting full loop\\n\")\n",
    "\n",
    "# ---------- full run ----------------------------------------------------------\n",
    "scores_haiku, latencies = {}, []\n",
    "pbar = tqdm(total=len(all_titles), desc=\"Haiku scoring\", ncols=100)\n",
    "\n",
    "for day, titles in all_titles.items():\n",
    "    user_block = \"### Headlines TODAY\\n\" + \"\\n\".join(f\"- {t}\" for t in titles[:MAX_HEADLINES])\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES+1):\n",
    "        try:\n",
    "            t0  = time.time()\n",
    "            rsp = client.messages.create(\n",
    "                    model      = MODEL_ID,\n",
    "                    max_tokens = MAX_TOKENS_OUT,\n",
    "                    system     = SYSTEM_PROMPT,\n",
    "                    messages   = [{\"role\":\"user\",\"content\": user_block}],\n",
    "                    timeout    = TIMEOUT_SEC,\n",
    "                  )\n",
    "            latencies.append(time.time() - t0)\n",
    "            scores_haiku[pd.to_datetime(day)] = int(rsp.content[0].text.strip())\n",
    "            break                                    # success\n",
    "        except Exception as e:\n",
    "            if attempt == 1:\n",
    "                pbar.write(f\"⚠️ {day.date()} err: {type(e).__name__}: {e}\")\n",
    "            if attempt == MAX_RETRIES:\n",
    "                pbar.write(f\"❌ {day.date()} → NaN\")\n",
    "                scores_haiku[pd.to_datetime(day)] = np.nan\n",
    "            else:\n",
    "                time.sleep(BACKOFF_SEC * attempt)\n",
    "\n",
    "    pbar.update(1)\n",
    "    time.sleep(SPACING_SEC)\n",
    "\n",
    "pbar.close()\n",
    "print(f\"✅ run done – mean latency {np.nanmean(latencies):.2f}s | failures {pd.isna(list(scores_haiku.values())).sum()}\")\n",
    "\n",
    "# ---------- simple cost estimate ---------------------------------------------\n",
    "TOK_IN  = len(all_titles) * 1900      # rough 1 900 input tok / day\n",
    "TOK_OUT = len(all_titles) * MAX_TOKENS_OUT\n",
    "cost_in  = TOK_IN  / 1_000_000 * 0.25   # $0.25/M input tok  – see Anthropic pricing&#8203;:contentReference[oaicite:0]{index=0}&#8203;:contentReference[oaicite:1]{index=1}\n",
    "cost_out = TOK_OUT / 1_000_000 * 1.25   # $1.25/M output tok – Haiku output\n",
    "print(f\"💲 approx cost ≈ ${cost_in+cost_out:0.2f}\")\n",
    "\n",
    "# ---------- compare to GPT-4o-mini -------------------------------------------\n",
    "haiku = pd.Series(scores_haiku).sort_index()\n",
    "cmp   = pd.DataFrame({\"mini\": full, \"haiku\": haiku}).dropna()\n",
    "\n",
    "rho_s  = cmp.mini.corr(cmp.haiku, method=\"spearman\").round(3)\n",
    "rho_p  = cmp.mini.corr(cmp.haiku, method=\"pearson\").round(3)\n",
    "print(f\"\\nPearson ρ = {rho_p}   |   Spearman ρ = {rho_s}\")\n",
    "\n",
    "ax = cmp.plot.scatter(\"mini\",\"haiku\",alpha=.35,figsize=(4,4))\n",
    "ax.plot([0,10],[0,10],'r--',lw=1)\n",
    "ax.set_xlabel(\"GPT-4o-mini score\"); ax.set_ylabel(\"Claude 3 Haiku score\")\n",
    "ax.set_title(\"Daily escalation scores: 4-o-mini vs Claude Haiku\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Compare trend & volatility – 4-o-mini vs 3.5-turbo          ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- align the two series --------------------------------------\n",
    "mini  = full.copy()              # 4-o-mini series (already complete)\n",
    "turbo = turbo.reindex(mini.index)   # ensure same index length\n",
    "\n",
    "# --- rolling stats ---------------------------------------------\n",
    "mini_7   = mini.rolling(7,  min_periods=1).mean()\n",
    "turbo_7  = turbo.rolling(7, min_periods=1).mean()\n",
    "\n",
    "mini_vol = mini.rolling(14, min_periods=7).std()\n",
    "turbo_vol= turbo.rolling(14, min_periods=7).std()\n",
    "\n",
    "# --- plot 7-day rolling mean -----------------------------------\n",
    "fig, ax = plt.subplots(2, 1, figsize=(14,6), sharex=True,\n",
    "                       gridspec_kw={\"height_ratios\":[2,1]})\n",
    "\n",
    "ax[0].plot(mini_7.index,  mini_7,  label=\"4-o-mini (7-day mean)\",  lw=2.0, c=\"tab:red\")\n",
    "ax[0].plot(turbo_7.index, turbo_7, label=\"3.5-turbo (7-day mean)\", lw=1.8, c=\"tab:blue\")\n",
    "ax[0].set_title(\"7-day rolling average of escalation score\")\n",
    "ax[0].set_ylabel(\"Mean score\"); ax[0].grid(alpha=.3); ax[0].legend()\n",
    "\n",
    "# --- plot 14-day volatility ------------------------------------\n",
    "ax[1].plot(mini_vol.index,  mini_vol,  label=\"4-o-mini (14-day σ)\",  c=\"tab:red\")\n",
    "ax[1].plot(turbo_vol.index, turbo_vol, label=\"3.5-turbo (14-day σ)\", c=\"tab:blue\")\n",
    "ax[1].set_title(\"14-day volatility of escalation score\")\n",
    "ax[1].set_ylabel(\"σ\"); ax[1].grid(alpha=.3); ax[1].legend()\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Trend & regime visuals – EWMA • rolling pct • CUSUM         ║\n",
    "# ║            (4-o-mini vs 3.5-turbo)                           ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import matplotlib.pyplot as plt, numpy as np, pandas as pd, datetime as dt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 0. ensure identical index\n",
    "mini  = mini.reindex(turbo.index)\n",
    "turbo = turbo.reindex(mini.index)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. EXPONENTIALLY-WEIGHTED MOVING AVERAGE  (half-life = 7 days)\n",
    "hl  = 7                               # half-life\n",
    "ewma_mini  = mini.ewm(halflife=hl).mean()\n",
    "ewma_turbo = turbo.ewm(halflife=hl).mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,3))\n",
    "ax.plot(ewma_mini,  c=\"tab:red\",  lw=2, label=\"4-o-mini EWMA (7-d hl)\")\n",
    "ax.plot(ewma_turbo, c=\"tab:blue\", lw=1.5, label=\"3.5-turbo EWMA\")\n",
    "ax.set_title(f\"Exponentially-weighted mean (half-life = {hl} days)\")\n",
    "ax.set_ylabel(\"Score\"); ax.grid(alpha=.3); ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. 90-DAY ROLLING PERCENTILE RANK\n",
    "win = 90\n",
    "pct_mini  = mini.rolling(win).apply(lambda s: s.rank(pct=True).iloc[-1])\n",
    "pct_turbo = turbo.rolling(win).apply(lambda s: s.rank(pct=True).iloc[-1])\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(14,3))\n",
    "ax.plot(pct_mini,  c=\"tab:red\",  label=\"4-o-mini\")\n",
    "ax.plot(pct_turbo, c=\"tab:blue\", label=\"3.5-turbo\")\n",
    "ax.set_ylim(0,1); ax.set_ylabel(\"Percentile\"); ax.set_title(f\"{win}-day rolling percentile rank\")\n",
    "ax.grid(alpha=.3); ax.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. CUMULATIVE-SUM (CUSUM) OF DEVIATIONS FROM LONG-RUN MEAN\n",
    "mu_mini  = mini.mean()\n",
    "mu_turbo = turbo.mean()\n",
    "\n",
    "cusum_mini  = (mini  - mu_mini ).cumsum()\n",
    "cusum_turbo = (turbo - mu_turbo).cumsum()\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(14,3))\n",
    "ax.plot(cusum_mini,  c=\"tab:red\",  lw=2, label=f\"4-o-mini (μ={mu_mini:.2f})\")\n",
    "ax.plot(cusum_turbo, c=\"tab:blue\", lw=1.5, label=f\"3.5-turbo (μ={mu_turbo:.2f})\")\n",
    "ax.axhline(0,c='k',lw=.7)\n",
    "ax.set_title(\"CUSUM of deviations (regime-shift visual)\")\n",
    "ax.set_ylabel(\"Cumulative deviation\"); ax.grid(alpha=.3); ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. Change-point detection (ruptures) ----------------------------\n",
    "try:\n",
    "    import ruptures as rpt\n",
    "    model = (mini - mini.mean()).values  # centre series\n",
    "    algo  = rpt.KernelCPD(kernel=\"rbf\").fit(model)\n",
    "    cpts  = algo.predict(pen=5)[:-1]      # last point is len(series)\n",
    "    fig, ax = plt.subplots(figsize=(14,2.5))\n",
    "    ax.plot(mini.index, mini, lw=.8)\n",
    "    for cp in cpts:\n",
    "        ax.axvline(mini.index[cp], c=\"tab:red\", lw=1)\n",
    "    ax.set_title(\"Change-point detection (vertical bars) – 4-o-mini\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "except ImportError:\n",
    "    print(\"⚠️  ruptures not installed – skip change-point plot\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 5. STL decomposition (trend + remainder) ------------------------\n",
    "try:\n",
    "    from statsmodels.tsa.seasonal import STL\n",
    "    stl = STL(mini, period=7, robust=True).fit()\n",
    "    fig = stl.plot()\n",
    "    fig.set_size_inches(14,4)\n",
    "    fig.axes[0].set_title(\"STL decomposition (trend & remainder) – 4-o-mini\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "except ImportError:\n",
    "    print(\"⚠️  statsmodels not installed – skip STL plot\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 6. Calendar heat-map (4-o-mini raw scores) ---------------------\n",
    "years     = sorted(mini.index.year.unique())\n",
    "day_cols  = 366                              # always pad to leap-year length\n",
    "rows      = []\n",
    "\n",
    "for y in years:\n",
    "    # full date range for that calendar year\n",
    "    yr_range = pd.date_range(f\"{y}-01-01\", f\"{y}-12-31\", freq=\"D\")\n",
    "    # align series to that range\n",
    "    yr_vals  = mini.reindex(yr_range).values\n",
    "    # pad / truncate to 366 so every row is equal length\n",
    "    if len(yr_vals) < day_cols:\n",
    "        yr_vals = np.append(yr_vals, [np.nan]*(day_cols - len(yr_vals)))\n",
    "    rows.append(yr_vals[:day_cols])\n",
    "\n",
    "mat = np.vstack(rows)                         # now all rows = 366 cols\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, len(years)*0.5))\n",
    "cmap = ListedColormap(plt.cm.viridis(np.linspace(0,1,11)))\n",
    "im   = ax.imshow(mat, aspect=\"auto\", cmap=cmap, vmin=0, vmax=10)\n",
    "\n",
    "ax.set_yticks(range(len(years))); ax.set_yticklabels(years)\n",
    "ax.set_xticks([0, 90, 181, 273, 355])\n",
    "ax.set_xticklabels([\"Jan\",\"Apr\",\"Jul\",\"Oct\",\"Dec\"])\n",
    "ax.set_title(\"Calendar heat-map – 4-o-mini escalation score\")\n",
    "plt.colorbar(im, ax=ax, label=\"Score\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 7. 30-day rolling correlation between models -------------------\n",
    "roll_corr = mini.rolling(30).corr(turbo)\n",
    "\n",
    "plt.figure(figsize=(14,2.5))\n",
    "plt.plot(roll_corr.index, roll_corr, lw=1.2, c=\"purple\")\n",
    "plt.axhline(0, c='k', lw=.6); plt.ylim(-1,1)\n",
    "plt.title(\"30-day rolling correlation (4-o-mini vs. 3.5-turbo)\")\n",
    "plt.ylabel(\"ρ\"); plt.grid(alpha=.3); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 8. Event “lollipop” overlay – ≥ ±3 point jumps -----------------\n",
    "fig, ax = plt.subplots(figsize=(14,2.5))\n",
    "ax.plot(mini.index, mini, lw=.8)\n",
    "ax.stem(events.index, events.values,\n",
    "        basefmt=\" \",               # hide baseline\n",
    "        markerfmt=\"C3o\",           # red circle markers\n",
    "        linefmt=\"C3-\")             # red stems\n",
    "ax.set_title(f\"Lollipop events (|Δ| ≥ {thr}) – 4-o-mini\")\n",
    "ax.set_ylabel(\"Score\"); ax.grid(alpha=.3)\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Save Model Comparison Results for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, pyarrow, pprint, importlib; print(sys.executable); print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Persist comparison results (4-o-mini vs 3.5-turbo)          ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "from pathlib import Path\n",
    "import pandas as pd, matplotlib.pyplot as plt, datetime as dt\n",
    "\n",
    "today_tag = dt.date.today().isoformat()\n",
    "out_dir   = Path(\"model_benchmarks\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- tidy DataFrame -------------------------------------\n",
    "df_save = (pd.DataFrame({\n",
    "            \"date\"      : df_cmp.index,\n",
    "            \"score_4o\"  : df_cmp[\"mini\"],\n",
    "            \"score_35\"  : df_cmp[\"turbo\"],\n",
    "            \"abs_diff\"  : (df_cmp[\"mini\"] - df_cmp[\"turbo\"]).abs()\n",
    "          })\n",
    "          .reset_index(drop=True))\n",
    "\n",
    "csv_path     = out_dir / f\"ukraine_escalation_compare_4o_vs_turbo_{today_tag}.csv\"\n",
    "feather_path = out_dir / f\"ukraine_escalation_compare_4o_vs_turbo_{today_tag}.feather\"\n",
    "\n",
    "df_save.to_csv(csv_path, index=False)\n",
    "print(f\"📁 Saved CSV → {csv_path}\")\n",
    "\n",
    "# feather (optional)\n",
    "try:\n",
    "    df_save.to_feather(feather_path)    # requires pyarrow\n",
    "    print(f\"📁 Saved feather → {feather_path}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  pyarrow not installed – feather file skipped.\")\n",
    "\n",
    "# ---------- scatter plot ---------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "ax.scatter(df_cmp[\"mini\"], df_cmp[\"turbo\"], alpha=.35)\n",
    "ax.plot([0,10],[0,10],'r--',lw=1)\n",
    "ax.set_xlabel(\"GPT-4o-mini score\"); ax.set_ylabel(\"3.5-turbo score\")\n",
    "ax.set_title(\"Daily escalation scores: 4-o-mini vs 3.5-turbo\")\n",
    "plt.tight_layout()\n",
    "\n",
    "png_path = out_dir / f\"scatter_4o_vs_turbo_{today_tag}.png\"\n",
    "fig.savefig(png_path, dpi=300)\n",
    "plt.show()\n",
    "print(f\"🖼️  Scatter plot saved → {png_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════╗\n",
    "# ║  Extra agreement metrics: 4-o-mini vs 3.5-turbo              ║\n",
    "# ╚══════════════════════════════════════════════════════════════╝\n",
    "import numpy as np, pandas as pd, scipy.stats as st\n",
    "\n",
    "# aligned series ------------------------------------------------\n",
    "s4  = df_cmp[\"mini\"]\n",
    "s35 = df_cmp[\"turbo\"]\n",
    "\n",
    "# 1. Spearman & Kendall -----------------------------------------\n",
    "spearman = st.spearmanr(s4,  s35).correlation\n",
    "kendall  = st.kendalltau(s4, s35).correlation\n",
    "\n",
    "# 2. Directional Accuracy ---------------------------------------\n",
    "d4  = np.sign(s4.diff().fillna(0))\n",
    "d35 = np.sign(s35.diff().fillna(0))\n",
    "DA  = (d4 == d35).mean()            # fraction of matching signs\n",
    "\n",
    "# 3. Correlation of first differences ---------------------------\n",
    "delta_r = st.pearsonr(s4.diff().iloc[1:], s35.diff().iloc[1:])[0]\n",
    "\n",
    "# 4. Up / Down capture ratios -----------------------------------\n",
    "ups   = d4 > 0\n",
    "downs = d4 < 0\n",
    "up_cap   = (s35.diff()[ups].mean())  / (s4.diff()[ups].mean())\n",
    "down_cap = (s35.diff()[downs].mean())/ (s4.diff()[downs].mean())\n",
    "\n",
    "print(f\"\\n--- Direction-oriented agreement metrics ---\")\n",
    "print(f\"Spearman ρ (ranks)        : {spearman: .3f}\")\n",
    "print(f\"Kendall τ                 : {kendall: .3f}\")\n",
    "print(f\"Directional accuracy      : {DA: .3%}\")\n",
    "print(f\"Corr of first differences : {delta_r: .3f}\")\n",
    "print(f\"Up-capture ratio          : {up_cap: .2f}\")\n",
    "print(f\"Down-capture ratio        : {down_cap: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "**Interpretation tips** :\n",
    "* A downward slope after 2 Apr 2025 for a source indicates more critical framing of tariffs.\n",
    "* Compare amplitudes—Fox vs CNN—to detect partisan divergence.\n",
    "* Check `df[df.tone.abs()>1.5]` to view the strongest headlines driving spikes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
