{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── GOLD-SET CANDIDATES  ──────────────────────────────────────────────\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT   = Path.cwd().parents[0]        # adjust if you’re already at repo root\n",
    "RAW_TG = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "OUT    = ROOT / \"outputs\" / \"telegram_gold_candidates_300.csv\"\n",
    "\n",
    "df = pd.read_csv(RAW_TG)\n",
    "\n",
    "# freeze RNG for reproducibility\n",
    "sample_df = df.sample(n=300, random_state=42).reset_index(drop=True)\n",
    "sample_df.to_csv(OUT, index=False)\n",
    "\n",
    "print(f\"✅ wrote 300-row sample → {OUT.relative_to(ROOT)}\")\n",
    "sample_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Score Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  SCORE Telegram Messages - Anthropic Batch API                        ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "OUT_DIR = ROOT / \"outputs\" / \"telegram_scoring\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model selection\n",
    "#MODEL = \"claude-3-5-sonnet-20241022\"  # Fast and good for large batches\n",
    "# MODEL = \"claude-3-5-haiku-20241022\"  # Even faster, still good\n",
    "MODEL = \"claude-sonnet-4-20250514\"   # Latest, if you want newest\n",
    "\n",
    "# Batch configuration\n",
    "BATCH_SIZE = 15000  # Safe batch size based on your 41k success\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# SCORING RUBRIC - Multi-dimensional like Truth Social\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "TELEGRAM_PROMPT = \"\"\"You are analyzing Telegram messages about the Russia-Ukraine war.\n",
    "Score each message on FOUR dimensions:\n",
    "\n",
    "1. Escalation (0-10): How militarily escalatory is the message?\n",
    "2. Blame (-1, 0, 1): Who does the message blame?\n",
    "   - 0 = Blames Ukraine/NATO/West\n",
    "   - 1 = Blames Russia/Putin  \n",
    "   - -1 = No clear blame/neutral reporting\n",
    "3. Propaganda (0-3): Level of propaganda/manipulation\n",
    "   - 0 = Factual reporting\n",
    "   - 1 = Mild spin/framing\n",
    "   - 2 = Clear propaganda\n",
    "   - 3 = Extreme propaganda/disinformation\n",
    "4. Call-to-Action (0, 1): Does it urge concrete action?\n",
    "   - 1 = Yes (donate, volunteer, protest, etc.)\n",
    "   - 0 = No explicit call\n",
    "\n",
    "Escalation scale reference:\n",
    "0: Humanitarian/diplomatic content\n",
    "1: Peace negotiation focus  \n",
    "2: Political warfare/disinformation\n",
    "3: Routine warfare updates\n",
    "4: Major aid package discussions\n",
    "5: Major weapons systems\n",
    "6: Advanced weapons escalation\n",
    "7: Nation-wide strikes\n",
    "8: Strategic infrastructure attacks\n",
    "9: Nuclear facility/weapons threats\n",
    "10: Direct nuclear war rhetoric\n",
    "\n",
    "---\n",
    "\n",
    "# Real message examples (message → E,B,P,C):\n",
    "\n",
    "- \"Гуманитарная помощь доставлена в освобожденные районы\"                                → 0,-1,0,0\n",
    "- \"Zelensky's advisor confirms Ukraine ready for talks: \"We're not against negotiations, but they must be based on international law and Ukraine's sovereignty.\"\" → 1,1,0,0\n",
    "- \"🇺🇸🇺🇦The United States will be transferring Patriot air defense systems to Ukraine, — Kirby\n",
    "The White House confirmed that Patriots are included in the aid package that Biden will announce.\" → 5,1,0,0\n",
    "- \"🇬🇧🇺🇦 Great Britain secretly supplied Ukraine with Storm Shadow cruise missiles, writes CNN.\n",
    "According to the publication, the deliveries took place a few weeks ago. The UK has asked Ukraine not to use missiles to attack facilities on the territory of the Russian Federation.\n",
    "It is noted that Storm Shadow has a range of more than 250 km.\" → 6,1,1,0\n",
    "- \"Biden pushing us to WW3, must negotiate NOW!\"                                         → 8,0,2,1  \n",
    "- \"Российские войска нанесли удар по складу ВСУ в Харькове\"                             → 3,1,1,0\n",
    "- \"⚡️BREAKING: Nuclear plant under attack! NATO must act!\"                               → 9,0,3,1\n",
    "- \"Сегодня ВСУ обстреляли жилые кварталы Донецка. 5 погибших.\"                          → 4,0,2,0\n",
    "- \"Отчет с передовой: позиции удерживаются, враг несет потери\"                          → 3,-1,1,0\n",
    "- \"DONATE NOW to help Ukrainian defenders! Every dollar saves lives!\"                     → 2,1,1,1\n",
    "- \"Путин угрожает ядерным оружием если НАТО вмешается\"                                  → 10,1,2,0\n",
    "- \"💥💥💥 Explosions all over Ukraine\n",
    "\n",
    "Explosions in:\n",
    "Vinnytsia;\n",
    "The Vinnitsa region;\n",
    "Dnipro;\n",
    "Zaporizhia;\n",
    "Kirovograd;\n",
    "Kharkov;\n",
    "Odessa;\n",
    "N.....\n",
    "\n",
    "Air raid throughout Ukraine.\" → 7,0,1,0\n",
    "\n",
    "---\n",
    "\n",
    "CRITICAL: Respond ONLY with four integers in format E,B,P,C\n",
    "No spaces, no explanations - just four numbers with three commas.\n",
    "Example: 5,0,2,1\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# LOAD AND PREPARE DATA\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📊 Loading Telegram data...\")\n",
    "df = pd.read_csv(TELEGRAM_CSV)\n",
    "print(f\"✅ Loaded {len(df):,} messages\")\n",
    "\n",
    "# Data validation\n",
    "df = df[df['message_text'].notna()].copy()\n",
    "df = df[df['message_text'].str.strip() != ''].copy()\n",
    "print(f\"✅ {len(df):,} messages with valid text\")\n",
    "\n",
    "# Add tracking index\n",
    "df['global_idx'] = range(len(df))\n",
    "\n",
    "# Show category distribution\n",
    "print(\"\\n📈 Message distribution by category:\")\n",
    "category_counts = df['channel_category'].value_counts()\n",
    "for cat, count in category_counts.items():\n",
    "    print(f\"   {cat}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# BATCH PROCESSING FUNCTIONS\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_batches(df, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Split dataframe into batches\"\"\"\n",
    "    n_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        batch_df['batch_num'] = i + 1\n",
    "        batch_df['batch_idx'] = range(len(batch_df))\n",
    "        batches.append(batch_df)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def prepare_batch_requests(batch_df):\n",
    "    \"\"\"Prepare requests for a batch\"\"\"\n",
    "    requests_list = []\n",
    "    \n",
    "    for _, row in batch_df.iterrows():\n",
    "        # Truncate very long messages\n",
    "        text = str(row['message_text'])[:1500]\n",
    "        \n",
    "        # Add channel category context\n",
    "        context = f\"[Channel: {row['channel_username']} ({row['channel_category']})]\\\\n{text}\"\n",
    "        \n",
    "        request = {\n",
    "            \"custom_id\": str(row['batch_idx']),\n",
    "            \"params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"max_tokens\": 15,  # Just need E,B,P,C\n",
    "                \"temperature\": 0,\n",
    "                \"system\": TELEGRAM_PROMPT,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": context}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        requests_list.append(request)\n",
    "    \n",
    "    return requests_list\n",
    "\n",
    "def process_batch_results(batch_df, results_data):\n",
    "    \"\"\"Parse results and add to dataframe\"\"\"\n",
    "    scores = {\n",
    "        \"escalation\": {},\n",
    "        \"blame\": {},\n",
    "        \"propaganda\": {},\n",
    "        \"cta\": {}\n",
    "    }\n",
    "    parse_errors = []\n",
    "    \n",
    "    for line in results_data:\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            result = json.loads(line)\n",
    "            custom_id = result.get(\"custom_id\")\n",
    "            \n",
    "            if custom_id is None:\n",
    "                continue\n",
    "                \n",
    "            idx = int(custom_id)\n",
    "            \n",
    "            # Check if request succeeded\n",
    "            if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                parse_errors.append(f\"Request {custom_id} failed\")\n",
    "                continue\n",
    "            \n",
    "            # Extract response\n",
    "            message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "            \n",
    "            # Parse E,B,P,C format\n",
    "            match = re.match(r'^(\\d+),(-?\\d+),(\\d+),(\\d+)', message_content)\n",
    "            \n",
    "            if match:\n",
    "                e = int(match.group(1))\n",
    "                b = int(match.group(2))\n",
    "                p = int(match.group(3))\n",
    "                c = int(match.group(4))\n",
    "                \n",
    "                # Validate ranges\n",
    "                if 0 <= e <= 10:\n",
    "                    scores[\"escalation\"][idx] = e\n",
    "                if b in (-1, 0, 1):\n",
    "                    scores[\"blame\"][idx] = b\n",
    "                if 0 <= p <= 3:\n",
    "                    scores[\"propaganda\"][idx] = p\n",
    "                if c in (0, 1):\n",
    "                    scores[\"cta\"][idx] = c\n",
    "            else:\n",
    "                parse_errors.append(f\"Parse error for {custom_id}: {message_content}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            parse_errors.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # Map scores to dataframe\n",
    "    batch_df[\"escalation_score\"] = batch_df[\"batch_idx\"].map(scores[\"escalation\"]).astype(\"Int64\")\n",
    "    batch_df[\"blame_direction\"] = batch_df[\"batch_idx\"].map(scores[\"blame\"]).astype(\"Int64\")\n",
    "    batch_df[\"propaganda_level\"] = batch_df[\"batch_idx\"].map(scores[\"propaganda\"]).astype(\"Int64\")\n",
    "    batch_df[\"has_cta\"] = batch_df[\"batch_idx\"].map(scores[\"cta\"]).astype(\"Int64\")\n",
    "    \n",
    "    return batch_df, len(scores[\"escalation\"]), parse_errors\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# MAIN PROCESSING LOOP\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Quiet logging\n",
    "for name in (\"httpx\", \"anthropic\"):\n",
    "    logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "# Create batches\n",
    "batches = create_batches(df)\n",
    "print(f\"\\n🔄 Created {len(batches)} batches\")\n",
    "\n",
    "all_results = []\n",
    "total_processed = 0\n",
    "total_errors = 0\n",
    "\n",
    "# Process each batch\n",
    "for batch_num, batch_df in enumerate(batches, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📦 Processing Batch {batch_num}/{len(batches)}\")\n",
    "    print(f\"   Messages: {len(batch_df):,}\")\n",
    "    print(f\"   Categories: {batch_df['channel_category'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Prepare requests\n",
    "    requests_list = prepare_batch_requests(batch_df)\n",
    "    print(f\"   Prepared {len(requests_list)} requests\")\n",
    "    \n",
    "    # Create Anthropic batch\n",
    "    try:\n",
    "        batch = client.messages.batches.create(requests=requests_list)\n",
    "        print(f\"   🚀 Launched batch {batch.id}\")\n",
    "        \n",
    "        # Monitor progress\n",
    "        bar = tqdm.tqdm(total=len(requests_list), desc=f\"Batch {batch_num}\", unit=\"msg\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            batch_status = client.messages.batches.retrieve(batch.id)\n",
    "            completed = (batch_status.request_counts.succeeded + \n",
    "                        batch_status.request_counts.errored + \n",
    "                        batch_status.request_counts.canceled + \n",
    "                        batch_status.request_counts.expired)\n",
    "            bar.n = completed\n",
    "            bar.refresh()\n",
    "            \n",
    "            if batch_status.processing_status == \"ended\":\n",
    "                bar.close()\n",
    "                break\n",
    "                \n",
    "            time.sleep(5)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"   ✅ Batch complete in {elapsed_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Retrieve results\n",
    "        batch_final = client.messages.batches.retrieve(batch.id)\n",
    "        \n",
    "        if batch_final.results_url:\n",
    "            headers = {\n",
    "                \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                \"anthropic-version\": \"2023-06-01\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(batch_final.results_url, headers=headers, stream=True)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                results_data = [line.decode('utf-8') for line in response.iter_lines()]\n",
    "                \n",
    "                # Process results\n",
    "                batch_df_scored, n_success, errors = process_batch_results(batch_df, results_data)\n",
    "                \n",
    "                print(f\"   ✅ Scored {n_success:,}/{len(batch_df):,} messages\")\n",
    "                if errors:\n",
    "                    print(f\"   ⚠️  {len(errors)} parse errors\")\n",
    "                    total_errors += len(errors)\n",
    "                \n",
    "                total_processed += n_success\n",
    "                all_results.append(batch_df_scored)\n",
    "                \n",
    "                # Save intermediate results\n",
    "                intermediate_file = OUT_DIR / f\"batch_{batch_num}_scored.csv\"\n",
    "                batch_df_scored.to_csv(intermediate_file, index=False)\n",
    "                print(f\"   💾 Saved to {intermediate_file.name}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   ❌ Error fetching results: HTTP {response.status_code}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Batch processing error: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Break between batches\n",
    "    if batch_num < len(batches):\n",
    "        print(f\"\\n⏸️  Waiting 30 seconds before next batch...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# COMBINE RESULTS AND SAVE\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 Combining all results...\")\n",
    "\n",
    "# Combine all batches\n",
    "if all_results:\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Sort by original order\n",
    "    final_df = final_df.sort_values('global_idx')\n",
    "    \n",
    "    # Drop processing columns\n",
    "    columns_to_drop = ['batch_num', 'batch_idx', 'global_idx']\n",
    "    final_df = final_df.drop(columns=[col for col in columns_to_drop if col in final_df.columns])\n",
    "    \n",
    "    # Save final results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_output = OUT_DIR / f\"telegram_scored_{MODEL}_{timestamp}.csv\"\n",
    "    final_df.to_csv(final_output, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ SCORING COMPLETE!\")\n",
    "    print(f\"   Total messages: {len(df):,}\")\n",
    "    print(f\"   Successfully scored: {total_processed:,}\")\n",
    "    print(f\"   Failed: {len(df) - total_processed:,}\")\n",
    "    print(f\"   Success rate: {total_processed/len(df)*100:.1f}%\")\n",
    "    print(f\"   Total errors: {total_errors}\")\n",
    "    \n",
    "    # Calculate statistics by category\n",
    "    print(\"\\n📊 SCORE DISTRIBUTIONS BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category in final_df['channel_category'].unique():\n",
    "        cat_df = final_df[final_df['channel_category'] == category]\n",
    "        n_scored = cat_df['escalation_score'].notna().sum()\n",
    "        \n",
    "        if n_scored > 0:\n",
    "            print(f\"\\n{category} ({n_scored:,} messages):\")\n",
    "            \n",
    "            # Escalation\n",
    "            esc_mean = cat_df['escalation_score'].mean()\n",
    "            esc_std = cat_df['escalation_score'].std()\n",
    "            print(f\"   Escalation: {esc_mean:.2f} ± {esc_std:.2f}\")\n",
    "            \n",
    "            # Blame\n",
    "            blame_counts = cat_df['blame_direction'].value_counts().sort_index()\n",
    "            blame_dict = {-1: \"Neutral\", 0: \"Blames West\", 1: \"Blames Russia\"}\n",
    "            print(\"   Blame direction:\")\n",
    "            for val, count in blame_counts.items():\n",
    "                if pd.notna(val):\n",
    "                    label = blame_dict.get(int(val), val)\n",
    "                    pct = count / n_scored * 100\n",
    "                    print(f\"      {label}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Propaganda\n",
    "            prop_mean = cat_df['propaganda_level'].mean()\n",
    "            print(f\"   Propaganda level: {prop_mean:.2f}/3.0\")\n",
    "            \n",
    "            # CTA\n",
    "            cta_pct = cat_df['has_cta'].sum() / n_scored * 100\n",
    "            print(f\"   Has call-to-action: {cta_pct:.1f}%\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    overall_esc = final_df['escalation_score'].mean()\n",
    "    print(f\"Average escalation: {overall_esc:.2f}\")\n",
    "    \n",
    "    # Compare pro-Russian vs pro-Ukrainian\n",
    "    pro_ru = final_df[final_df['channel_category'] == 'pro_russian_grassroots']['escalation_score'].mean()\n",
    "    pro_ua = final_df[final_df['channel_category'] == 'pro_ukrainian_grassroots']['escalation_score'].mean()\n",
    "    \n",
    "    if pd.notna(pro_ru) and pd.notna(pro_ua):\n",
    "        print(f\"\\nPro-Russian avg escalation: {pro_ru:.2f}\")\n",
    "        print(f\"Pro-Ukrainian avg escalation: {pro_ua:.2f}\")\n",
    "        print(f\"Difference: {abs(pro_ru - pro_ua):.2f}\")\n",
    "    \n",
    "    print(f\"\\n📁 Final results saved to: {final_output}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No results to combine!\")\n",
    "\n",
    "print(\"\\n✨ Ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Detect Complete + Resume Batch Scoring Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  SCORE Telegram Messages - Anthropic Batch API (RESUME VERSION)       ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "OUT_DIR = ROOT / \"outputs\" / \"telegram_scoring\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Model selection\n",
    "MODEL = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "# Batch configuration\n",
    "BATCH_SIZE = 15000\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# CHECK FOR EXISTING PROGRESS\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔍 Checking for existing progress...\")\n",
    "existing_batches = []\n",
    "for i in range(1, 100):  # Check up to 100 batches\n",
    "    batch_file = OUT_DIR / f\"batch_{i}_scored.csv\"\n",
    "    if batch_file.exists():\n",
    "        existing_batches.append(i)\n",
    "        print(f\"   ✅ Found completed batch {i}\")\n",
    "    else:\n",
    "        break\n",
    "\n",
    "if existing_batches:\n",
    "    print(f\"\\n📂 Found {len(existing_batches)} completed batches\")\n",
    "    start_from_batch = max(existing_batches) + 1\n",
    "    print(f\"   Will resume from batch {start_from_batch}\")\n",
    "else:\n",
    "    start_from_batch = 1\n",
    "    print(\"   Starting fresh from batch 1\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# SCORING RUBRIC - Same as before\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "TELEGRAM_PROMPT = \"\"\"You are analyzing Telegram messages about the Russia-Ukraine war.\n",
    "Score each message on FOUR dimensions:\n",
    "\n",
    "1. Escalation (0-10): How militarily escalatory is the message?\n",
    "2. Blame (-1, 0, 1): Who does the message blame?\n",
    "   - 0 = Blames Ukraine/NATO/West\n",
    "   - 1 = Blames Russia/Putin  \n",
    "   - -1 = No clear blame/neutral reporting\n",
    "3. Propaganda (0-3): Level of propaganda/manipulation\n",
    "   - 0 = Factual reporting\n",
    "   - 1 = Mild spin/framing\n",
    "   - 2 = Clear propaganda\n",
    "   - 3 = Extreme propaganda/disinformation\n",
    "4. Call-to-Action (0, 1): Does it urge concrete action?\n",
    "   - 1 = Yes (donate, volunteer, protest, etc.)\n",
    "   - 0 = No explicit call\n",
    "\n",
    "Escalation scale reference:\n",
    "0: Humanitarian/diplomatic content\n",
    "1: Peace negotiation focus  \n",
    "2: Political warfare/disinformation\n",
    "3: Routine warfare updates\n",
    "4: Major aid package discussions\n",
    "5: Major weapons systems\n",
    "6: Advanced weapons escalation\n",
    "7: Nation-wide strikes\n",
    "8: Strategic infrastructure attacks\n",
    "9: Nuclear facility/weapons threats\n",
    "10: Direct nuclear war rhetoric\n",
    "\n",
    "---\n",
    "\n",
    "# Real message examples (message → E,B,P,C):\n",
    "\n",
    "- \"Гуманитарная помощь доставлена в освобожденные районы\"                                → 0,-1,0,0\n",
    "- \"Zelensky's advisor confirms Ukraine ready for talks: \"We're not against negotiations, but they must be based on international law and Ukraine's sovereignty.\"\" → 1,1,0,0\n",
    "- \"🇺🇸🇺🇦The United States will be transferring Patriot air defense systems to Ukraine, — Kirby\n",
    "The White House confirmed that Patriots are included in the aid package that Biden will announce.\" → 5,1,0,0\n",
    "- \"🇬🇧🇺🇦 Great Britain secretly supplied Ukraine with Storm Shadow cruise missiles, writes CNN.\n",
    "According to the publication, the deliveries took place a few weeks ago. The UK has asked Ukraine not to use missiles to attack facilities on the territory of the Russian Federation.\n",
    "It is noted that Storm Shadow has a range of more than 250 km.\" → 6,1,1,0\n",
    "- \"Biden pushing us to WW3, must negotiate NOW!\"                                         → 8,0,2,1  \n",
    "- \"Российские войска нанесли удар по складу ВСУ в Харькове\"                             → 3,1,1,0\n",
    "- \"⚡️BREAKING: Nuclear plant under attack! NATO must act!\"                               → 9,0,3,1\n",
    "- \"Сегодня ВСУ обстреляли жилые кварталы Донецка. 5 погибших.\"                          → 4,0,2,0\n",
    "- \"Отчет с передовой: позиции удерживаются, враг несет потери\"                          → 3,-1,1,0\n",
    "- \"DONATE NOW to help Ukrainian defenders! Every dollar saves lives!\"                     → 2,1,1,1\n",
    "- \"Путин угрожает ядерным оружием если НАТО вмешается\"                                  → 10,1,2,0\n",
    "- \"💥💥💥 Explosions all over Ukraine\n",
    "\n",
    "Explosions in:\n",
    "Vinnytsia;\n",
    "The Vinnitsa region;\n",
    "Dnipro;\n",
    "Zaporizhia;\n",
    "Kirovograd;\n",
    "Kharkov;\n",
    "Odessa;\n",
    "N.....\n",
    "\n",
    "Air raid throughout Ukraine.\" → 7,0,1,0\n",
    "\n",
    "---\n",
    "\n",
    "CRITICAL: Respond ONLY with four integers in format E,B,P,C\n",
    "No spaces, no explanations - just four numbers with three commas.\n",
    "Example: 5,0,2,1\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# LOAD AND PREPARE DATA\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n📊 Loading Telegram data...\")\n",
    "df = pd.read_csv(TELEGRAM_CSV)\n",
    "print(f\"✅ Loaded {len(df):,} messages\")\n",
    "\n",
    "# Data validation\n",
    "df = df[df['message_text'].notna()].copy()\n",
    "df = df[df['message_text'].str.strip() != ''].copy()\n",
    "print(f\"✅ {len(df):,} messages with valid text\")\n",
    "\n",
    "# Add tracking index\n",
    "df['global_idx'] = range(len(df))\n",
    "\n",
    "# Show category distribution\n",
    "print(\"\\n📈 Message distribution by category:\")\n",
    "category_counts = df['channel_category'].value_counts()\n",
    "for cat, count in category_counts.items():\n",
    "    print(f\"   {cat}: {count:,} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# BATCH PROCESSING FUNCTIONS (same as before)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def create_batches(df, batch_size=BATCH_SIZE):\n",
    "    \"\"\"Split dataframe into batches\"\"\"\n",
    "    n_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(df))\n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        batch_df['batch_num'] = i + 1\n",
    "        batch_df['batch_idx'] = range(len(batch_df))\n",
    "        batches.append(batch_df)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "def prepare_batch_requests(batch_df):\n",
    "    \"\"\"Prepare requests for a batch\"\"\"\n",
    "    requests_list = []\n",
    "    \n",
    "    for _, row in batch_df.iterrows():\n",
    "        # Truncate very long messages\n",
    "        text = str(row['message_text'])[:1500]\n",
    "        \n",
    "        # Add channel category context\n",
    "        context = f\"[Channel: {row['channel_username']} ({row['channel_category']})]\\\\n{text}\"\n",
    "        \n",
    "        request = {\n",
    "            \"custom_id\": str(row['batch_idx']),\n",
    "            \"params\": {\n",
    "                \"model\": MODEL,\n",
    "                \"max_tokens\": 15,\n",
    "                \"temperature\": 0,\n",
    "                \"system\": TELEGRAM_PROMPT,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": context}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        requests_list.append(request)\n",
    "    \n",
    "    return requests_list\n",
    "\n",
    "def process_batch_results(batch_df, results_data):\n",
    "    \"\"\"Parse results and add to dataframe\"\"\"\n",
    "    scores = {\n",
    "        \"escalation\": {},\n",
    "        \"blame\": {},\n",
    "        \"propaganda\": {},\n",
    "        \"cta\": {}\n",
    "    }\n",
    "    parse_errors = []\n",
    "    \n",
    "    for line in results_data:\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            result = json.loads(line)\n",
    "            custom_id = result.get(\"custom_id\")\n",
    "            \n",
    "            if custom_id is None:\n",
    "                continue\n",
    "                \n",
    "            idx = int(custom_id)\n",
    "            \n",
    "            # Check if request succeeded\n",
    "            if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                parse_errors.append(f\"Request {custom_id} failed\")\n",
    "                continue\n",
    "            \n",
    "            # Extract response\n",
    "            message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "            \n",
    "            # Parse E,B,P,C format\n",
    "            match = re.match(r'^(\\d+),(-?\\d+),(\\d+),(\\d+)', message_content)\n",
    "            \n",
    "            if match:\n",
    "                e = int(match.group(1))\n",
    "                b = int(match.group(2))\n",
    "                p = int(match.group(3))\n",
    "                c = int(match.group(4))\n",
    "                \n",
    "                # Validate ranges\n",
    "                if 0 <= e <= 10:\n",
    "                    scores[\"escalation\"][idx] = e\n",
    "                if b in (-1, 0, 1):\n",
    "                    scores[\"blame\"][idx] = b\n",
    "                if 0 <= p <= 3:\n",
    "                    scores[\"propaganda\"][idx] = p\n",
    "                if c in (0, 1):\n",
    "                    scores[\"cta\"][idx] = c\n",
    "            else:\n",
    "                parse_errors.append(f\"Parse error for {custom_id}: {message_content}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            parse_errors.append(f\"Error: {str(e)}\")\n",
    "    \n",
    "    # Map scores to dataframe\n",
    "    batch_df[\"escalation_score\"] = batch_df[\"batch_idx\"].map(scores[\"escalation\"]).astype(\"Int64\")\n",
    "    batch_df[\"blame_direction\"] = batch_df[\"batch_idx\"].map(scores[\"blame\"]).astype(\"Int64\")\n",
    "    batch_df[\"propaganda_level\"] = batch_df[\"batch_idx\"].map(scores[\"propaganda\"]).astype(\"Int64\")\n",
    "    batch_df[\"has_cta\"] = batch_df[\"batch_idx\"].map(scores[\"cta\"]).astype(\"Int64\")\n",
    "    \n",
    "    return batch_df, len(scores[\"escalation\"]), parse_errors\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# MAIN PROCESSING LOOP (MODIFIED FOR RESUME)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Quiet logging\n",
    "for name in (\"httpx\", \"anthropic\"):\n",
    "    logging.getLogger(name).setLevel(logging.WARNING)\n",
    "\n",
    "# Create batches\n",
    "batches = create_batches(df)\n",
    "print(f\"\\n🔄 Created {len(batches)} batches total\")\n",
    "\n",
    "# Load already processed results\n",
    "all_results = []\n",
    "total_processed = 0\n",
    "total_errors = 0\n",
    "\n",
    "# Load existing batch results\n",
    "if existing_batches:\n",
    "    print(\"\\n📥 Loading existing batch results...\")\n",
    "    for batch_num in existing_batches:\n",
    "        batch_file = OUT_DIR / f\"batch_{batch_num}_scored.csv\"\n",
    "        batch_df = pd.read_csv(batch_file)\n",
    "        all_results.append(batch_df)\n",
    "        n_scored = batch_df['escalation_score'].notna().sum()\n",
    "        total_processed += n_scored\n",
    "        print(f\"   Loaded batch {batch_num}: {n_scored:,} scored messages\")\n",
    "\n",
    "print(f\"\\n🚀 Resuming from batch {start_from_batch}/{len(batches)}\")\n",
    "\n",
    "# Process remaining batches\n",
    "for batch_num, batch_df in enumerate(batches, 1):\n",
    "    # Skip already processed batches\n",
    "    if batch_num < start_from_batch:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📦 Processing Batch {batch_num}/{len(batches)}\")\n",
    "    print(f\"   Messages: {len(batch_df):,}\")\n",
    "    print(f\"   Categories: {batch_df['channel_category'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Prepare requests\n",
    "    requests_list = prepare_batch_requests(batch_df)\n",
    "    print(f\"   Prepared {len(requests_list)} requests\")\n",
    "    \n",
    "    # Create Anthropic batch\n",
    "    try:\n",
    "        batch = client.messages.batches.create(requests=requests_list)\n",
    "        print(f\"   🚀 Launched batch {batch.id}\")\n",
    "        \n",
    "        # Monitor progress\n",
    "        bar = tqdm.tqdm(total=len(requests_list), desc=f\"Batch {batch_num}\", unit=\"msg\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while True:\n",
    "            batch_status = client.messages.batches.retrieve(batch.id)\n",
    "            completed = (batch_status.request_counts.succeeded + \n",
    "                        batch_status.request_counts.errored + \n",
    "                        batch_status.request_counts.canceled + \n",
    "                        batch_status.request_counts.expired)\n",
    "            bar.n = completed\n",
    "            bar.refresh()\n",
    "            \n",
    "            if batch_status.processing_status == \"ended\":\n",
    "                bar.close()\n",
    "                break\n",
    "                \n",
    "            time.sleep(5)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"   ✅ Batch complete in {elapsed_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Retrieve results\n",
    "        batch_final = client.messages.batches.retrieve(batch.id)\n",
    "        \n",
    "        if batch_final.results_url:\n",
    "            headers = {\n",
    "                \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                \"anthropic-version\": \"2023-06-01\"\n",
    "            }\n",
    "            \n",
    "            response = requests.get(batch_final.results_url, headers=headers, stream=True)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                results_data = [line.decode('utf-8') for line in response.iter_lines()]\n",
    "                \n",
    "                # Process results\n",
    "                batch_df_scored, n_success, errors = process_batch_results(batch_df, results_data)\n",
    "                \n",
    "                print(f\"   ✅ Scored {n_success:,}/{len(batch_df):,} messages\")\n",
    "                if errors:\n",
    "                    print(f\"   ⚠️  {len(errors)} parse errors\")\n",
    "                    total_errors += len(errors)\n",
    "                \n",
    "                total_processed += n_success\n",
    "                all_results.append(batch_df_scored)\n",
    "                \n",
    "                # Save intermediate results\n",
    "                intermediate_file = OUT_DIR / f\"batch_{batch_num}_scored.csv\"\n",
    "                batch_df_scored.to_csv(intermediate_file, index=False)\n",
    "                print(f\"   💾 Saved to {intermediate_file.name}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"   ❌ Error fetching results: HTTP {response.status_code}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Batch processing error: {str(e)}\")\n",
    "        continue\n",
    "    \n",
    "    # Break between batches\n",
    "    if batch_num < len(batches):\n",
    "        print(f\"\\n⏸️  Waiting 30 seconds before next batch...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# COMBINE RESULTS AND SAVE (same as before)\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📊 Combining all results...\")\n",
    "\n",
    "# Combine all batches\n",
    "if all_results:\n",
    "    final_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Sort by original order if global_idx exists\n",
    "    if 'global_idx' in final_df.columns:\n",
    "        final_df = final_df.sort_values('global_idx')\n",
    "    \n",
    "    # Drop processing columns\n",
    "    columns_to_drop = ['batch_num', 'batch_idx', 'global_idx']\n",
    "    final_df = final_df.drop(columns=[col for col in columns_to_drop if col in final_df.columns])\n",
    "    \n",
    "    # Save final results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_output = OUT_DIR / f\"telegram_scored_{MODEL}_{timestamp}.csv\"\n",
    "    final_df.to_csv(final_output, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ SCORING COMPLETE!\")\n",
    "    print(f\"   Total messages: {len(df):,}\")\n",
    "    print(f\"   Successfully scored: {total_processed:,}\")\n",
    "    print(f\"   Failed: {len(df) - total_processed:,}\")\n",
    "    print(f\"   Success rate: {total_processed/len(df)*100:.1f}%\")\n",
    "    print(f\"   Total errors: {total_errors}\")\n",
    "    \n",
    "    # Calculate statistics by category\n",
    "    print(\"\\n📊 SCORE DISTRIBUTIONS BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category in final_df['channel_category'].unique():\n",
    "        cat_df = final_df[final_df['channel_category'] == category]\n",
    "        n_scored = cat_df['escalation_score'].notna().sum()\n",
    "        \n",
    "        if n_scored > 0:\n",
    "            print(f\"\\n{category} ({n_scored:,} messages):\")\n",
    "            \n",
    "            # Escalation\n",
    "            esc_mean = cat_df['escalation_score'].mean()\n",
    "            esc_std = cat_df['escalation_score'].std()\n",
    "            print(f\"   Escalation: {esc_mean:.2f} ± {esc_std:.2f}\")\n",
    "            \n",
    "            # Blame\n",
    "            blame_counts = cat_df['blame_direction'].value_counts().sort_index()\n",
    "            blame_dict = {-1: \"Neutral\", 0: \"Blames West\", 1: \"Blames Russia\"}\n",
    "            print(\"   Blame direction:\")\n",
    "            for val, count in blame_counts.items():\n",
    "                if pd.notna(val):\n",
    "                    label = blame_dict.get(int(val), val)\n",
    "                    pct = count / n_scored * 100\n",
    "                    print(f\"      {label}: {count:,} ({pct:.1f}%)\")\n",
    "            \n",
    "            # Propaganda\n",
    "            prop_mean = cat_df['propaganda_level'].mean()\n",
    "            print(f\"   Propaganda level: {prop_mean:.2f}/3.0\")\n",
    "            \n",
    "            # CTA\n",
    "            cta_pct = cat_df['has_cta'].sum() / n_scored * 100\n",
    "            print(f\"   Has call-to-action: {cta_pct:.1f}%\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    overall_esc = final_df['escalation_score'].mean()\n",
    "    print(f\"Average escalation: {overall_esc:.2f}\")\n",
    "    \n",
    "    # Compare pro-Russian vs pro-Ukrainian\n",
    "    pro_ru = final_df[final_df['channel_category'] == 'pro_russian_grassroots']['escalation_score'].mean()\n",
    "    pro_ua = final_df[final_df['channel_category'] == 'pro_ukrainian_grassroots']['escalation_score'].mean()\n",
    "    \n",
    "    if pd.notna(pro_ru) and pd.notna(pro_ua):\n",
    "        print(f\"\\nPro-Russian avg escalation: {pro_ru:.2f}\")\n",
    "        print(f\"Pro-Ukrainian avg escalation: {pro_ua:.2f}\")\n",
    "        print(f\"Difference: {abs(pro_ru - pro_ua):.2f}\")\n",
    "    \n",
    "    print(f\"\\n📁 Final results saved to: {final_output}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No results to combine!\")\n",
    "\n",
    "print(\"\\n✨ Ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Retrieve Partial Batch 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  RECOVER AND RESUME - Anthropic Batch Processing (FIXED)              ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, re, tqdm\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "OUT_DIR = ROOT / \"outputs\" / \"telegram_scoring\"\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL = \"claude-sonnet-4-20250514\"\n",
    "BATCH_SIZE = 15000\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 1: Find all existing batches\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔍 Scanning for all existing batches...\")\n",
    "\n",
    "# List recent batches\n",
    "recent_batches = client.messages.batches.list(limit=50)\n",
    "completed_batches = []\n",
    "\n",
    "for batch in recent_batches.data:\n",
    "    if batch.processing_status == \"ended\" and batch.request_counts.succeeded > 0:\n",
    "        # created_at is already a datetime object\n",
    "        completed_batches.append({\n",
    "            'id': batch.id,\n",
    "            'succeeded': batch.request_counts.succeeded,\n",
    "            'failed': batch.request_counts.errored,\n",
    "            'created': batch.created_at,  # Already a datetime\n",
    "            'results_url': getattr(batch, 'results_url', None)\n",
    "        })\n",
    "        print(f\"   ✅ {batch.id}: {batch.request_counts.succeeded} succeeded\")\n",
    "\n",
    "print(f\"\\nFound {len(completed_batches)} completed batches\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 2: Download any missing batch results\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Check which batch files we already have\n",
    "existing_files = list(OUT_DIR.glob(\"batch_*_scored.csv\"))\n",
    "existing_batch_nums = set()\n",
    "for f in existing_files:\n",
    "    match = re.search(r'batch_(\\d+)_scored\\.csv', f.name)\n",
    "    if match:\n",
    "        existing_batch_nums.add(int(match.group(1)))\n",
    "\n",
    "print(f\"\\n📂 Already have batch files: {sorted(existing_batch_nums)}\")\n",
    "\n",
    "# Known batch IDs from your output\n",
    "known_batch_ids = {\n",
    "    10: \"msgbatch_01AZ767eqYBQuRrzN2PidyaC\",  # The one in your screenshot\n",
    "    11: \"msgbatch_01GFn7vV3JzBQaXeTRW3m1gv\"   # The one that was processing\n",
    "}\n",
    "\n",
    "# Try to download any missing batch results\n",
    "for batch_num, batch_id in known_batch_ids.items():\n",
    "    if batch_num not in existing_batch_nums:\n",
    "        print(f\"\\n📥 Checking batch {batch_num} ({batch_id})...\")\n",
    "        try:\n",
    "            batch_status = client.messages.batches.retrieve(batch_id)\n",
    "            print(f\"   Status: {batch_status.processing_status}\")\n",
    "            print(f\"   Succeeded: {batch_status.request_counts.succeeded}\")\n",
    "            \n",
    "            if batch_status.results_url and batch_status.request_counts.succeeded > 0:\n",
    "                print(\"   📥 Downloading results...\")\n",
    "                headers = {\n",
    "                    \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                    \"anthropic-version\": \"2023-06-01\"\n",
    "                }\n",
    "                response = requests.get(batch_status.results_url, headers=headers)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    # Save raw results\n",
    "                    raw_file = OUT_DIR / f\"batch_{batch_num}_raw_results.jsonl\"\n",
    "                    with open(raw_file, 'wb') as f:\n",
    "                        f.write(response.content)\n",
    "                    print(f\"   💾 Saved raw results to {raw_file}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ Error downloading: HTTP {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error retrieving batch: {str(e)}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 3: Load original data\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n📊 Loading original Telegram data...\")\n",
    "df = pd.read_csv(TELEGRAM_CSV)\n",
    "df = df[df['message_text'].notna()].copy()\n",
    "df = df[df['message_text'].str.strip() != ''].copy()\n",
    "df['global_idx'] = range(len(df))\n",
    "print(f\"✅ Loaded {len(df):,} messages\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 4: Process any raw batch results\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def process_raw_batch_file(batch_num, df_full):\n",
    "    \"\"\"Process a raw batch results file\"\"\"\n",
    "    raw_file = OUT_DIR / f\"batch_{batch_num}_raw_results.jsonl\"\n",
    "    if not raw_file.exists():\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n📊 Processing raw results for batch {batch_num}...\")\n",
    "    \n",
    "    # Get the slice of data for this batch\n",
    "    start_idx = (batch_num - 1) * BATCH_SIZE\n",
    "    end_idx = min(batch_num * BATCH_SIZE, len(df_full))\n",
    "    batch_df = df_full.iloc[start_idx:end_idx].copy()\n",
    "    batch_df['batch_idx'] = range(len(batch_df))\n",
    "    batch_df['batch_num'] = batch_num\n",
    "    batch_df['global_idx'] = range(start_idx, end_idx)\n",
    "    \n",
    "    # Process results\n",
    "    scores = {\n",
    "        \"escalation\": {},\n",
    "        \"blame\": {},\n",
    "        \"propaganda\": {},\n",
    "        \"cta\": {}\n",
    "    }\n",
    "    parse_errors = 0\n",
    "    \n",
    "    with open(raw_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                result = json.loads(line)\n",
    "                custom_id = result.get(\"custom_id\")\n",
    "                if custom_id is None:\n",
    "                    continue\n",
    "                    \n",
    "                idx = int(custom_id)\n",
    "                \n",
    "                if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                    parse_errors += 1\n",
    "                    continue\n",
    "                \n",
    "                message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "                match = re.match(r'^(\\d+),(-?\\d+),(\\d+),(\\d+)', message_content)\n",
    "                \n",
    "                if match:\n",
    "                    scores[\"escalation\"][idx] = int(match.group(1))\n",
    "                    scores[\"blame\"][idx] = int(match.group(2))\n",
    "                    scores[\"propaganda\"][idx] = int(match.group(3))\n",
    "                    scores[\"cta\"][idx] = int(match.group(4))\n",
    "                else:\n",
    "                    parse_errors += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                parse_errors += 1\n",
    "                continue\n",
    "    \n",
    "    # Map scores to dataframe\n",
    "    batch_df[\"escalation_score\"] = batch_df[\"batch_idx\"].map(scores[\"escalation\"]).astype(\"Int64\")\n",
    "    batch_df[\"blame_direction\"] = batch_df[\"batch_idx\"].map(scores[\"blame\"]).astype(\"Int64\")\n",
    "    batch_df[\"propaganda_level\"] = batch_df[\"batch_idx\"].map(scores[\"propaganda\"]).astype(\"Int64\")\n",
    "    batch_df[\"has_cta\"] = batch_df[\"batch_idx\"].map(scores[\"cta\"]).astype(\"Int64\")\n",
    "    \n",
    "    # Save processed results\n",
    "    output_file = OUT_DIR / f\"batch_{batch_num}_scored.csv\"\n",
    "    batch_df.to_csv(output_file, index=False)\n",
    "    print(f\"   ✅ Saved {len(scores['escalation'])} scored messages to {output_file}\")\n",
    "    if parse_errors > 0:\n",
    "        print(f\"   ⚠️  {parse_errors} messages failed to parse\")\n",
    "    \n",
    "    return batch_df\n",
    "\n",
    "# Process any raw batch files\n",
    "for batch_num in range(1, 13):\n",
    "    raw_file = OUT_DIR / f\"batch_{batch_num}_raw_results.jsonl\"\n",
    "    scored_file = OUT_DIR / f\"batch_{batch_num}_scored.csv\"\n",
    "    if raw_file.exists() and not scored_file.exists():\n",
    "        process_raw_batch_file(batch_num, df)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 5: Combine all existing results\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n📊 Combining all existing results...\")\n",
    "all_results = []\n",
    "total_scored = 0\n",
    "batch_coverage = {}\n",
    "\n",
    "for batch_num in range(1, 13):  # Check batches 1-12\n",
    "    batch_file = OUT_DIR / f\"batch_{batch_num}_scored.csv\"\n",
    "    if batch_file.exists():\n",
    "        batch_df = pd.read_csv(batch_file)\n",
    "        all_results.append(batch_df)\n",
    "        n_scored = batch_df['escalation_score'].notna().sum()\n",
    "        total_scored += n_scored\n",
    "        batch_coverage[batch_num] = n_scored\n",
    "        print(f\"   Batch {batch_num}: {n_scored:,} messages\")\n",
    "\n",
    "print(f\"\\n✅ Total scored so far: {total_scored:,} / {len(df):,}\")\n",
    "remaining = len(df) - total_scored\n",
    "print(f\"   Remaining to score: {remaining:,}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# STEP 6: Create final output and identify gaps\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "if all_results:\n",
    "    # Combine all results\n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    \n",
    "    # Sort and clean\n",
    "    if 'global_idx' in combined_df.columns:\n",
    "        combined_df = combined_df.sort_values('global_idx')\n",
    "    \n",
    "    # Drop processing columns\n",
    "    columns_to_drop = ['batch_num', 'batch_idx', 'global_idx']\n",
    "    combined_df = combined_df.drop(columns=[col for col in columns_to_drop if col in combined_df.columns], errors='ignore')\n",
    "    \n",
    "    # Save combined results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_output = OUT_DIR / f\"telegram_scored_COMBINED_{timestamp}.csv\"\n",
    "    combined_df.to_csv(final_output, index=False)\n",
    "    print(f\"\\n💾 Saved combined results to: {final_output}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    print(\"\\n📊 SCORE DISTRIBUTIONS BY CATEGORY:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for category in combined_df['channel_category'].unique():\n",
    "        cat_df = combined_df[combined_df['channel_category'] == category]\n",
    "        n_scored = cat_df['escalation_score'].notna().sum()\n",
    "        \n",
    "        if n_scored > 0:\n",
    "            print(f\"\\n{category} ({n_scored:,} messages):\")\n",
    "            esc_mean = cat_df['escalation_score'].mean()\n",
    "            esc_std = cat_df['escalation_score'].std()\n",
    "            print(f\"   Escalation: {esc_mean:.2f} ± {esc_std:.2f}\")\n",
    "            \n",
    "            # Propaganda\n",
    "            prop_mean = cat_df['propaganda_level'].mean()\n",
    "            print(f\"   Propaganda level: {prop_mean:.2f}/3.0\")\n",
    "            \n",
    "            # CTA\n",
    "            cta_pct = cat_df['has_cta'].sum() / n_scored * 100\n",
    "            print(f\"   Has call-to-action: {cta_pct:.1f}%\")\n",
    "\n",
    "# Identify missing batches\n",
    "print(f\"\\n📌 BATCH SUMMARY:\")\n",
    "for i in range(1, 13):\n",
    "    if i in batch_coverage:\n",
    "        expected = BATCH_SIZE if i < 12 else 9862\n",
    "        actual = batch_coverage[i]\n",
    "        if actual < expected:\n",
    "            print(f\"   Batch {i}: {actual:,}/{expected:,} ⚠️  INCOMPLETE\")\n",
    "        else:\n",
    "            print(f\"   Batch {i}: {actual:,}/{expected:,} ✅\")\n",
    "    else:\n",
    "        expected = BATCH_SIZE if i < 12 else 9862\n",
    "        print(f\"   Batch {i}: 0/{expected:,} ❌ MISSING\")\n",
    "\n",
    "if remaining > 0:\n",
    "    print(f\"\\n💡 NEXT STEPS:\")\n",
    "    print(f\"   - You need to score {remaining:,} more messages\")\n",
    "    print(f\"   - Consider using the OpenAI script for faster processing\")\n",
    "    print(f\"   - Or create smaller Anthropic batches (5k messages each)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  SCORE REMAINING MESSAGES - Single Anthropic Batch                    ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "from pathlib import Path\n",
    "import json, time, pandas as pd, tqdm, re\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "OUT_DIR = ROOT / \"outputs\" / \"telegram_scoring\"\n",
    "MODEL = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# SCORING PROMPT - PASTE YOUR PROMPT HERE\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "TELEGRAM_PROMPT = \"\"\"You are analyzing Telegram messages about the Russia-Ukraine war.\n",
    "Score each message on FOUR dimensions:\n",
    "\n",
    "1. Escalation (0-10): How militarily escalatory is the message?\n",
    "2. Blame (-1, 0, 1): Who does the message blame?\n",
    "   - 0 = Blames Ukraine/NATO/West\n",
    "   - 1 = Blames Russia/Putin  \n",
    "   - -1 = No clear blame/neutral reporting\n",
    "3. Propaganda (0-3): Level of propaganda/manipulation\n",
    "   - 0 = Factual reporting\n",
    "   - 1 = Mild spin/framing\n",
    "   - 2 = Clear propaganda\n",
    "   - 3 = Extreme propaganda/disinformation\n",
    "4. Call-to-Action (0, 1): Does it urge concrete action?\n",
    "   - 1 = Yes (donate, volunteer, protest, etc.)\n",
    "   - 0 = No explicit call\n",
    "\n",
    "Escalation scale reference:\n",
    "0: Humanitarian/diplomatic content\n",
    "1: Peace negotiation focus  \n",
    "2: Political warfare/disinformation\n",
    "3: Routine warfare updates\n",
    "4: Major aid package discussions\n",
    "5: Major weapons systems\n",
    "6: Advanced weapons escalation\n",
    "7: Nation-wide strikes\n",
    "8: Strategic infrastructure attacks\n",
    "9: Nuclear facility/weapons threats\n",
    "10: Direct nuclear war rhetoric\n",
    "\n",
    "---\n",
    "\n",
    "# Real message examples (message → E,B,P,C):\n",
    "\n",
    "- \"Гуманитарная помощь доставлена в освобожденные районы\"                                → 0,-1,0,0\n",
    "- \"Zelensky's advisor confirms Ukraine ready for talks: \"We're not against negotiations, but they must be based on international law and Ukraine's sovereignty.\"\" → 1,1,0,0\n",
    "- \"🇺🇸🇺🇦The United States will be transferring Patriot air defense systems to Ukraine, — Kirby\n",
    "The White House confirmed that Patriots are included in the aid package that Biden will announce.\" → 5,1,0,0\n",
    "- \"🇬🇧🇺🇦 Great Britain secretly supplied Ukraine with Storm Shadow cruise missiles, writes CNN.\n",
    "According to the publication, the deliveries took place a few weeks ago. The UK has asked Ukraine not to use missiles to attack facilities on the territory of the Russian Federation.\n",
    "It is noted that Storm Shadow has a range of more than 250 km.\" → 6,1,1,0\n",
    "- \"Biden pushing us to WW3, must negotiate NOW!\"                                         → 8,0,2,1  \n",
    "- \"Российские войска нанесли удар по складу ВСУ в Харькове\"                             → 3,1,1,0\n",
    "- \"⚡️BREAKING: Nuclear plant under attack! NATO must act!\"                               → 9,0,3,1\n",
    "- \"Сегодня ВСУ обстреляли жилые кварталы Донецка. 5 погибших.\"                          → 4,0,2,0\n",
    "- \"Отчет с передовой: позиции удерживаются, враг несет потери\"                          → 3,-1,1,0\n",
    "- \"DONATE NOW to help Ukrainian defenders! Every dollar saves lives!\"                     → 2,1,1,1\n",
    "- \"Путин угрожает ядерным оружием если НАТО вмешается\"                                  → 10,1,2,0\n",
    "- \"💥💥💥 Explosions all over Ukraine\n",
    "\n",
    "Explosions in:\n",
    "Vinnytsia;\n",
    "The Vinnitsa region;\n",
    "Dnipro;\n",
    "Zaporizhia;\n",
    "Kirovograd;\n",
    "Kharkov;\n",
    "Odessa;\n",
    "N.....\n",
    "\n",
    "Air raid throughout Ukraine.\" → 7,0,1,0\n",
    "\n",
    "---\n",
    "\n",
    "CRITICAL: Respond ONLY with four integers in format E,B,P,C\n",
    "No spaces, no explanations - just four numbers with three commas.\n",
    "Example: 5,0,2,1\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# LOAD DATA AND FIND UNSCORED MESSAGES\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📊 Loading data...\")\n",
    "\n",
    "# Load original data\n",
    "df_original = pd.read_csv(TELEGRAM_CSV)\n",
    "df_original = df_original[df_original['message_text'].notna()].copy()\n",
    "df_original = df_original[df_original['message_text'].str.strip() != ''].copy()\n",
    "df_original['original_idx'] = range(len(df_original))\n",
    "print(f\"✅ Total messages: {len(df_original):,}\")\n",
    "\n",
    "# Load already scored data\n",
    "scored_file = sorted(OUT_DIR.glob(\"telegram_scored_COMBINED_*.csv\"))[-1]  # Get most recent\n",
    "df_scored = pd.read_csv(scored_file)\n",
    "print(f\"✅ Already scored: {len(df_scored):,}\")\n",
    "\n",
    "# Find unscored messages\n",
    "# We need to identify which original messages haven't been scored\n",
    "# This is tricky because we need to match on content, not just index\n",
    "\n",
    "# Create a unique identifier for matching\n",
    "def create_message_id(row):\n",
    "    # Use first 100 chars of message + channel + date as identifier\n",
    "    msg = str(row['message_text'])[:100]\n",
    "    channel = str(row.get('channel_username', ''))\n",
    "    date = str(row.get('date', ''))\n",
    "    return f\"{msg}|{channel}|{date}\"\n",
    "\n",
    "df_original['msg_id'] = df_original.apply(create_message_id, axis=1)\n",
    "df_scored['msg_id'] = df_scored.apply(create_message_id, axis=1)\n",
    "\n",
    "# Find unscored messages\n",
    "scored_ids = set(df_scored['msg_id'])\n",
    "df_unscored = df_original[~df_original['msg_id'].isin(scored_ids)].copy()\n",
    "\n",
    "print(f\"\\n📊 Messages to score: {len(df_unscored):,}\")\n",
    "print(f\"   Expected: 25,472\")\n",
    "print(f\"   Actual: {len(df_unscored):,}\")\n",
    "\n",
    "if len(df_unscored) == 0:\n",
    "    print(\"\\n✅ All messages already scored!\")\n",
    "    exit()\n",
    "\n",
    "# Show category breakdown\n",
    "print(\"\\n📈 Unscored messages by category:\")\n",
    "for cat, count in df_unscored['channel_category'].value_counts().items():\n",
    "    print(f\"   {cat}: {count:,}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# PREPARE BATCH REQUESTS\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"\\n🚀 Preparing batch requests...\")\n",
    "\n",
    "# Reset index for batch processing\n",
    "df_unscored = df_unscored.reset_index(drop=True)\n",
    "df_unscored['batch_idx'] = range(len(df_unscored))\n",
    "\n",
    "requests_list = []\n",
    "for _, row in df_unscored.iterrows():\n",
    "    # Truncate very long messages\n",
    "    text = str(row['message_text'])[:1500]\n",
    "    \n",
    "    # Add channel category context\n",
    "    context = f\"[Channel: {row['channel_username']} ({row['channel_category']})]\\\\n{text}\"\n",
    "    \n",
    "    request = {\n",
    "        \"custom_id\": str(row['batch_idx']),\n",
    "        \"params\": {\n",
    "            \"model\": MODEL,\n",
    "            \"max_tokens\": 15,\n",
    "            \"temperature\": 0,\n",
    "            \"system\": TELEGRAM_PROMPT,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": context}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    requests_list.append(request)\n",
    "\n",
    "print(f\"✅ Prepared {len(requests_list):,} requests\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "# SEND TO ANTHROPIC\n",
    "# ═══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "print(\"\\n🚀 Creating Anthropic batch...\")\n",
    "print(f\"   Model: {MODEL}\")\n",
    "print(f\"   Messages: {len(requests_list):,}\")\n",
    "\n",
    "# Create batch\n",
    "try:\n",
    "    batch = client.messages.batches.create(requests=requests_list)\n",
    "    print(f\"\\n✅ Batch created successfully!\")\n",
    "    print(f\"   Batch ID: {batch.id}\")\n",
    "    print(f\"   Status: {batch.processing_status}\")\n",
    "    \n",
    "    # Save batch info\n",
    "    batch_info = {\n",
    "        'batch_id': batch.id,\n",
    "        'created_at': str(datetime.now()),\n",
    "        'n_messages': len(requests_list),\n",
    "        'model': MODEL\n",
    "    }\n",
    "    \n",
    "    with open(OUT_DIR / 'final_batch_info.json', 'w') as f:\n",
    "        json.dump(batch_info, f, indent=2)\n",
    "    \n",
    "    print(\"\\n📄 Batch info saved to final_batch_info.json\")\n",
    "    \n",
    "    # Monitor progress\n",
    "    print(\"\\n⏳ Monitoring batch progress...\")\n",
    "    print(\"   This may take 10-30 minutes for 25k messages\")\n",
    "    \n",
    "    bar = tqdm.tqdm(total=len(requests_list), desc=\"Processing\", unit=\"msg\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        batch_status = client.messages.batches.retrieve(batch.id)\n",
    "        completed = (batch_status.request_counts.succeeded + \n",
    "                    batch_status.request_counts.errored + \n",
    "                    batch_status.request_counts.canceled + \n",
    "                    batch_status.request_counts.expired)\n",
    "        bar.n = completed\n",
    "        bar.refresh()\n",
    "        \n",
    "        # Show detailed status every 30 seconds\n",
    "        if int(time.time() - start_time) % 30 == 0:\n",
    "            bar.set_postfix({\n",
    "                'succeeded': batch_status.request_counts.succeeded,\n",
    "                'errors': batch_status.request_counts.errored,\n",
    "                'status': batch_status.processing_status\n",
    "            })\n",
    "        \n",
    "        if batch_status.processing_status == \"ended\":\n",
    "            bar.close()\n",
    "            break\n",
    "            \n",
    "        time.sleep(5)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n✅ Batch complete in {elapsed_time/60:.1f} minutes\")\n",
    "    print(f\"   Succeeded: {batch_status.request_counts.succeeded:,}\")\n",
    "    print(f\"   Failed: {batch_status.request_counts.errored:,}\")\n",
    "    \n",
    "    # Retrieve and save results\n",
    "    if batch_status.results_url:\n",
    "        print(\"\\n📥 Downloading results...\")\n",
    "        headers = {\n",
    "            \"x-api-key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            \"anthropic-version\": \"2023-06-01\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(batch_status.results_url, headers=headers, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Save raw results\n",
    "            raw_file = OUT_DIR / \"final_batch_raw_results.jsonl\"\n",
    "            with open(raw_file, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"   💾 Saved raw results to {raw_file}\")\n",
    "            \n",
    "            # Process results\n",
    "            print(\"\\n📊 Processing results...\")\n",
    "            from collections import defaultdict\n",
    "            scores = defaultdict(dict)\n",
    "            parse_errors = []\n",
    "            \n",
    "            with open(raw_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if not line.strip():\n",
    "                        continue\n",
    "                    try:\n",
    "                        result = json.loads(line)\n",
    "                        custom_id = result.get(\"custom_id\")\n",
    "                        if custom_id is None:\n",
    "                            continue\n",
    "                            \n",
    "                        idx = int(custom_id)\n",
    "                        \n",
    "                        if result.get(\"result\", {}).get(\"type\") != \"succeeded\":\n",
    "                            parse_errors.append(f\"Request {custom_id} failed\")\n",
    "                            continue\n",
    "                        \n",
    "                        message_content = result[\"result\"][\"message\"][\"content\"][0][\"text\"].strip()\n",
    "                        match = re.match(r'^(\\d+),(-?\\d+),(\\d+),(\\d+)', message_content)\n",
    "                        \n",
    "                        if match:\n",
    "                            scores[\"escalation_score\"][idx] = int(match.group(1))\n",
    "                            scores[\"blame_direction\"][idx] = int(match.group(2))\n",
    "                            scores[\"propaganda_level\"][idx] = int(match.group(3))\n",
    "                            scores[\"has_cta\"][idx] = int(match.group(4))\n",
    "                        else:\n",
    "                            parse_errors.append(f\"Parse error: {message_content}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        parse_errors.append(f\"Error: {str(e)}\")\n",
    "            \n",
    "            # Map scores to dataframe\n",
    "            for col in ['escalation_score', 'blame_direction', 'propaganda_level', 'has_cta']:\n",
    "                df_unscored[col] = df_unscored['batch_idx'].map(scores.get(col, {})).astype(\"Int64\")\n",
    "            \n",
    "            # Save final scored data\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            final_file = OUT_DIR / f\"telegram_remaining_scored_{timestamp}.csv\"\n",
    "            df_unscored.drop(columns=['batch_idx', 'original_idx', 'msg_id'], inplace=True)\n",
    "            df_unscored.to_csv(final_file, index=False)\n",
    "            \n",
    "            print(f\"\\n✅ SCORING COMPLETE!\")\n",
    "            print(f\"   Successfully scored: {len(scores['escalation_score']):,}\")\n",
    "            print(f\"   Parse errors: {len(parse_errors)}\")\n",
    "            print(f\"   Saved to: {final_file}\")\n",
    "            \n",
    "            # Combine with existing results\n",
    "            print(\"\\n📊 Creating final combined file...\")\n",
    "            df_combined = pd.concat([df_scored, df_unscored], ignore_index=True)\n",
    "            final_combined = OUT_DIR / f\"telegram_FINAL_COMPLETE_{timestamp}.csv\"\n",
    "            df_combined.to_csv(final_combined, index=False)\n",
    "            print(f\"   💾 Complete dataset saved to: {final_combined}\")\n",
    "            print(f\"   Total messages: {len(df_combined):,}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"   ❌ Error downloading results: HTTP {response.status_code}\")\n",
    "    else:\n",
    "        print(\"   ❌ No results URL available\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error creating batch: {str(e)}\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"- Daily batch limit reached\")\n",
    "    print(\"- API key issues\")\n",
    "    print(\"- Network problems\")\n",
    "    print(\"\\nTry again later or use smaller batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║  FAST CONCURRENT VERSION - Like your working script                    ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration\n",
    "ROOT = Path.cwd().resolve().parents[0] if Path.cwd().name != 'ukraine-final-project' else Path.cwd()\n",
    "TELEGRAM_CSV = ROOT / \"outputs\" / \"telegram_full_20250605_213258.csv\"\n",
    "OUT_DIR = ROOT / \"outputs\" / \"telegram_scoring\"\n",
    "MODEL = \"claude-sonnet-4-20250514\"\n",
    "\n",
    "# Match your working script\n",
    "MAX_WORKERS = 10  # Same as your fast script\n",
    "\n",
    "# Your prompt\n",
    "TELEGRAM_PROMPT = \"\"\"You are analyzing Telegram messages about the Russia-Ukraine war.\n",
    "Score each message on FOUR dimensions:\n",
    "\n",
    "1. Escalation (0-10): How militarily escalatory is the message?\n",
    "2. Blame (-1, 0, 1): Who does the message blame?\n",
    "   - 0 = Blames Ukraine/NATO/West\n",
    "   - 1 = Blames Russia/Putin  \n",
    "   - -1 = No clear blame/neutral reporting\n",
    "3. Propaganda (0-3): Level of propaganda/manipulation\n",
    "   - 0 = Factual reporting\n",
    "   - 1 = Mild spin/framing\n",
    "   - 2 = Clear propaganda\n",
    "   - 3 = Extreme propaganda/disinformation\n",
    "4. Call-to-Action (0, 1): Does it urge concrete action?\n",
    "   - 1 = Yes (donate, volunteer, protest, etc.)\n",
    "   - 0 = No explicit call\n",
    "\n",
    "Escalation scale reference:\n",
    "0: Humanitarian/diplomatic content\n",
    "1: Peace negotiation focus  \n",
    "2: Political warfare/disinformation\n",
    "3: Routine warfare updates\n",
    "4: Major aid package discussions\n",
    "5: Major weapons systems\n",
    "6: Advanced weapons escalation\n",
    "7: Nation-wide strikes\n",
    "8: Strategic infrastructure attacks\n",
    "9: Nuclear facility/weapons threats\n",
    "10: Direct nuclear war rhetoric\n",
    "\n",
    "---\n",
    "\n",
    "# Real message examples (message → E,B,P,C):\n",
    "\n",
    "- \"Гуманитарная помощь доставлена в освобожденные районы\"                                → 0,-1,0,0\n",
    "- \"Zelensky's advisor confirms Ukraine ready for talks: \"We're not against negotiations, but they must be based on international law and Ukraine's sovereignty.\"\" → 1,1,0,0\n",
    "- \"Biden pushing us to WW3, must negotiate NOW!\"                                         → 8,0,2,1  \n",
    "- \"Российские войска нанесли удар по складу ВСУ в Харькове\"                             → 3,1,1,0\n",
    "- \"⚡️BREAKING: Nuclear plant under attack! NATO must act!\"                               → 9,0,3,1\n",
    "- \"Сегодня ВСУ обстреляли жилые кварталы Донецка. 5 погибших.\"                          → 4,0,2,0\n",
    "- \"Отчет с передовой: позиции удерживаются, враг несет потери\"                          → 3,-1,1,0\n",
    "- \"DONATE NOW to help Ukrainian defenders! Every dollar saves lives!\"                     → 2,1,1,1\n",
    "- \"Путин угрожает ядерным оружием если НАТО вмешается\"                                  → 10,1,2,0\n",
    "\n",
    "---\n",
    "\n",
    "CRITICAL: Respond ONLY with four integers in format E,B,P,C\n",
    "No spaces, no explanations - just four numbers with three commas.\n",
    "Example: 5,0,2,1\"\"\"\n",
    "\n",
    "print(\"🚀 Starting FAST Telegram scoring...\")\n",
    "print(f\"   Time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Initialize client\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "\n",
    "# Load data (same as before)\n",
    "print(\"\\n📊 Loading data...\")\n",
    "df_original = pd.read_csv(TELEGRAM_CSV)\n",
    "df_original = df_original[df_original['message_text'].notna()].copy()\n",
    "df_original = df_original[df_original['message_text'].str.strip() != ''].copy()\n",
    "\n",
    "scored_file = sorted(OUT_DIR.glob(\"telegram_scored_COMBINED_*.csv\"))[-1]\n",
    "df_scored = pd.read_csv(scored_file)\n",
    "\n",
    "# Find unscored messages\n",
    "def create_message_id(row):\n",
    "    msg = str(row['message_text'])[:100]\n",
    "    channel = str(row.get('channel_username', ''))\n",
    "    date = str(row.get('date', ''))\n",
    "    return f\"{msg}|{channel}|{date}\"\n",
    "\n",
    "df_original['msg_id'] = df_original.apply(create_message_id, axis=1)\n",
    "df_scored['msg_id'] = df_scored.apply(create_message_id, axis=1)\n",
    "\n",
    "scored_ids = set(df_scored['msg_id'])\n",
    "df_unscored = df_original[~df_original['msg_id'].isin(scored_ids)].copy()\n",
    "df_unscored = df_unscored.reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Found {len(df_unscored):,} messages to score\")\n",
    "\n",
    "# Function to score a single message (like your working script)\n",
    "def score_message(idx, row):\n",
    "    \"\"\"Score a single message\"\"\"\n",
    "    results = {\"idx\": idx, \"scores\": None, \"error\": None}\n",
    "    \n",
    "    try:\n",
    "        text = str(row['message_text'])[:1500]\n",
    "        context = f\"[Channel: {row['channel_username']} ({row['channel_category']})]\\\\n{text}\"\n",
    "        \n",
    "        response = client.messages.create(\n",
    "            model=MODEL,\n",
    "            max_tokens=15,\n",
    "            temperature=0,\n",
    "            system=TELEGRAM_PROMPT,\n",
    "            messages=[{\"role\": \"user\", \"content\": context}]\n",
    "        )\n",
    "        \n",
    "        content = response.content[0].text.strip()\n",
    "        match = re.match(r'^(\\d+),(-?\\d+),(\\d+),(\\d+)', content)\n",
    "        \n",
    "        if match:\n",
    "            results[\"scores\"] = {\n",
    "                'escalation_score': int(match.group(1)),\n",
    "                'blame_direction': int(match.group(2)),\n",
    "                'propaganda_level': int(match.group(3)),\n",
    "                'has_cta': int(match.group(4))\n",
    "            }\n",
    "        else:\n",
    "            results[\"error\"] = f\"Parse error: {content}\"\n",
    "            \n",
    "    except anthropic.RateLimitError as e:\n",
    "        # Wait and retry once\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=MODEL,\n",
    "                max_tokens=15,\n",
    "                temperature=0,\n",
    "                system=TELEGRAM_PROMPT,\n",
    "                messages=[{\"role\": \"user\", \"content\": context}]\n",
    "            )\n",
    "            content = response.content[0].text.strip()\n",
    "            match = re.match(r'^(\\d+),(-?\\d+),(\\d+),(\\d+)', content)\n",
    "            if match:\n",
    "                results[\"scores\"] = {\n",
    "                    'escalation_score': int(match.group(1)),\n",
    "                    'blame_direction': int(match.group(2)),\n",
    "                    'propaganda_level': int(match.group(3)),\n",
    "                    'has_cta': int(match.group(4))\n",
    "                }\n",
    "        except Exception as retry_error:\n",
    "            results[\"error\"] = f\"Rate limit: {str(retry_error)}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process in parallel (like your working script!)\n",
    "print(f\"\\n🚀 Processing with {MAX_WORKERS} concurrent workers...\")\n",
    "print(f\"   This should be MUCH faster!\\n\")\n",
    "\n",
    "results_dict = {}\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    futures = {\n",
    "        executor.submit(score_message, idx, row): idx \n",
    "        for idx, row in df_unscored.iterrows()\n",
    "    }\n",
    "    \n",
    "    # Process completed tasks with progress bar\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Scoring\"):\n",
    "        result = future.result()\n",
    "        results_dict[result[\"idx\"]] = result\n",
    "        \n",
    "        if result[\"scores\"]:\n",
    "            success_count += 1\n",
    "        else:\n",
    "            error_count += 1\n",
    "        \n",
    "        # Save checkpoint every 500\n",
    "        if (success_count + error_count) % 500 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = (success_count + error_count) / elapsed\n",
    "            tqdm.write(f\"💾 Progress: {success_count:,} success, {error_count:,} errors, {rate:.1f} msg/sec\")\n",
    "\n",
    "# Apply results to dataframe\n",
    "print(\"\\n📊 Applying results...\")\n",
    "for idx in sorted(results_dict.keys()):\n",
    "    result = results_dict[idx]\n",
    "    if result[\"scores\"]:\n",
    "        for col, val in result[\"scores\"].items():\n",
    "            df_unscored.at[idx, col] = val\n",
    "\n",
    "# Save results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_file = OUT_DIR / f\"telegram_remaining_scored_{timestamp}.csv\"\n",
    "df_unscored.drop(columns=['msg_id'], inplace=True, errors='ignore')\n",
    "df_unscored.to_csv(final_file, index=False)\n",
    "\n",
    "# Combine with existing\n",
    "df_combined = pd.concat([df_scored, df_unscored], ignore_index=True)\n",
    "final_complete = OUT_DIR / f\"telegram_FINAL_COMPLETE_{timestamp}.csv\"\n",
    "df_combined.to_csv(final_complete, index=False)\n",
    "\n",
    "# Summary\n",
    "elapsed_total = time.time() - start_time\n",
    "print(f\"\\n✅ COMPLETE in {elapsed_total/60:.1f} minutes!\")\n",
    "print(f\"   Successfully scored: {success_count:,} / {len(df_unscored):,} ({success_count/len(df_unscored)*100:.1f}%)\")\n",
    "print(f\"   Errors: {error_count}\")\n",
    "print(f\"   Average rate: {len(df_unscored)/elapsed_total:.1f} messages/second\")\n",
    "print(f\"\\n📁 Complete dataset saved to: {final_complete}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
