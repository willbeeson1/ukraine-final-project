{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telegram Data Collection for Ukraine-Russia Conflict Analysis\n",
    "# This notebook scrapes Telegram channels from Feb 24, 2022 to June 6, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Telegram Data Collection for Ukraine-Russia Conflict Analysis\n",
    " \n",
    "This notebook implements a comprehensive Telegram scraping system for collecting grassroots discourse about the Ukraine-Russia conflict from February 24, 2022 (invasion date) to present.\n",
    "\n",
    "## Key Features:\n",
    "- Balanced collection from pro-Ukrainian and pro-Russian channels\n",
    "- Rate limiting and anti-ban measures\n",
    "- Multilingual text processing (Russian, Ukrainian, English)\n",
    "- Sentiment analysis preparation\n",
    "- SQLite storage with comprehensive metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from telethon import TelegramClient\n",
    "from telethon.errors import FloodWaitError, ChannelPrivateError\n",
    "from langdetect import detect\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tqdm if needed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- repo bootstrap ---------------------------------------------------------\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "def repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    while cur != cur.parent:\n",
    "        if (cur / \".env\").exists() or (cur / \".git\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise RuntimeError(\"repo root not found\")\n",
    "\n",
    "ROOT = repo_root(Path.cwd())\n",
    "load_dotenv(ROOT / \".env\")             # loads secrets\n",
    "sys.path.append(str(ROOT / \"src\"))     # optional helpers\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "OUT_DIR  = ROOT / \"outputs\"\n",
    "FIG_DIR  = OUT_DIR / \"figs\"; FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Repo root:\", ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Hard Coded Timeline Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the timeline events\n",
    "timeline_events = [\n",
    "    {\"date\":\"2022-02-24\",\"event\":\"Russia launches full-scale invasion of Ukraine\",\"label\":\"Invasion\",\"major\":True},\n",
    "    {\"date\":\"2022-04-02\",\"event\":\"Bucha massacre discovered\",\"label\":\"Bucha\",\"major\":True},\n",
    "    {\"date\":\"2022-05-20\",\"event\":\"Mariupol falls\",\"label\":\"Mariupol Falls\",\"major\":True},\n",
    "    {\"date\":\"2022-09-11\",\"event\":\"Ukrainian counteroffensive liberates Kharkiv\",\"label\":\"Kharkiv Liberation\",\"major\":True},\n",
    "    {\"date\":\"2022-11-11\",\"event\":\"Ukraine liberates Kherson city\",\"label\":\"Kherson Free\",\"major\":True},\n",
    "    {\"date\":\"2023-05-20\",\"event\":\"Bakhmut falls\",\"label\":\"Bakhmut Falls\",\"major\":True},\n",
    "    {\"date\":\"2023-06-06\",\"event\":\"Kakhovka Dam destroyed\",\"label\":\"Dam Destroyed\",\"major\":True},\n",
    "    {\"date\":\"2024-02-17\",\"event\":\"Russia captures Avdiivka\",\"label\":\"Avdiivka Falls\",\"major\":True},\n",
    "    {\"date\":\"2024-08-06\",\"event\":\"Ukraine launches Kursk incursion\",\"label\":\"Kursk\",\"major\":True},\n",
    "    {\"date\":\"2024-11-05\",\"event\":\"Trump wins US election\",\"label\":\"Trump Elected\",\"major\":True},\n",
    "    {\"date\":\"2025-01-20\",\"event\":\"Trump inaugurated\",\"label\":\"Trump Policy\",\"major\":True},\n",
    "    {\"date\":\"2025-02-28\",\"event\":\"Trump-Zelensky Oval Office confrontation\",\"label\":\"US-Ukraine Rift\",\"major\":True},\n",
    "    {\"date\":\"2025-05-17\",\"event\":\"Istanbul talks\",\"label\":\"Istanbul\",\"major\":True},\n",
    "    {\"date\":\"2025-05-19\",\"event\":\"Trump announces 'immediate' ceasefire talks\",\"label\":\"Peace Claims\",\"major\":True},\n",
    "    {\"date\":\"2025-05-29\",\"event\":\"Ukrainian drone 'Trojan Horse' infiltrates Russian base\",\"label\":\"Drone Trojan Horse\",\"major\":True},\n",
    "    {\"date\":\"2025-06-02\",\"event\":\"Russia intensifies Chasiv Yar assault\",\"label\":\"Chasiv Yar\",\"major\":True},\n",
    "    {\"date\":\"2025-06-04\",\"event\":\"NATO announces expanded weapons package\",\"label\":\"NATO Aid\",\"major\":True}\n",
    "]\n",
    "\n",
    "# Convert to datetime objects\n",
    "for event in timeline_events:\n",
    "    event['datetime'] = datetime.strptime(event['date'], '%Y-%m-%d').replace(tzinfo=timezone.utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### API & Channel Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configuration (you'll need to get these from https://my.telegram.org)\n",
    "API_ID = os.getenv('TELEGRAM_API_ID', 'YOUR_API_ID')  # Store in .env file\n",
    "API_HASH = os.getenv('TELEGRAM_API_HASH', 'YOUR_API_HASH')  # Store in .env file\n",
    "PHONE = os.getenv('TELEGRAM_PHONE', '+YOUR_PHONE_NUMBER')  # Store in .env file\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(OUT_DIR / 'telegram_scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Channel lists for balanced collection\n",
    "CHANNELS = {\n",
    "    'pro_ukrainian_grassroots': [\n",
    "        '@ssternenko',  # Serhii Sternenko (activist/volunteer)\n",
    "        '@operativnoZSU',  # AFU Operational\n",
    "        '@voynareal',  # War Real\n",
    "        '@DeepStateUA',  # DeepState UA (mapping)\n",
    "        '@horevica',  # Military blogger\n",
    "        '@ukr_sof',  # UA Special Forces\n",
    "        '@aerobomber',  # UA Drone operations\n",
    "        '@combat_ftg'  # Combat footage\n",
    "    ],\n",
    "    'pro_russian_grassroots': [\n",
    "        '@rybar',  # Rybar (major military blogger)\n",
    "        '@starshe_eddy',  # Starshe Eddy (war correspondent)\n",
    "        '@wargonzo',  # WarGonzo\n",
    "        '@grey_zone',  # Grey Zone (Wagner)\n",
    "        '@vysokygovorit',  # Military analyst\n",
    "        '@milinfolive',  # Military Informant\n",
    "        '@voenkorKotenok',  # War correspondent\n",
    "        '@romanov_92'  # Military blogger\n",
    "    ],\n",
    "    'neutral_independent': [\n",
    "        '@astra',  # Independent Russian media\n",
    "        '@CITeam_en',  # Conflict Intelligence Team\n",
    "        '@bellingcat',  # Investigative journalism\n",
    "        '@GirkinGirkin'  # Igor Girkin updates\n",
    "    ],\n",
    "    'official_comparison': [\n",
    "        '@V_Zelenskiy_official',  # President Zelenskyy\n",
    "        '@spravdi',  # Ukraine Gov\n",
    "        '@mod_russia',  # Russian MoD\n",
    "        '@MID_RF'  # Russian MFA\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Test channels for initial run\n",
    "TEST_CHANNELS = {\n",
    "    'test_ukrainian': ['@operativnoZSU'],  # High activity Ukrainian channel\n",
    "    'test_russian': ['@rybar'],  # High activity Russian channel\n",
    "    'test_neutral': ['@astra']  # Independent channel\n",
    "}\n",
    "\n",
    "print(f\"✅ Configuration loaded\")\n",
    "print(f\"   Total channels configured: {sum(len(ch) for ch in CHANNELS.values())}\")\n",
    "print(f\"   Test channels selected: {sum(len(ch) for ch in TEST_CHANNELS.values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Generate Sampling Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sampling_schedule(start_date, end_date, events, baseline_interval=3, event_window=14):\n",
    "    \"\"\"\n",
    "    Generate dates to scrape based on:\n",
    "    - Baseline: every N days throughout the conflict\n",
    "    - Intensive: daily for ±event_window days around major events\n",
    "    \"\"\"\n",
    "    schedule = set()\n",
    "    \n",
    "    # 1. Baseline sampling (every 3rd day)\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        schedule.add(current.date())\n",
    "        current += timedelta(days=baseline_interval)\n",
    "    \n",
    "    # 2. Intensive sampling around major events\n",
    "    for event in events:\n",
    "        if event.get('major', False):\n",
    "            event_date = event['datetime']\n",
    "            # Sample daily for ±14 days around event\n",
    "            for offset in range(-event_window, event_window + 1):\n",
    "                sample_date = event_date + timedelta(days=offset)\n",
    "                if start_date <= sample_date <= end_date:\n",
    "                    schedule.add(sample_date.date())\n",
    "    \n",
    "    # Convert back to sorted list\n",
    "    schedule_list = sorted(list(schedule))\n",
    "    \n",
    "    return schedule_list\n",
    "\n",
    "# Generate the schedule\n",
    "START_DATE = datetime(2022, 2, 24, tzinfo=timezone.utc)\n",
    "END_DATE = datetime(2025, 6, 6, tzinfo=timezone.utc)\n",
    "\n",
    "sampling_dates = generate_sampling_schedule(START_DATE, END_DATE, timeline_events)\n",
    "\n",
    "print(f\"📅 Sampling Strategy Summary:\")\n",
    "print(f\"   Total days in conflict: {(END_DATE - START_DATE).days}\")\n",
    "print(f\"   Days to scrape: {len(sampling_dates)} ({len(sampling_dates)/(END_DATE - START_DATE).days*100:.1f}%)\")\n",
    "print(f\"   Time saved: {100 - len(sampling_dates)/(END_DATE - START_DATE).days*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Visualize Sampling Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of sampling density\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count samples per month\n",
    "monthly_samples = {}\n",
    "for date in sampling_dates:\n",
    "    month_key = f\"{date.year}-{date.month:02d}\"\n",
    "    monthly_samples[month_key] = monthly_samples.get(month_key, 0) + 1\n",
    "\n",
    "# Plot\n",
    "months = sorted(monthly_samples.keys())\n",
    "counts = [monthly_samples[m] for m in months]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.bar(range(len(months)), counts)\n",
    "plt.xticks(range(0, len(months), 3), months[::3], rotation=45)\n",
    "plt.title('Telegram Sampling Density by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Days to Scrape')\n",
    "\n",
    "# Mark major events\n",
    "for event in timeline_events:\n",
    "    if event.get('major'):\n",
    "        event_month = f\"{event['datetime'].year}-{event['datetime'].month:02d}\"\n",
    "        if event_month in months:\n",
    "            idx = months.index(event_month)\n",
    "            plt.annotate(event['label'], xy=(idx, counts[idx]), \n",
    "                        xytext=(idx, counts[idx] + 2), rotation=90, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Priority Channel Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_channel_importance(channel_username, category):\n",
    "    \"\"\"\n",
    "    Assign priority scores to channels for selective scraping\n",
    "    Higher score = more important to scrape completely\n",
    "    \"\"\"\n",
    "    scores = {\n",
    "        # High priority military channels\n",
    "        '@rybar': 10,  # Major Russian military blogger\n",
    "        '@operativnoZSU': 10,  # Ukrainian military official\n",
    "        '@starshe_eddy': 9,  # Detailed Russian reports\n",
    "        '@_3_brigade': 9,  # Elite Ukrainian unit\n",
    "        \n",
    "        # Medium priority\n",
    "        '@wargonzo': 7,\n",
    "        '@grey_zone': 7,\n",
    "        '@astra': 8,  # Independent, anti-Kremlin\n",
    "        \n",
    "        # Lower priority (sample more sparsely)\n",
    "        '@brussinf': 5,\n",
    "        '@combat_ftg': 5,\n",
    "    }\n",
    "    \n",
    "    # Default scores by category\n",
    "    category_defaults = {\n",
    "        'pro_ukrainian': 7,\n",
    "        'pro_russian': 7,\n",
    "        'neutral': 8,\n",
    "        'official': 6  # Lower priority, we have Truth Social for officials\n",
    "    }\n",
    "    \n",
    "    return scores.get(channel_username, category_defaults.get(category, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Optimized Scraping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_with_sampling(client, channel_username, category, conn, sampling_dates):\n",
    "    \"\"\"\n",
    "    Scrape channel but only for dates in our sampling schedule\n",
    "    This dramatically reduces the amount of data to collect\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    messages_collected = 0\n",
    "    channel_priority = estimate_channel_importance(channel_username, category)\n",
    "    \n",
    "    try:\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        \n",
    "        # Store channel metadata\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO channels \n",
    "            (username, title, category, priority_score, last_scraped)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            channel_username,\n",
    "            getattr(channel, 'title', channel_username),\n",
    "            category,\n",
    "            channel_priority,\n",
    "            datetime.now()\n",
    "        ))\n",
    "        \n",
    "        # Convert sampling_dates to set for faster lookup\n",
    "        sample_date_set = set(sampling_dates)\n",
    "        \n",
    "        # For high-priority channels during major events, sample more densely\n",
    "        async for message in client.iter_messages(channel, limit=None):\n",
    "            # Check if message date is in our sampling schedule\n",
    "            message_date = message.date.date() if message.date else None\n",
    "            \n",
    "            if message_date not in sample_date_set:\n",
    "                continue\n",
    "                \n",
    "            # For very high priority channels (score >= 9), also collect adjacent days\n",
    "            if channel_priority >= 9:\n",
    "                # Check if within 1 day of a scheduled date\n",
    "                nearby_dates = [message_date + timedelta(days=i) for i in [-1, 0, 1]]\n",
    "                if not any(d in sample_date_set for d in nearby_dates):\n",
    "                    continue\n",
    "            \n",
    "            # Rate limiting - less aggressive for sampled approach\n",
    "            if messages_collected % 50 == 0:\n",
    "                await asyncio.sleep(0.5)\n",
    "            \n",
    "            # Store message (same as before)\n",
    "            if message.text:\n",
    "                cursor.execute('''\n",
    "                    INSERT OR IGNORE INTO messages \n",
    "                    (channel_username, channel_category, message_id, date, message_text, ...)\n",
    "                    VALUES (?, ?, ?, ?, ?, ...)\n",
    "                ''', (...))  # Same as original\n",
    "                \n",
    "                messages_collected += 1\n",
    "                \n",
    "                if messages_collected % 100 == 0:\n",
    "                    conn.commit()\n",
    "                    print(f\"   {channel_username}: {messages_collected} messages\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {channel_username}: {e}\")\n",
    "    \n",
    "    conn.commit()\n",
    "    return messages_collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Execution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main_event_driven():\n",
    "    \"\"\"\n",
    "    Main execution function with event-driven sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize client\n",
    "    client = TelegramClient('ukraine_russia_event_sampling', API_ID, API_HASH)\n",
    "    await client.start(PHONE)\n",
    "    \n",
    "    # Connect to database\n",
    "    conn = sqlite3.connect('telegram_sampled_data.db')\n",
    "    \n",
    "    # Create tables (include priority_score field)\n",
    "    create_database_with_priority(conn)\n",
    "    \n",
    "    total_messages = 0\n",
    "    channel_stats = {}\n",
    "    \n",
    "    # Process channels by priority\n",
    "    all_channels = []\n",
    "    for category, channel_list in CHANNELS.items():\n",
    "        for channel in channel_list:\n",
    "            priority = estimate_channel_importance(channel, category)\n",
    "            all_channels.append((priority, channel, category))\n",
    "    \n",
    "    # Sort by priority (highest first)\n",
    "    all_channels.sort(reverse=True, key=lambda x: x[0])\n",
    "    \n",
    "    print(f\"\\n📊 Processing {len(all_channels)} channels by priority\")\n",
    "    \n",
    "    for priority, channel, category in all_channels:\n",
    "        print(f\"\\n🔍 Scraping {channel} (priority: {priority}, category: {category})\")\n",
    "        \n",
    "        messages = await scrape_with_sampling(\n",
    "            client, channel, category, conn, sampling_dates\n",
    "        )\n",
    "        \n",
    "        channel_stats[channel] = messages\n",
    "        total_messages += messages\n",
    "        \n",
    "        # Longer break between channels\n",
    "        await asyncio.sleep(30)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n✅ Event-driven scraping complete!\")\n",
    "    print(f\"   Total messages: {total_messages:,}\")\n",
    "    print(f\"   Average per channel: {total_messages/len(all_channels):,.0f}\")\n",
    "    \n",
    "    # Show channel statistics\n",
    "    print(f\"\\n📊 Top channels by message count:\")\n",
    "    sorted_stats = sorted(channel_stats.items(), key=lambda x: x[1], reverse=True)\n",
    "    for channel, count in sorted_stats[:10]:\n",
    "        print(f\"   {channel}: {count:,} messages\")\n",
    "    \n",
    "    conn.close()\n",
    "    await client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Time and Data Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_time_and_data():\n",
    "    \"\"\"\n",
    "    Estimate time and data volume for event-driven approach\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assumptions\n",
    "    channels = 24\n",
    "    days_baseline = len(sampling_dates)\n",
    "    avg_messages_per_channel_per_day = 50\n",
    "    \n",
    "    # Event-driven approach\n",
    "    total_messages_sampled = channels * days_baseline * avg_messages_per_channel_per_day\n",
    "    \n",
    "    # Original approach\n",
    "    total_days_full = (END_DATE - START_DATE).days\n",
    "    total_messages_full = channels * total_days_full * avg_messages_per_channel_per_day\n",
    "    \n",
    "    # Time estimates (with rate limiting)\n",
    "    seconds_per_message = 0.1  # Faster with sampling\n",
    "    time_sampled_hours = (total_messages_sampled * seconds_per_message) / 3600\n",
    "    time_full_hours = (total_messages_full * seconds_per_message) / 3600\n",
    "    \n",
    "    print(\"📊 Comparison: Event-Driven vs Full Scraping\")\n",
    "    print(f\"\\nFull Scraping:\")\n",
    "    print(f\"  Messages: {total_messages_full:,}\")\n",
    "    print(f\"  Time: {time_full_hours:.1f} hours ({time_full_hours/24:.1f} days)\")\n",
    "    \n",
    "    print(f\"\\nEvent-Driven Sampling:\")\n",
    "    print(f\"  Messages: {total_messages_sampled:,} ({total_messages_sampled/total_messages_full*100:.1f}% of full)\")\n",
    "    print(f\"  Time: {time_sampled_hours:.1f} hours ({time_sampled_hours/24:.1f} days)\")\n",
    "    print(f\"  Time saved: {100 - time_sampled_hours/time_full_hours*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n✨ Key Benefits:\")\n",
    "    print(f\"  - Captures all major escalation periods\")\n",
    "    print(f\"  - Maintains baseline coverage throughout conflict\")\n",
    "    print(f\"  - Reduces scraping time by ~70%\")\n",
    "    print(f\"  - Focuses on high-priority channels during critical periods\")\n",
    "\n",
    "estimate_time_and_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Database and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minimal_database(db_path='telegram_sampled_data.db'):\n",
    "    \"\"\"Minimal database for AI processing\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Simplified messages table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS messages_minimal (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            channel_username TEXT,\n",
    "            channel_category TEXT,\n",
    "            message_id INTEGER,\n",
    "            date DATETIME,\n",
    "            message_text TEXT,\n",
    "            views INTEGER,\n",
    "            forwards INTEGER,\n",
    "            replies_count INTEGER,\n",
    "            has_media BOOLEAN,\n",
    "            days_from_event INTEGER,\n",
    "            nearest_event TEXT,\n",
    "            near_major_event BOOLEAN,\n",
    "            escalation_score INTEGER,  -- To be filled by AI\n",
    "            UNIQUE(channel_username, message_id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Channel metadata\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS channels (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            username TEXT UNIQUE,\n",
    "            title TEXT,\n",
    "            category TEXT,\n",
    "            priority_score INTEGER,\n",
    "            last_scraped DATETIME\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Create indices\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON messages_minimal(date)')\n",
    "    cursor.execute('CREATE INDEX IF NOT EXISTS idx_views ON messages_minimal(views)')\n",
    "    \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Text Processing Functions\n",
    "\n",
    "# %%\n",
    "def detect_language_safe(text):\n",
    "    \"\"\"Safely detect language with fallback\"\"\"\n",
    "    if not text or len(text.strip()) < 3:\n",
    "        return 'unknown'\n",
    "    \n",
    "    try:\n",
    "        # Remove URLs and mentions for better detection\n",
    "        clean_text = re.sub(r'http\\\\S+|@\\\\S+', '', text)\n",
    "        if len(clean_text.strip()) > 3:\n",
    "            return detect(clean_text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback: check for Cyrillic vs Latin\n",
    "    cyrillic = len(re.findall(r'[а-яА-ЯёЁїЇіІєЄґҐ]', text))\n",
    "    latin = len(re.findall(r'[a-zA-Z]', text))\n",
    "    \n",
    "    if cyrillic > latin * 2:\n",
    "        return 'ru'  # Could be Russian or Ukrainian\n",
    "    elif latin > cyrillic * 2:\n",
    "        return 'en'\n",
    "    else:\n",
    "        return 'mixed'\n",
    "\n",
    "def extract_multilingual_text(text):\n",
    "    \"\"\"Extract text by script/language\"\"\"\n",
    "    if not text:\n",
    "        return {'original': '', 'english': '', 'russian': '', 'ukrainian': ''}\n",
    "    \n",
    "    # Clean the text first\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Regex patterns for different scripts\n",
    "    english_pattern = re.compile(r'[a-zA-Z0-9\\\\s\\\\.\\\\,\\\\!\\\\?\\\\-]+')\n",
    "    cyrillic_pattern = re.compile(r'[а-яА-ЯёЁїЇіІєЄґҐ\\\\s\\\\.\\\\,\\\\!\\\\?\\\\-]+')\n",
    "    \n",
    "    english_text = ' '.join(english_pattern.findall(text))\n",
    "    cyrillic_text = ' '.join(cyrillic_pattern.findall(text))\n",
    "    \n",
    "    # Try to distinguish Russian from Ukrainian (simplified)\n",
    "    ukrainian_chars = set('їієґ')\n",
    "    has_ukrainian = any(char in text.lower() for char in ukrainian_chars)\n",
    "    \n",
    "    return {\n",
    "        'original': text,\n",
    "        'english': english_text.strip(),\n",
    "        'russian': cyrillic_text.strip() if not has_ukrainian else '',\n",
    "        'ukrainian': cyrillic_text.strip() if has_ukrainian else ''\n",
    "    }\n",
    "\n",
    "def count_escalation_keywords(text):\n",
    "    \"\"\"Count escalation-related keywords in text\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    \n",
    "    # Escalation keywords in multiple languages\n",
    "    keywords = [\n",
    "        # English\n",
    "        'nuclear', 'NATO', 'escalation', 'war', 'attack', 'missile', 'strike',\n",
    "        'offensive', 'killed', 'destroyed', 'captured', 'liberated',\n",
    "        # Russian\n",
    "        'ядерный', 'НАТО', 'эскалация', 'война', 'атака', 'ракета', 'удар',\n",
    "        'наступление', 'убит', 'уничтожен', 'захвачен', 'освобожден',\n",
    "        # Ukrainian  \n",
    "        'ядерний', 'НАТО', 'ескалація', 'війна', 'атака', 'ракета', 'удар',\n",
    "        'наступ', 'вбито', 'знищено', 'захоплено', 'звільнено'\n",
    "    ]\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for keyword in keywords if keyword.lower() in text_lower)\n",
    "\n",
    "def export_to_csv(conn, output_path=None):\n",
    "    \"\"\"\n",
    "    Export scraped data to CSV with timestamp\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_path = OUT_DIR / f'telegram_data_{timestamp}.csv'\n",
    "    \n",
    "    # Query with all relevant fields\n",
    "    query = '''\n",
    "        SELECT \n",
    "            m.channel_username,\n",
    "            m.channel_category,\n",
    "            c.priority_score,\n",
    "            m.message_id,\n",
    "            m.date,\n",
    "            m.message_text,\n",
    "            m.detected_language,\n",
    "            m.views,\n",
    "            m.forwards,\n",
    "            m.replies_count,\n",
    "            m.is_forwarded,\n",
    "            m.media_type,\n",
    "            m.escalation_keywords,\n",
    "            m.days_from_event,\n",
    "            m.nearest_event,\n",
    "            m.near_major_event,\n",
    "            c.title as channel_title,\n",
    "            c.subscribers_count\n",
    "        FROM messages m\n",
    "        LEFT JOIN channels c ON m.channel_username = c.username\n",
    "        ORDER BY m.date DESC\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        # Load to DataFrame\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        \n",
    "        # Add some derived columns\n",
    "        df['message_length'] = df['message_text'].str.len()\n",
    "        df['has_media'] = df['media_type'].notna()\n",
    "        df['engagement_score'] = df['views'].fillna(0) + (df['forwards'].fillna(0) * 10) + (df['replies_count'].fillna(0) * 5)\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "        \n",
    "        print(f\"\\n📊 Export Summary:\")\n",
    "        print(f\"   Total messages: {len(df)}\")\n",
    "        if len(df) > 0:\n",
    "            print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "            print(f\"   Channels: {df['channel_username'].nunique()}\")\n",
    "            print(f\"   Average escalation keywords: {df['escalation_keywords'].mean():.2f}\")\n",
    "            print(f\"   Messages near major events: {df['near_major_event'].sum()}\")\n",
    "        print(f\"\\n💾 Exported to: {output_path}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting to CSV: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on error\n",
    "\n",
    "def find_nearest_event(date, events):\n",
    "    \"\"\"Find the nearest major event to a given date\"\"\"\n",
    "    min_distance = float('inf')\n",
    "    nearest_event = None\n",
    "    \n",
    "    for event in events:\n",
    "        event_date = event['datetime']\n",
    "        distance = abs((date - event_date).days)\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            nearest_event = event\n",
    "    \n",
    "    return {\n",
    "        'days_from_event': min_distance,\n",
    "        'nearest_event': nearest_event['label'] if nearest_event else None,\n",
    "        'near_major_event': min_distance <= 14 and nearest_event.get('major', False)\n",
    "    }\n",
    "\n",
    "# %% [markdown] \n",
    "# ## 5. Rate Limiting Implementation\n",
    "\n",
    "# %%\n",
    "class RateLimiter:\n",
    "    \"\"\"Sophisticated rate limiting to avoid Telegram bans\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.delays = {\n",
    "            'message': 0.5,        # 0.5 seconds between messages\n",
    "            'api_call': 2.0,       # 2 seconds between API calls\n",
    "            'channel_switch': 30,  # 30 seconds between channels\n",
    "            'batch': 300,          # 5 minutes between large batches\n",
    "        }\n",
    "        self.last_action = {}\n",
    "        self.message_count = 0\n",
    "        \n",
    "    async def wait_if_needed(self, action_type):\n",
    "        \"\"\"Implement smart delays based on action type\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if action_type in self.last_action:\n",
    "            elapsed = current_time - self.last_action[action_type]\n",
    "            required_delay = self.delays.get(action_type, 1.0)\n",
    "            \n",
    "            # Add jitter to avoid pattern detection\n",
    "            jitter = random.uniform(0.1, 0.5)\n",
    "            \n",
    "            if elapsed < required_delay:\n",
    "                wait_time = required_delay - elapsed + jitter\n",
    "                logging.debug(f\"Rate limiting: waiting {wait_time:.2f}s for {action_type}\")\n",
    "                await asyncio.sleep(wait_time)\n",
    "        \n",
    "        self.last_action[action_type] = time.time()\n",
    "        \n",
    "        # Track messages for batch delays\n",
    "        if action_type == 'message':\n",
    "            self.message_count += 1\n",
    "            if self.message_count % 500 == 0:\n",
    "                logging.info(f\"Reached {self.message_count} messages, taking extended break\")\n",
    "                await asyncio.sleep(self.delays['batch'])\n",
    "\n",
    "rate_limiter = RateLimiter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Complete Scraping Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Scrape, TDQM, Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_channel_optimized(client, channel_username, category, conn, sampling_dates, \n",
    "                                  max_messages=None, batch_size=100):\n",
    "    \"\"\"\n",
    "    Optimized scraper with progress bars and batch processing - FIXED\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    messages_collected = 0\n",
    "    messages_buffer = []  # Buffer for batch inserts\n",
    "    channel_priority = estimate_channel_importance(channel_username, category)\n",
    "    \n",
    "    try:\n",
    "        # Get channel entity\n",
    "        await rate_limiter.wait_if_needed('api_call')\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        \n",
    "        # Get subscriber count safely\n",
    "        subscriber_count = getattr(channel, 'participants_count', None)\n",
    "        \n",
    "        # Store channel metadata\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO channels \n",
    "            (username, title, category, subscribers_count, description, is_verified, priority_score, last_scraped)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            channel_username,\n",
    "            getattr(channel, 'title', channel_username),\n",
    "            category,\n",
    "            subscriber_count,  # Can be None\n",
    "            getattr(channel, 'about', ''),\n",
    "            getattr(channel, 'verified', False),\n",
    "            channel_priority,\n",
    "            datetime.now()\n",
    "        ))\n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"\\n📱 Scraping {channel_username} ({category})\")\n",
    "        print(f\"   Priority: {channel_priority}\")\n",
    "        print(f\"   Title: {getattr(channel, 'title', 'Unknown')}\")\n",
    "        \n",
    "        # Fix the subscriber count formatting\n",
    "        if subscriber_count is not None:\n",
    "            print(f\"   Subscribers: {subscriber_count:,}\")\n",
    "        else:\n",
    "            print(f\"   Subscribers: Unknown (likely a channel)\")\n",
    "        \n",
    "        # Convert sampling_dates to set for faster lookup\n",
    "        sample_date_set = set(sampling_dates)\n",
    "        \n",
    "        # First, estimate total messages to process\n",
    "        print(\"   Estimating message count...\")\n",
    "        total_in_range = 0\n",
    "        messages_to_check = []\n",
    "        \n",
    "        # Quick scan to count messages in date range\n",
    "        async for message in client.iter_messages(channel, limit=1000):  # Check last 1000\n",
    "            if not message.date:\n",
    "                continue\n",
    "            if message.date.date() in sample_date_set:\n",
    "                total_in_range += 1\n",
    "                messages_to_check.append(message)\n",
    "            if message.date < START_DATE:\n",
    "                break\n",
    "        \n",
    "        print(f\"   Found ~{total_in_range} messages in sampling dates (from last 1000)\")\n",
    "        \n",
    "        # If we want to limit messages\n",
    "        if max_messages and total_in_range > max_messages:\n",
    "            print(f\"   Limiting to {max_messages} most recent messages\")\n",
    "            messages_to_check = messages_to_check[:max_messages]\n",
    "        \n",
    "        # Process messages with progress bar\n",
    "        with tqdm_sync(total=len(messages_to_check), desc=f\"{channel_username}\", \n",
    "                      unit=\"msg\", leave=True, ncols=100) as pbar:\n",
    "            \n",
    "            for message in messages_to_check:\n",
    "                # Extract text and metadata\n",
    "                if message.text and len(message.text) > 10:  # Skip very short messages\n",
    "                    ml_text = extract_multilingual_text(message.text)\n",
    "                    lang = detect_language_safe(message.text)\n",
    "                    escalation_count = count_escalation_keywords(message.text)\n",
    "                    event_context = find_nearest_event(message.date, timeline_events)\n",
    "                    \n",
    "                    # Add to buffer instead of inserting immediately\n",
    "                    messages_buffer.append((\n",
    "                        channel_username,\n",
    "                        category,\n",
    "                        message.id,\n",
    "                        message.date,\n",
    "                        message.sender_id,\n",
    "                        ml_text['original'][:1000],  # Limit text length\n",
    "                        ml_text['english'][:500],\n",
    "                        ml_text['russian'][:500],\n",
    "                        ml_text['ukrainian'][:500],\n",
    "                        lang,\n",
    "                        getattr(message, 'views', 0),\n",
    "                        getattr(message, 'forwards', 0),\n",
    "                        message.replies.replies if hasattr(message, 'replies') and message.replies else 0,\n",
    "                        bool(message.fwd_from),\n",
    "                        str(message.fwd_from.from_id) if message.fwd_from and hasattr(message.fwd_from, 'from_id') else None,\n",
    "                        type(message.media).__name__ if message.media else None,\n",
    "                        bool(getattr(message, 'reactions', None)),\n",
    "                        bool(message.edit_date),\n",
    "                        message.edit_date,\n",
    "                        escalation_count,\n",
    "                        event_context['days_from_event'],\n",
    "                        event_context['nearest_event'],\n",
    "                        event_context['near_major_event']\n",
    "                    ))\n",
    "                    \n",
    "                    messages_collected += 1\n",
    "                    \n",
    "                    # Batch insert when buffer is full\n",
    "                    if len(messages_buffer) >= batch_size:\n",
    "                        cursor.executemany('''\n",
    "                            INSERT OR IGNORE INTO messages \n",
    "                            (channel_username, channel_category, message_id, date, sender_id,\n",
    "                             message_text, message_text_english, message_text_russian, \n",
    "                             message_text_ukrainian, detected_language, views, forwards,\n",
    "                             replies_count, is_forwarded, forward_from_channel, media_type,\n",
    "                             has_reactions, is_edited, edit_date, escalation_keywords,\n",
    "                             days_from_event, nearest_event, near_major_event)\n",
    "                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                        ''', messages_buffer)\n",
    "                        conn.commit()\n",
    "                        messages_buffer = []\n",
    "                        pbar.set_postfix({'saved': messages_collected, 'buffer': 0})\n",
    "                    else:\n",
    "                        pbar.set_postfix({'saved': messages_collected - len(messages_buffer), \n",
    "                                         'buffer': len(messages_buffer)})\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Minimal rate limiting (reduce from 0.5s to 0.1s)\n",
    "                if messages_collected % 50 == 0:\n",
    "                    await asyncio.sleep(0.1)\n",
    "        \n",
    "        # Insert remaining messages in buffer\n",
    "        if messages_buffer:\n",
    "            cursor.executemany('''\n",
    "                INSERT OR IGNORE INTO messages \n",
    "                (channel_username, channel_category, message_id, date, sender_id,\n",
    "                 message_text, message_text_english, message_text_russian, \n",
    "                 message_text_ukrainian, detected_language, views, forwards,\n",
    "                 replies_count, is_forwarded, forward_from_channel, media_type,\n",
    "                 has_reactions, is_edited, edit_date, escalation_keywords,\n",
    "                 days_from_event, nearest_event, near_major_event)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', messages_buffer)\n",
    "            conn.commit()\n",
    "        \n",
    "        print(f\"   ✅ Collected {messages_collected} messages\")\n",
    "                    \n",
    "    except FloodWaitError as e:\n",
    "        wait_time = e.seconds + random.randint(10, 30)\n",
    "        print(f\"   ⚠️  Flood wait: {wait_time} seconds\")\n",
    "        await asyncio.sleep(wait_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        conn.commit()\n",
    "        \n",
    "    return messages_collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Minimal Channel Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_channel_minimal(client, channel_username, category, conn, sampling_dates, \n",
    "                                max_messages=None, batch_size=200):\n",
    "    \"\"\"\n",
    "    Minimal scraper - just collect raw messages for AI processing\n",
    "    No language detection, no keyword counting, no translation\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    messages_collected = 0\n",
    "    messages_buffer = []\n",
    "    channel_priority = estimate_channel_importance(channel_username, category)\n",
    "    \n",
    "    try:\n",
    "        # Get channel entity\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        \n",
    "        # Store channel metadata\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO channels \n",
    "            (username, title, category, priority_score, last_scraped)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            channel_username,\n",
    "            getattr(channel, 'title', channel_username),\n",
    "            category,\n",
    "            channel_priority,\n",
    "            datetime.now()\n",
    "        ))\n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"\\n📱 Scraping {channel_username} ({category})\")\n",
    "        print(f\"   Title: {getattr(channel, 'title', 'Unknown')}\")\n",
    "        \n",
    "        # Convert sampling_dates to set\n",
    "        sample_date_set = set(sampling_dates)\n",
    "        \n",
    "        # Collect messages\n",
    "        print(\"   Collecting messages...\", end='', flush=True)\n",
    "        messages_to_process = []\n",
    "        \n",
    "        async for message in client.iter_messages(channel, limit=3000):\n",
    "            if not message.date:\n",
    "                continue\n",
    "            if message.date.date() in sample_date_set:\n",
    "                messages_to_process.append(message)\n",
    "                if len(messages_to_process) % 100 == 0:\n",
    "                    print(f\"\\r   Collecting messages... {len(messages_to_process)} found\", end='', flush=True)\n",
    "            if message.date < START_DATE:\n",
    "                break\n",
    "                \n",
    "        print(f\"\\r   Found {len(messages_to_process)} messages in date range\")\n",
    "        \n",
    "        # Limit if requested\n",
    "        if max_messages and len(messages_to_process) > max_messages:\n",
    "            print(f\"   Limiting to {max_messages} messages\")\n",
    "            messages_to_process = messages_to_process[:max_messages]\n",
    "        \n",
    "        # Process messages quickly\n",
    "        print(\"   Processing messages...\", end='', flush=True)\n",
    "        \n",
    "        for i, message in enumerate(messages_to_process):\n",
    "            if message.text and len(message.text) > 20:  # Skip very short\n",
    "                \n",
    "                # Find nearest event for context\n",
    "                event_context = find_nearest_event(message.date, timeline_events)\n",
    "                \n",
    "                # Simple buffer append - no text processing\n",
    "                messages_buffer.append((\n",
    "                    channel_username,\n",
    "                    category,\n",
    "                    message.id,\n",
    "                    message.date,\n",
    "                    message.text[:2000],  # Just limit length\n",
    "                    getattr(message, 'views', 0),\n",
    "                    getattr(message, 'forwards', 0),\n",
    "                    message.replies.replies if hasattr(message, 'replies') and message.replies else 0,\n",
    "                    bool(message.media),\n",
    "                    event_context['days_from_event'],\n",
    "                    event_context['nearest_event'],\n",
    "                    event_context['near_major_event']\n",
    "                ))\n",
    "                \n",
    "                messages_collected += 1\n",
    "                \n",
    "                # Progress update\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    print(f\"\\r   Processing messages... {i+1}/{len(messages_to_process)} saved\", \n",
    "                          end='', flush=True)\n",
    "                \n",
    "                # Batch insert\n",
    "                if len(messages_buffer) >= batch_size:\n",
    "                    cursor.executemany('''\n",
    "                        INSERT OR IGNORE INTO messages_minimal \n",
    "                        (channel_username, channel_category, message_id, date,\n",
    "                         message_text, views, forwards, replies_count, has_media,\n",
    "                         days_from_event, nearest_event, near_major_event)\n",
    "                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', messages_buffer)\n",
    "                    conn.commit()\n",
    "                    messages_buffer = []\n",
    "        \n",
    "        # Insert remaining\n",
    "        if messages_buffer:\n",
    "            cursor.executemany('''\n",
    "                INSERT OR IGNORE INTO messages_minimal \n",
    "                (channel_username, channel_category, message_id, date,\n",
    "                 message_text, views, forwards, replies_count, has_media,\n",
    "                 days_from_event, nearest_event, near_major_event)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', messages_buffer)\n",
    "            conn.commit()\n",
    "        \n",
    "        print(f\"\\n   ✅ Saved {messages_collected} messages\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n   ❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return messages_collected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### TDQM Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Allows nested event loops in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Smart Sampling Strategy - Top Messages Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_top_messages_per_day_with_progress(client, channel_username, category, conn, \n",
    "                                                   messages_per_day=20):\n",
    "    \"\"\"\n",
    "    Collect only top N most viewed/engaged messages per day with progress bar\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    messages_collected = 0\n",
    "    channel_priority = estimate_channel_importance(channel_username, category)\n",
    "    \n",
    "    try:\n",
    "        channel = await client.get_entity(channel_username)\n",
    "        \n",
    "        print(f\"\\n📱 Scanning {channel_username} for top messages...\")\n",
    "        print(f\"   Strategy: Top {messages_per_day} messages per day\")\n",
    "        \n",
    "        # Store channel metadata\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO channels \n",
    "            (username, title, category, priority_score, last_scraped)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            channel_username,\n",
    "            getattr(channel, 'title', channel_username),\n",
    "            category,\n",
    "            channel_priority,\n",
    "            datetime.now()\n",
    "        ))\n",
    "        \n",
    "        # First pass: collect messages with progress bar\n",
    "        daily_messages = {}\n",
    "        messages_scanned = 0\n",
    "        \n",
    "        # Create progress bar for scanning\n",
    "        with tqdm(desc=f\"Scanning {channel_username}\", unit=\"msg\", leave=False) as pbar:\n",
    "            async for message in client.iter_messages(channel, limit=None):\n",
    "                if not message.date or not message.text or len(message.text) < 20:\n",
    "                    continue\n",
    "                    \n",
    "                # Skip if before war start\n",
    "                if message.date < START_DATE:\n",
    "                    break\n",
    "                    \n",
    "                date_key = message.date.date()\n",
    "                \n",
    "                if date_key not in daily_messages:\n",
    "                    daily_messages[date_key] = []\n",
    "                \n",
    "                # Calculate engagement score\n",
    "                views = getattr(message, 'views', 0) or 0\n",
    "                forwards = getattr(message, 'forwards', 0) or 0\n",
    "                replies = message.replies.replies if hasattr(message, 'replies') and message.replies else 0\n",
    "\n",
    "                engagement = views + (forwards * 10) + (replies * 5)\n",
    "                \n",
    "                daily_messages[date_key].append((engagement, message))\n",
    "                \n",
    "                # Keep only top N per day\n",
    "                if len(daily_messages[date_key]) > messages_per_day:\n",
    "                    daily_messages[date_key].sort(key=lambda x: x[0], reverse=True)\n",
    "                    daily_messages[date_key] = daily_messages[date_key][:messages_per_day]\n",
    "                \n",
    "                messages_scanned += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Update description with progress\n",
    "                if messages_scanned % 100 == 0:\n",
    "                    pbar.set_description(f\"{channel_username} - {len(daily_messages)} days\")\n",
    "        \n",
    "        print(f\"   Scanned {messages_scanned} messages from {len(daily_messages)} days\")\n",
    "        \n",
    "        # Second pass: save top messages with progress bar\n",
    "        messages_buffer = []\n",
    "        total_to_save = sum(len(msgs) for msgs in daily_messages.values())\n",
    "        \n",
    "        with tqdm(total=total_to_save, desc=f\"Saving top messages\", unit=\"msg\", leave=False) as pbar:\n",
    "            for date_key in sorted(daily_messages.keys()):\n",
    "                for engagement, message in daily_messages[date_key]:\n",
    "                    event_context = find_nearest_event(message.date, timeline_events)\n",
    "                    \n",
    "                    messages_buffer.append((\n",
    "                        channel_username,\n",
    "                        category,\n",
    "                        message.id,\n",
    "                        message.date,\n",
    "                        message.text[:2000],\n",
    "                        getattr(message, 'views', 0),\n",
    "                        getattr(message, 'forwards', 0),\n",
    "                        message.replies.replies if hasattr(message, 'replies') and message.replies else 0,\n",
    "                        bool(message.media),\n",
    "                        event_context['days_from_event'],\n",
    "                        event_context['nearest_event'],\n",
    "                        event_context['near_major_event']\n",
    "                    ))\n",
    "                    \n",
    "                    messages_collected += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                    if len(messages_buffer) >= 200:\n",
    "                        cursor.executemany('''\n",
    "                            INSERT OR IGNORE INTO messages_minimal \n",
    "                            (channel_username, channel_category, message_id, date,\n",
    "                             message_text, views, forwards, replies_count, has_media,\n",
    "                             days_from_event, nearest_event, near_major_event)\n",
    "                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                        ''', messages_buffer)\n",
    "                        conn.commit()\n",
    "                        messages_buffer = []\n",
    "                        pbar.set_postfix({'saved': messages_collected})\n",
    "        \n",
    "        # Insert remaining\n",
    "        if messages_buffer:\n",
    "            cursor.executemany('''\n",
    "                INSERT OR IGNORE INTO messages_minimal \n",
    "                (channel_username, channel_category, message_id, date,\n",
    "                 message_text, views, forwards, replies_count, has_media,\n",
    "                 days_from_event, nearest_event, near_major_event)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', messages_buffer)\n",
    "            conn.commit()\n",
    "        \n",
    "        print(f\"   ✅ Saved {messages_collected} top messages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return messages_collected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_simple(days_back=3, max_messages_per_channel=100):\n",
    "    \"\"\"\n",
    "    Simple test without complex progress bars\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Running simple test scraper\")\n",
    "    print(f\"   Days back: {days_back}\")\n",
    "    print(f\"   Max messages per channel: {max_messages_per_channel}\")\n",
    "    \n",
    "    # Test dates\n",
    "    test_dates = [\n",
    "        datetime.now(timezone.utc).date() - timedelta(days=i) \n",
    "        for i in range(days_back)\n",
    "    ]\n",
    "    print(f\"   Dates: {test_dates[-1]} to {test_dates[0]}\")\n",
    "    \n",
    "    # Test channels - let's test with just 2 first\n",
    "    test_channels = [\n",
    "        ('@operativnoZSU', 'pro_ukrainian'),\n",
    "        ('@rybar', 'pro_russian')\n",
    "    ]\n",
    "    \n",
    "    # Initialize client\n",
    "    client = TelegramClient('telegram_simple_test', API_ID, API_HASH)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n📱 Connecting to Telegram...\")\n",
    "        await client.start(PHONE)\n",
    "        print(\"✅ Connected!\")\n",
    "        \n",
    "        # Create test database\n",
    "        test_db = OUT_DIR / f'telegram_test_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.db'\n",
    "        conn = create_database(test_db)\n",
    "        print(f\"✅ Created database: {test_db.name}\")\n",
    "        \n",
    "        # Process channels\n",
    "        total_messages = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, (channel, category) in enumerate(test_channels, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Channel {i}/{len(test_channels)}\")\n",
    "            \n",
    "            channel_start = time.time()\n",
    "            messages = await scrape_channel_simple(\n",
    "                client, channel, category, conn, test_dates,\n",
    "                max_messages=max_messages_per_channel,\n",
    "                batch_size=50\n",
    "            )\n",
    "            channel_time = time.time() - channel_start\n",
    "            \n",
    "            total_messages += messages\n",
    "            if messages > 0:\n",
    "                print(f\"   ⏱️  Time: {channel_time:.1f}s ({messages/channel_time:.1f} msg/s)\")\n",
    "            \n",
    "            # Break between channels\n",
    "            if i < len(test_channels):\n",
    "                print(f\"\\n⏸️  Waiting 3 seconds...\")\n",
    "                await asyncio.sleep(3)\n",
    "        \n",
    "        # Export results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"📊 Exporting results...\")\n",
    "        csv_path = OUT_DIR / f'telegram_test_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        df = export_to_csv(conn, csv_path)\n",
    "        \n",
    "        # Summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n✅ Test complete!\")\n",
    "        print(f\"   Total messages: {total_messages}\")\n",
    "        print(f\"   Total time: {total_time:.1f}s\")\n",
    "        if total_messages > 0:\n",
    "            print(f\"   Average speed: {total_messages/total_time:.1f} msg/s\")\n",
    "        print(f\"   Output: {csv_path.name}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\n📝 First few messages:\")\n",
    "            cols = ['channel_username', 'date', 'escalation_keywords', 'views']\n",
    "            print(df[cols].head().to_string())\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️  Interrupted by user!\")\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        await client.disconnect()\n",
    "        print(\"\\n👋 Disconnected from Telegram\")\n",
    "\n",
    "# Run the simple test\n",
    "await test_simple(days_back=3, max_messages_per_channel=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def quick_test_verbose(days_back=3, max_messages_per_channel=200):\n",
    "    \"\"\"\n",
    "    Quick verbose test with progress tracking\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Running quick verbose test...\")\n",
    "    print(f\"   Days back: {days_back}\")\n",
    "    print(f\"   Max messages per channel: {max_messages_per_channel}\")\n",
    "    \n",
    "    # Test dates\n",
    "    test_dates = [\n",
    "        datetime.now(timezone.utc).date() - timedelta(days=i) \n",
    "        for i in range(days_back)\n",
    "    ]\n",
    "    print(f\"   Dates: {test_dates[0]} to {test_dates[-1]}\")\n",
    "    \n",
    "    # Test channels\n",
    "    test_channels = [\n",
    "        ('@operativnoZSU', 'pro_ukrainian'),\n",
    "        ('@rybar', 'pro_russian'),\n",
    "        ('@astra', 'neutral')\n",
    "    ]\n",
    "    \n",
    "    # Initialize client\n",
    "    client = TelegramClient('telegram_quick_test', API_ID, API_HASH)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n📱 Connecting to Telegram...\")\n",
    "        await client.start(PHONE)\n",
    "        print(\"✅ Connected!\")\n",
    "        \n",
    "        # Create test database\n",
    "        test_db = OUT_DIR / f'telegram_quicktest_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.db'\n",
    "        conn = create_database(test_db)\n",
    "        print(f\"✅ Created database: {test_db.name}\")\n",
    "        \n",
    "        # Overall progress\n",
    "        total_messages = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, (channel, category) in enumerate(test_channels, 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Channel {i}/{len(test_channels)}\")\n",
    "            \n",
    "            channel_start = time.time()\n",
    "            messages = await scrape_channel_optimized(\n",
    "                client, channel, category, conn, test_dates,\n",
    "                max_messages=max_messages_per_channel,\n",
    "                batch_size=50  # Smaller batches for test\n",
    "            )\n",
    "            channel_time = time.time() - channel_start\n",
    "            \n",
    "            total_messages += messages\n",
    "            print(f\"   ⏱️  Time: {channel_time:.1f}s ({messages/channel_time:.1f} msg/s)\")\n",
    "            \n",
    "            # Quick break between channels\n",
    "            if i < len(test_channels):\n",
    "                print(f\"\\n⏸️  Waiting 5 seconds before next channel...\")\n",
    "                await asyncio.sleep(5)\n",
    "        \n",
    "        # Export results\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"📊 Exporting results...\")\n",
    "        csv_path = OUT_DIR / f'telegram_quicktest_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "        df = export_to_csv(conn, csv_path)\n",
    "        \n",
    "        # Summary\n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\n✅ Test complete!\")\n",
    "        print(f\"   Total messages: {total_messages}\")\n",
    "        print(f\"   Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "        print(f\"   Average speed: {total_messages/total_time:.1f} msg/s\")\n",
    "        print(f\"   Output: {csv_path}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        if len(df) > 0:\n",
    "            print(f\"\\n📝 Sample messages:\")\n",
    "            sample_cols = ['channel_username', 'date', 'message_text', 'escalation_keywords', 'views']\n",
    "            print(df[sample_cols].head(10).to_string(max_colwidth=50))\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️  Interrupted by user!\")\n",
    "        print(\"Saving collected data...\")\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        await client.disconnect()\n",
    "        print(\"\\n👋 Disconnected from Telegram\")\n",
    "\n",
    "# Run the optimized test\n",
    "await quick_test_verbose(days_back=3, max_messages_per_channel=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Full War Timeline Scraping Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Scrape Full Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_full_timeline_smart_with_progress():\n",
    "    \"\"\"\n",
    "    Smart scraping for entire war timeline with comprehensive progress tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting smart full timeline scrape\")\n",
    "    print(f\"   Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "    \n",
    "    # Calculate expected volume\n",
    "    days_of_war = (END_DATE - START_DATE).days\n",
    "    total_channels = sum(len(ch) for ch in CHANNELS.values())\n",
    "    avg_messages_per_day = 15\n",
    "    \n",
    "    estimated_total = days_of_war * total_channels * avg_messages_per_day\n",
    "    print(f\"   Channels: {total_channels}\")\n",
    "    print(f\"   Estimated messages: {estimated_total:,}\")\n",
    "    print(f\"   Target: 100-300k messages\")\n",
    "    \n",
    "    # Initialize client\n",
    "    client = TelegramClient('telegram_full_scrape', API_ID, API_HASH)\n",
    "    \n",
    "    try:\n",
    "        await client.start(PHONE)\n",
    "        print(\"✅ Connected to Telegram\\n\")\n",
    "        \n",
    "        # Create database\n",
    "        db_path = OUT_DIR / f'telegram_full_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.db'\n",
    "        conn = create_minimal_database(db_path)\n",
    "        \n",
    "        total_messages = 0\n",
    "        channel_times = []\n",
    "        \n",
    "        # Process channels by priority\n",
    "        all_channels = []\n",
    "        for category, channel_list in CHANNELS.items():\n",
    "            for channel in channel_list:\n",
    "                priority = estimate_channel_importance(channel, category)\n",
    "                all_channels.append((priority, channel, category))\n",
    "        \n",
    "        all_channels.sort(reverse=True, key=lambda x: x[0])\n",
    "        \n",
    "        # Main progress bar for channels\n",
    "        with tqdm(total=len(all_channels), desc=\"Overall Progress\", unit=\"channel\") as main_pbar:\n",
    "            \n",
    "            for i, (priority, channel, category) in enumerate(all_channels, 1):\n",
    "                channel_start = time.time()\n",
    "                \n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Channel {i}/{len(all_channels)}: {channel} (Priority: {priority})\")\n",
    "                \n",
    "                # More messages for high priority channels\n",
    "                if priority >= 9:\n",
    "                    messages_per_day = 20\n",
    "                elif priority >= 7:\n",
    "                    messages_per_day = 15\n",
    "                else:\n",
    "                    messages_per_day = 10\n",
    "                \n",
    "                messages = await scrape_top_messages_per_day_with_progress(\n",
    "                    client, channel, category, conn, \n",
    "                    messages_per_day=messages_per_day\n",
    "                )\n",
    "                \n",
    "                total_messages += messages\n",
    "                channel_time = time.time() - channel_start\n",
    "                channel_times.append(channel_time)\n",
    "                \n",
    "                # Update main progress bar\n",
    "                main_pbar.update(1)\n",
    "                avg_time = sum(channel_times) / len(channel_times)\n",
    "                remaining_channels = len(all_channels) - i\n",
    "                eta_seconds = avg_time * remaining_channels\n",
    "                \n",
    "                main_pbar.set_postfix({\n",
    "                    'Total': f\"{total_messages:,}\",\n",
    "                    'ETA': f\"{eta_seconds/60:.0f}m\"\n",
    "                })\n",
    "                \n",
    "                # Break between channels\n",
    "                await asyncio.sleep(5)\n",
    "                \n",
    "                # Extended break every 5 channels\n",
    "                if i % 5 == 0 and i < len(all_channels):\n",
    "                    print(\"\\n⏸️  Taking 1-minute break...\")\n",
    "                    for _ in tqdm(range(60), desc=\"Break\", unit=\"s\", leave=False):\n",
    "                        await asyncio.sleep(1)\n",
    "        \n",
    "        # Export final dataset with progress\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"📊 Exporting final dataset...\")\n",
    "        \n",
    "        query = '''\n",
    "            SELECT * FROM messages_minimal \n",
    "            ORDER BY date DESC\n",
    "        '''\n",
    "        \n",
    "        # Show export progress\n",
    "        with tqdm(desc=\"Exporting to CSV\", unit=\"rows\") as export_pbar:\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            export_pbar.update(len(df))\n",
    "            \n",
    "            csv_path = OUT_DIR / f'telegram_full_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "            df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        # Final summary\n",
    "        total_time = sum(channel_times)\n",
    "        print(f\"\\n✅ Scraping complete!\")\n",
    "        print(f\"   Total messages: {total_messages:,}\")\n",
    "        print(f\"   Total time: {total_time/60:.1f} minutes ({total_time/3600:.1f} hours)\")\n",
    "        print(f\"   Average speed: {total_messages/total_time:.1f} msg/s\")\n",
    "        print(f\"   Output: {csv_path}\")\n",
    "        \n",
    "        # Channel statistics\n",
    "        print(f\"\\n📊 Channel Statistics:\")\n",
    "        stats_query = '''\n",
    "            SELECT channel_username, COUNT(*) as msg_count \n",
    "            FROM messages_minimal \n",
    "            GROUP BY channel_username \n",
    "            ORDER BY msg_count DESC\n",
    "            LIMIT 10\n",
    "        '''\n",
    "        stats_df = pd.read_sql_query(stats_query, conn)\n",
    "        print(stats_df.to_string(index=False))\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n⚠️  Scraping interrupted by user!\")\n",
    "        print(f\"   Collected {total_messages:,} messages before interruption\")\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Fatal error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    finally:\n",
    "        await client.disconnect()\n",
    "        print(\"\\n👋 Disconnected from Telegram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_tqdm_scraper(num_channels=3):\n",
    "    \"\"\"Quick test of the TQDM implementation\"\"\"\n",
    "    \n",
    "    print(\"🧪 Testing TQDM scraper with limited channels\")\n",
    "    \n",
    "    # Use only first N channels for test\n",
    "    test_channels = []\n",
    "    for category, channels in CHANNELS.items():\n",
    "        for channel in channels[:1]:  # One from each category\n",
    "            test_channels.append((channel, category))\n",
    "        if len(test_channels) >= num_channels:\n",
    "            break\n",
    "    \n",
    "    client = TelegramClient('telegram_tqdm_test', API_ID, API_HASH)\n",
    "    \n",
    "    try:\n",
    "        await client.start(PHONE)\n",
    "        print(\"✅ Connected\\n\")\n",
    "        \n",
    "        # Create test database\n",
    "        db_path = OUT_DIR / f'telegram_tqdm_test_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.db'\n",
    "        conn = create_minimal_database(db_path)\n",
    "        \n",
    "        total_messages = 0\n",
    "        \n",
    "        with tqdm(total=len(test_channels), desc=\"Test Progress\", unit=\"channel\") as pbar:\n",
    "            for channel, category in test_channels:\n",
    "                messages = await scrape_top_messages_per_day_with_progress(\n",
    "                    client, channel, category, conn, \n",
    "                    messages_per_day=10\n",
    "                )\n",
    "                total_messages += messages\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({'Total': total_messages})\n",
    "        \n",
    "        print(f\"\\n✅ Test complete! Collected {total_messages} messages\")\n",
    "        conn.close()\n",
    "        \n",
    "    finally:\n",
    "        await client.disconnect()\n",
    "\n",
    "# Run the test\n",
    "await test_tqdm_scraper(num_channels=2)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Usage Instructions\n",
    "\n",
    "# Run the full smart scrape:\n",
    "# await scrape_full_timeline_smart_with_progress()\n",
    "\n",
    "# Or test with a few channels first:\n",
    "# await test_tqdm_scraper(num_channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### RUN FULL SCRAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full smart scrape\n",
    "await scrape_full_timeline_smart_with_progress()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### Analysis of Scraped Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "csv_path = OUT_DIR / 'telegram_full_20250605_213258.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert date column to datetime and remove timezone for consistency\n",
    "df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "# Define channel categories based on your CHANNELS dict\n",
    "channel_categories = {\n",
    "    # Pro-Ukrainian\n",
    "    '@ssternenko': 'Pro-Ukrainian',\n",
    "    '@operativnoZSU': 'Pro-Ukrainian', \n",
    "    '@voynareal': 'Pro-Ukrainian',\n",
    "    '@DeepStateUA': 'Pro-Ukrainian',\n",
    "    '@horevica': 'Pro-Ukrainian',\n",
    "    '@ukr_sof': 'Pro-Ukrainian',\n",
    "    '@aerobomber': 'Pro-Ukrainian',\n",
    "    '@combat_ftg': 'Pro-Ukrainian',\n",
    "    \n",
    "    # Pro-Russian\n",
    "    '@rybar': 'Pro-Russian',\n",
    "    '@starshe_eddy': 'Pro-Russian',\n",
    "    '@wargonzo': 'Pro-Russian',\n",
    "    '@grey_zone': 'Pro-Russian',\n",
    "    '@vysokygovorit': 'Pro-Russian',\n",
    "    '@milinfolive': 'Pro-Russian',\n",
    "    '@voenkorKotenok': 'Pro-Russian',\n",
    "    '@romanov_92': 'Pro-Russian',\n",
    "    \n",
    "    # Neutral/Independent\n",
    "    '@astra': 'Neutral/Independent',\n",
    "    '@CITeam_en': 'Neutral/Independent',\n",
    "    '@bellingcat': 'Neutral/Independent',\n",
    "    '@GirkinGirkin': 'Neutral/Independent',\n",
    "    \n",
    "    # Official\n",
    "    '@V_Zelenskiy_official': 'Official',\n",
    "    '@spravdi': 'Official',\n",
    "    '@mod_russia': 'Official',\n",
    "    '@MID_RF': 'Official'\n",
    "}\n",
    "\n",
    "# Add category to dataframe\n",
    "df['category'] = df['channel_username'].map(channel_categories)\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Messages over time by category\n",
    "ax1 = plt.subplot(3, 2, 1)\n",
    "df_daily = df.groupby([pd.Grouper(key='date', freq='D'), 'category']).size().reset_index(name='count')\n",
    "pivot_daily = df_daily.pivot(index='date', columns='category', values='count').fillna(0)\n",
    "\n",
    "# Stacked area chart\n",
    "colors = {'Pro-Ukrainian': '#0057B7', 'Pro-Russian': '#D52B1E', \n",
    "          'Neutral/Independent': '#808080', 'Official': '#FFD700'}\n",
    "pivot_daily.plot(kind='area', stacked=True, ax=ax1, color=[colors.get(col, '#333') for col in pivot_daily.columns])\n",
    "ax1.set_title('Messages Over Time by Category (Daily)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Number of Messages')\n",
    "ax1.legend(title='Category', loc='upper left')\n",
    "\n",
    "# Add major events\n",
    "events = [\n",
    "    ('2022-02-24', 'Invasion'),\n",
    "    ('2022-09-11', 'Kharkiv Liberation'),\n",
    "    ('2023-06-06', 'Dam Destroyed'),\n",
    "    ('2024-08-06', 'Kursk Incursion'),\n",
    "    ('2024-11-05', 'Trump Elected')\n",
    "]\n",
    "for date, label in events:\n",
    "    event_date = pd.to_datetime(date)\n",
    "    if pivot_daily.index.min() <= event_date <= pivot_daily.index.max():\n",
    "        ax1.axvline(x=event_date, color='red', linestyle='--', alpha=0.5)\n",
    "        ax1.text(event_date, ax1.get_ylim()[1]*0.9, label, rotation=90, fontsize=8)\n",
    "\n",
    "# 2. Total messages by channel\n",
    "ax2 = plt.subplot(3, 2, 2)\n",
    "channel_counts = df['channel_username'].value_counts().head(15)\n",
    "channel_counts_df = pd.DataFrame({'channel': channel_counts.index, 'count': channel_counts.values})\n",
    "channel_counts_df['category'] = channel_counts_df['channel'].map(channel_categories)\n",
    "\n",
    "# Color bars by category\n",
    "colors_list = [colors.get(cat, '#333') for cat in channel_counts_df['category']]\n",
    "bars = ax2.barh(channel_counts_df['channel'], channel_counts_df['count'], color=colors_list)\n",
    "ax2.set_xlabel('Number of Messages')\n",
    "ax2.set_title('Top 15 Channels by Message Count', fontsize=14, fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width, bar.get_y() + bar.get_height()/2, f'{int(width):,}', \n",
    "             ha='left', va='center', fontsize=8)\n",
    "\n",
    "# 3. Category distribution pie chart\n",
    "ax3 = plt.subplot(3, 2, 3)\n",
    "category_counts = df['category'].value_counts()\n",
    "wedges, texts, autotexts = ax3.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "        colors=[colors.get(cat, '#333') for cat in category_counts.index],\n",
    "        explode=[0.05 if cat == 'Pro-Ukrainian' else 0 for cat in category_counts.index])\n",
    "ax3.set_title('Message Distribution by Category', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add total counts to labels\n",
    "for i, (text, autotext) in enumerate(zip(texts, autotexts)):\n",
    "    text.set_text(f\"{text.get_text()}\\n({category_counts.values[i]:,})\")\n",
    "    autotext.set_fontsize(10)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "# 4. Weekly message volume heatmap\n",
    "ax4 = plt.subplot(3, 2, 4)\n",
    "df['week'] = df['date'].dt.to_period('W')\n",
    "df['dayofweek'] = df['date'].dt.day_name()\n",
    "weekly_heatmap = df.groupby(['week', 'dayofweek']).size().reset_index(name='count')\n",
    "\n",
    "# Limit to last 20 weeks for readability\n",
    "recent_weeks = sorted(df['week'].unique())[-20:]\n",
    "weekly_heatmap = weekly_heatmap[weekly_heatmap['week'].isin(recent_weeks)]\n",
    "\n",
    "# Pivot for heatmap\n",
    "pivot_heatmap = weekly_heatmap.pivot(index='week', columns='dayofweek', values='count').fillna(0)\n",
    "# Reorder days\n",
    "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "pivot_heatmap = pivot_heatmap.reindex(columns=days_order)\n",
    "\n",
    "sns.heatmap(pivot_heatmap.T, cmap='YlOrRd', ax=ax4, cbar_kws={'label': 'Messages'})\n",
    "ax4.set_title('Message Activity Heatmap (Last 20 Weeks)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Week')\n",
    "ax4.set_ylabel('Day of Week')\n",
    "\n",
    "# 5. Engagement metrics by category\n",
    "ax5 = plt.subplot(3, 2, 5)\n",
    "# Calculate average engagement per category\n",
    "df['engagement_score'] = df['views'].fillna(0) + (df['forwards'].fillna(0) * 10) + (df['replies_count'].fillna(0) * 5)\n",
    "engagement_by_cat = df.groupby('category')['engagement_score'].agg(['mean', 'median'])\n",
    "\n",
    "x = np.arange(len(engagement_by_cat))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax5.bar(x - width/2, engagement_by_cat['mean'], width, label='Mean', \n",
    "                 color=[colors.get(cat, '#333') for cat in engagement_by_cat.index])\n",
    "bars2 = ax5.bar(x + width/2, engagement_by_cat['median'], width, label='Median', alpha=0.7,\n",
    "                 color=[colors.get(cat, '#333') for cat in engagement_by_cat.index])\n",
    "\n",
    "ax5.set_xlabel('Category')\n",
    "ax5.set_ylabel('Engagement Score')\n",
    "ax5.set_title('Average Engagement by Category', fontsize=14, fontweight='bold')\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(engagement_by_cat.index, rotation=45)\n",
    "ax5.legend()\n",
    "ax5.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 6. Time series of Pro-Ukrainian vs Pro-Russian balance\n",
    "ax6 = plt.subplot(3, 2, 6)\n",
    "balance_df = df[df['category'].isin(['Pro-Ukrainian', 'Pro-Russian'])]\n",
    "daily_balance = balance_df.groupby([pd.Grouper(key='date', freq='D'), 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "if 'Pro-Ukrainian' in daily_balance.columns and 'Pro-Russian' in daily_balance.columns:\n",
    "    daily_balance['ratio'] = daily_balance['Pro-Ukrainian'] / (daily_balance['Pro-Russian'] + 1)  # +1 to avoid division by zero\n",
    "    daily_balance['ratio_ma'] = daily_balance['ratio'].rolling(window=7).mean()\n",
    "    \n",
    "    ax6.plot(daily_balance.index, daily_balance['ratio_ma'], color='green', linewidth=2)\n",
    "    ax6.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "    ax6.fill_between(daily_balance.index, 1, daily_balance['ratio_ma'], \n",
    "                     where=(daily_balance['ratio_ma'] >= 1), color='#0057B7', alpha=0.3, label='Pro-Ukrainian')\n",
    "    ax6.fill_between(daily_balance.index, 1, daily_balance['ratio_ma'], \n",
    "                     where=(daily_balance['ratio_ma'] < 1), color='#D52B1E', alpha=0.3, label='Pro-Russian')\n",
    "    \n",
    "    ax6.set_title('Pro-Ukrainian vs Pro-Russian Message Ratio (7-day MA)', fontsize=14, fontweight='bold')\n",
    "    ax6.set_xlabel('Date')\n",
    "    ax6.set_ylabel('Ratio (Pro-UKR / Pro-RU)')\n",
    "    ax6.set_yscale('log')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'telegram_data_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n📊 TELEGRAM DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total messages collected: {len(df):,}\")\n",
    "print(f\"Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total channels: {df['channel_username'].nunique()}\")\n",
    "print(f\"Average messages per channel: {len(df) / df['channel_username'].nunique():,.0f}\")\n",
    "print(f\"Average messages per day: {len(df) / (df['date'].max() - df['date'].min()).days:,.0f}\")\n",
    "\n",
    "print(\"\\n📈 CATEGORY BREAKDOWN\")\n",
    "print(\"-\" * 40)\n",
    "for cat, count in df['category'].value_counts().items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"{cat:<20} {count:>8,} ({percentage:>5.1f}%)\")\n",
    "\n",
    "print(\"\\n🎯 TOP ENGAGEMENT POSTS\")\n",
    "print(\"-\" * 40)\n",
    "top_posts = df.nlargest(5, 'engagement_score')[['channel_username', 'date', 'message_text', 'views', 'forwards', 'engagement_score']]\n",
    "for idx, post in top_posts.iterrows():\n",
    "    print(f\"\\nChannel: {post['channel_username']} | Date: {post['date'].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Views: {post['views']:,} | Forwards: {post['forwards']:,} | Score: {post['engagement_score']:,.0f}\")\n",
    "    print(f\"Text: {post['message_text'][:100]}...\")\n",
    "\n",
    "print(\"\\n💾 MESSAGES NEAR MAJOR EVENTS\")\n",
    "print(\"-\" * 40)\n",
    "near_events = df[df['near_major_event'] == True]\n",
    "event_summary = near_events.groupby('nearest_event').size().sort_values(ascending=False)\n",
    "for event, count in event_summary.items():\n",
    "    print(f\"{event:<20} {count:>8,} messages\")\n",
    "\n",
    "print(\"\\n✅ Visualization saved to:\", FIG_DIR / 'telegram_data_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
